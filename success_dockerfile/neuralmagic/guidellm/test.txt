============================= test session starts ==============================
platform linux -- Python 3.10.15, pytest-8.3.4, pluggy-1.5.0 -- /usr/local/bin/python3.10
cachedir: .pytest_cache
rootdir: /repo
configfile: pyproject.toml
plugins: requests-mock-1.12.1, anyio-4.7.0, xdist-3.6.1
collecting ... None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
collected 345 items

<Dir repo>
  <Package tests>
    <Package e2e>
      <Module test_guidellm.py>
        <Function test_import>
    <Package integration>
      <Module test_guidellm.py>
        <Function test_import>
    <Package unit>
      <Package backend>
        <Module test_base.py>
          <Function test_backend_registry>
          <Function test_generative_response_creation>
          <Function test_backend_make_request>
          <Function test_backend_submit_final>
          <Function test_backend_submit_multi>
          <Function test_backend_submit_no_response>
          <Function test_backend_submit_multi_final>
          <Function test_backend_models>
          <Function test_backend_test_connection>
          <Function test_backend_tokenizer>
          <Function test_backend_abstract_methods>
        <Module test_openai_backend.py>
          <Function test_openai_backend_create[test_key-http://test-target-test-model-request_args0-http://test-target]>
          <Function test_openai_backend_create[None-None-None-request_args1-http://localhost:8000/v1]>
          <Function test_openai_backend_models>
          <Function test_openai_backend_make_request[req0-None]>
          <Function test_openai_backend_make_request[req1-None]>
          <Function test_openai_backend_make_request[req2-request_args2]>
          <Function test_openai_backend_make_request[req3-request_args3]>
          <Function test_openai_backend_submit>
          <Function test_openai_backend_api_key>
          <Function test_openai_backend_api_key_env>
          <Function test_openai_backend_target>
          <Function test_openai_backend_target_env>
          <Function test_openai_backend_target_none_error>
      <Package cli>
        <Module test_custom_type_params.py>
          <Function test_valid_integer_input>
          <Function test_valid_dataset_input>
          <Function test_invalid_string_input>
          <Function test_invalid_float_input>
          <Function test_invalid_non_numeric_string_input>
          <Function test_invalid_mixed_string_input>
      <Package core>
        <Module test_distribution.py>
          <Function test_distribution_initialization>
          <Function test_distribution_statistics>
          <Function test_distribution_no_data>
          <Function test_distribution_add_data>
          <Function test_distribution_remove_data>
          <Function test_distribution_str>
          <Function test_distribution_repr>
          <Function test_distribution_json>
          <Function test_distribution_yaml>
        <Module test_report.py>
          <Function test_guidance_report_initialization>
          <Function test_guidance_report_initialization_with_params>
          <Function test_guidance_report_print>
          <Function test_guidance_report_json>
          <Function test_guidance_report_yaml>
          <Function test_guidance_report_save_load_file>
          <Function test_empty_guidance_report>
          <Function test_compare_guidance_reports>
          <Function test_compare_guidance_reports_inequality>
        <Module test_request.py>
          <Function test_text_generation_request_initialization>
          <Function test_text_generation_request_initialization_with_params>
          <Function test_request_json>
          <Function test_request_yaml>
        <Module test_result.py>
          <Function test_text_generation_result_initialization>
          <Function test_text_generation_result_start>
          <Function test_text_generation_result_output_token>
          <Function test_text_generation_result_end>
          <Function test_text_generation_result_improper_lifecycle>
          <Function test_text_generation_result_json>
          <Function test_text_generation_result_yaml>
          <Function test_text_generation_error_initialization>
          <Function test_text_generation_error_json>
          <Function test_text_generation_error_yaml>
          <Function test_text_generation_benchmark_initialization>
          <Function test_text_generation_benchmark_started>
          <Function test_text_generation_benchmark_expected_rate>
          <Function test_text_generation_benchmark_overloaded_rate>
          <Function test_text_generation_benchmark_completed_with_result>
          <Function test_text_generation_benchmark_completed_with_error>
          <Function test_text_generation_benchmark_iter>
          <Function test_text_generation_benchmark_json>
          <Function test_text_generation_benchmark_yaml>
          <Function test_text_generation_benchmark_report_initialization>
          <Function test_text_generation_benchmark_report_add_benchmark>
          <Function test_text_generation_benchmark_report_iter>
          <Function test_text_generation_benchmark_report_json>
          <Function test_text_generation_benchmark_report_yaml>
        <Module test_serializable.py>
          <Function test_serializable_json>
          <Function test_serializable_yaml>
          <Function test_serializable_file_json>
          <Function test_serializable_file_yaml>
          <Function test_serializable_file_without_extension>
          <Function test_serializable_file_with_directory_json>
          <Function test_serializable_file_with_directory_yaml>
          <Function test_serializable_file_infer_extension>
          <Function test_serializable_file_invalid_extension>
          <Function test_serializable_load_missing_path>
          <Function test_serializable_load_non_file_path>
      <Package executor>
        <Module test_base.py>
          <Function test_executor_result_instantiation>
          <Function test_executor_instantiation[sweep-None]>
          <Function test_executor_instantiation[synchronous-None]>
          <Function test_executor_instantiation[throughput-None]>
          <Function test_executor_instantiation[constant-10]>
          <Function test_executor_instantiation[constant-rate4]>
          <Function test_executor_instantiation[poisson-10]>
          <Function test_executor_instantiation[poisson-rate6]>
          <Function test_executor_run_sweep>
          <Function test_executor_run_non_rate_modes[synchronous]>
          <Function test_executor_run_non_rate_modes[throughput]>
          <Function test_executor_run_rate_modes[constant-10]>
          <Function test_executor_run_rate_modes[constant-rate1]>
          <Function test_executor_run_rate_modes[poisson-10]>
          <Function test_executor_run_rate_modes[poisson-rate3]>
        <Module test_profile_generator.py>
          <Function test_profile_generator_mode>
          <Function test_profile_instantiation>
          <Function test_profile_generator_instantiation[sweep-None]>
          <Function test_profile_generator_instantiation[synchronous-None]>
          <Function test_profile_generator_instantiation[throughput-None]>
          <Function test_profile_generator_instantiation[constant-10]>
          <Function test_profile_generator_instantiation[constant-rate4]>
          <Function test_profile_generator_instantiation[poisson-10]>
          <Function test_profile_generator_instantiation[poisson-rate6]>
          <Function test_profile_generator_invalid_instantiation[invalid_mode-None]>
          <Function test_profile_generator_invalid_instantiation[sweep-10]>
          <Function test_profile_generator_invalid_instantiation[sweep-rate2]>
          <Function test_profile_generator_invalid_instantiation[synchronous-10]>
          <Function test_profile_generator_invalid_instantiation[synchronous-rate4]>
          <Function test_profile_generator_invalid_instantiation[throughput-10]>
          <Function test_profile_generator_invalid_instantiation[throughput-rate6]>
          <Function test_profile_generator_invalid_instantiation[constant-None]>
          <Function test_profile_generator_invalid_instantiation[constant--1]>
          <Function test_profile_generator_invalid_instantiation[constant-0]>
          <Function test_profile_generator_invalid_instantiation[poisson-None]>
          <Function test_profile_generator_invalid_instantiation[poisson--1]>
          <Function test_profile_generator_invalid_instantiation[poisson-0]>
          <Function test_profile_generator_next_sweep>
          <Function test_profile_generator_next_synchronous>
          <Function test_profile_generator_next_throughput>
          <Function test_profile_generator_next_constant[10]>
          <Function test_profile_generator_next_constant[rate1]>
          <Function test_profile_generator_next_poisson[10]>
          <Function test_profile_generator_next_poisson[rate1]>
      <Package request>
        <Module test_base.py>
          <Function test_request_generator_sync_constructor>
          <Function test_request_generator_async_constructor>
          <Function test_request_generator_sync_iter>
          <Function test_request_generator_async_iter>
          <Function test_request_generator_iter_calls_create_item>
          <Function test_request_generator_async_iter_calls_create_item>
          <Function test_request_generator_repr>
          <Function test_request_generator_stop>
          <Function test_request_generator_with_mock_tokenizer>
          <Function test_request_generator_populate_queue>
          <Function test_request_generator_async_stop_during_population>
        <Module test_emulated.py>
          <Function test_emulated_config_construction>
          <Function test_emulated_config_create_dict>
          <Function test_emulated_config_token_range[10-2-None-None-expected_range0]>
          <Function test_emulated_config_token_range[10-2-5-15-expected_range1]>
          <Function test_emulated_config_token_range[10-None-5-15-expected_range2]>
          <Function test_emulated_config_token_range[10-2-1-None-expected_range3]>
          <Function test_emulated_config_sample_tokens[10-None-None-None-expected_range0]>
          <Function test_emulated_config_sample_tokens[10-5-None-None-expected_range1]>
          <Function test_emulated_config_sample_tokens[10-5-5-15-expected_range2]>
          <Function test_emulated_config_sample_tokens[10-None-5-15-expected_range3]>
          <Function test_emulated_config_sample_tokens[10-5-2-None-expected_range4]>
          <Function test_emulated_config_sample_tokens[10-5-None-20-expected_range5]>
          <Function test_emulated_config_create>
          <Function test_endless_data_words_construction[word1 word2  word3\nword4   word5-expected_words0-expected_indices0]>
          <Function test_endless_data_words_construction[word1  word2\n  word3   word4\n word5-expected_words1-expected_indices1]>
          <Function test_endless_data_words_create_from_basic_file>
          <Function test_endless_data_words_create_text[word1 word2 word3 word4-0-2-word1 word2]>
          <Function test_endless_data_words_create_text[word1     word2\nword3   word4-1-2-word2\nword3]>
          <Function test_endless_data_words_create_text[word1     word2\nword3   word4-1-6-word2\nword3   word4 word1     word2\nword3]>
          <Function test_emulated_request_generator_construction>
          <Function test_emulated_request_generator_create_item>
          <Function test_emulated_request_generator_sample_prompt>
          <Function test_emulated_request_generator_random_seed>
          <Function test_emulated_request_generator_lifecycle[dict-config0]>
          <Function test_emulated_request_generator_lifecycle[dict-config1]>
          <Function test_emulated_request_generator_lifecycle[dict-config2]>
          <Function test_emulated_request_generator_lifecycle[json_str-{"prompt_tokens": 10, "generated_tokens": 20}]>
          <Function test_emulated_request_generator_lifecycle[key_value_str-prompt_tokens=10, generated_tokens=20]>
          <Function test_emulated_request_generator_lifecycle[file_str-{"prompt_tokens": 10, "generated_tokens": 20}]>
          <Function test_emulated_request_generator_lifecycle[file_path-{"prompt_tokens": 10, "generated_tokens": 20}]>
        <Module test_file.py>
          <Function test_file_request_generator_constructor>
          <Function test_file_request_generator_create_item>
          <Function test_file_request_generator_file_types_lifecycle[txt-Test content 1.\nTest content 2.\nTest content 3.\n]>
          <Function test_file_request_generator_file_types_lifecycle[csv-text,label,extra\nTest content 1.,1,extra 1\nTest content 2.,2,extra 2\nTest content 3.,3,extra 3\n]>
          <Function test_file_request_generator_file_types_lifecycle[jsonl-{"text": "Test content 1."}\n{"text": "Test content 2."}\n{"text": "Test content 3."}\n]>
          <Function test_file_request_generator_file_types_lifecycle[csv-prompt,text,extra\nTest content 1., text 1, extra 1\nTest content 2., text 2, extra 2\nTest content 3., text 3, extra 3\n]>
          <Function test_file_request_generator_file_types_lifecycle[json-[{"text": "Test content 1."}, {"text": "Test content 2."}, {"text": "Test content 3."}]\n]>
          <Function test_file_request_generator_file_types_lifecycle[json-{"object_1": {"text": "Test content 1."}, "object_2": {"text": "Test content 2."}, "object_3": {"text": "Test content 3."}}\n]>
          <Function test_file_request_generator_file_types_lifecycle[yaml-items:\n   - text: Test content 1.\n   - text: Test content 2.\n   - text: Test content 3.\n]>
          <Function test_file_request_generator_file_types_lifecycle[yaml-object_1:\n  text: Test content 1.\nobject_2:\n  text: Test content 2.\nobject_3:\n  text: Test content 3.\n]>
          <Function test_file_request_generator_len[txt-Test content 1.\nTest content 2.\nTest content 3.\n]>
          <Function test_file_request_generator_len[csv-text,label,extra\nTest content 1.,1,extra 1\nTest content 2.,2,extra 2\nTest content 3.,3,extra 3\n]>
          <Function test_file_request_generator_len[jsonl-{"text": "Test content 1."}\n{"text": "Test content 2."}\n{"text": "Test content 3."}\n]>
          <Function test_file_request_generator_len[csv-prompt,text,extra\nTest content 1., text 1, extra 1\nTest content 2., text 2, extra 2\nTest content 3., text 3, extra 3\n]>
          <Function test_file_request_generator_len[json-[{"text": "Test content 1."}, {"text": "Test content 2."}, {"text": "Test content 3."}]\n]>
          <Function test_file_request_generator_len[json-{"object_1": {"text": "Test content 1."}, "object_2": {"text": "Test content 2."}, "object_3": {"text": "Test content 3."}}\n]>
          <Function test_file_request_generator_len[yaml-items:\n   - text: Test content 1.\n   - text: Test content 2.\n   - text: Test content 3.\n]>
          <Function test_file_request_generator_len[yaml-object_1:\n  text: Test content 1.\nobject_2:\n  text: Test content 2.\nobject_3:\n  text: Test content 3.\n]>
        <Module test_transformers.py>
          <Function test_transformers_dataset_request_generator_constructor>
          <Function test_transformers_dataset_request_generator_create_item>
          <Function test_transformers_dataset_request_generator_lifecycle[mock/directory/file.csv-dataset0]>
          <Function test_transformers_dataset_request_generator_lifecycle[mock/directory/file.json-dataset1]>
          <Function test_transformers_dataset_request_generator_lifecycle[mock/directory/file.py-dataset2]>
          <Function test_transformers_dataset_request_generator_lifecycle[dataset_arg3-None]>
          <Function test_transformers_dataset_request_generator_lifecycle[dataset_arg4-None]>
          <Function test_transformers_dataset_request_generator_lifecycle[dataset_arg5-None]>
          <Function test_transformers_dataset_request_generator_lifecycle[dataset_arg6-None]>
          <Function test_transformers_dataset_request_generator_len[mock/directory/file.csv-dataset0]>
          <Function test_transformers_dataset_request_generator_len[mock/directory/file.json-dataset1]>
          <Function test_transformers_dataset_request_generator_len[mock/directory/file.py-dataset2]>
          <Function test_transformers_dataset_request_generator_len[dataset_arg3-None]>
          <Function test_transformers_dataset_request_generator_len[dataset_arg4-None]>
      <Package scheduler>
        <Module test_base.py>
          <Function test_scheduler_result>
          <Function test_scheduler_instantiation[synchronous-None-10-None]>
          <Function test_scheduler_instantiation[throughput-5.0-None-60.0]>
          <Function test_scheduler_instantiation[poisson-10.0-100-None]>
          <Function test_scheduler_instantiation[constant-1.0-None-120.0]>
          <Function test_scheduler_invalid_instantiation[invalid_mode-None-10-None]>
          <Function test_scheduler_invalid_instantiation[synchronous-None-None-None]>
          <Function test_scheduler_invalid_instantiation[synchronous-None--1-10]>
          <Function test_scheduler_invalid_instantiation[synchronous-None-10--1]>
          <Function test_scheduler_invalid_instantiation[constant--1-None-10]>
          <Function test_scheduler_invalid_instantiation[constant-None-None-10]>
          <Function test_scheduler_invalid_instantiation[poisson--1-None-10]>
          <Function test_scheduler_invalid_instantiation[poisson-None-None-10]>
          <Function test_scheduler_run_number[synchronous]>
          <Function test_scheduler_run_number[throughput]>
          <Function test_scheduler_run_number[poisson]>
          <Function test_scheduler_run_number[constant]>
          <Function test_scheduler_run_duration[synchronous]>
          <Function test_scheduler_run_duration[constant]>
        <Module test_load_generator.py>
          <Function test_load_generator_mode>
          <Function test_load_generator_instantiation[constant-10]>
          <Function test_load_generator_instantiation[poisson-5]>
          <Function test_load_generator_instantiation[throughput-None]>
          <Function test_load_generator_instantiation[synchronous-None]>
          <Function test_load_generator_invalid_instantiation[invalid_mode-None-ValueError]>
          <Function test_load_generator_invalid_instantiation[constant-0-ValueError]>
          <Function test_load_generator_invalid_instantiation[poisson--1-ValueError]>
          <Function test_load_generator_times[synchronous-None]>
          <Function test_load_generator_times[throughput-None]>
          <Function test_load_generator_times[constant-1]>
          <Function test_load_generator_times[poisson-5]>
          <Function test_load_generator_invalid_times>
          <Function test_load_generator_throughput_times>
          <Function test_load_generator_constant_times[1]>
          <Function test_load_generator_constant_times[10]>
          <Function test_load_generator_constant_times[42]>
          <Function test_load_generator_poisson_times>
      <Module test_config.py>
        <Function test_default_settings>
        <Function test_settings_from_env_variables>
        <Function test_report_generation_default_source>
        <Function test_logging_settings>
        <Function test_openai_settings>
        <Function test_report_generation_settings>
      <Module test_logger.py>
        <Function test_default_logger_settings>
        <Function test_configure_logger_console_settings>
        <Function test_configure_logger_file_settings>
        <Function test_configure_logger_console_and_file>
        <Function test_environment_variable_override>
        <Function test_logging_disabled>
      <Module test_main.py>
        <Function test_generate_benchmark_report_invoke_smoke>
        <Function test_generate_benchmark_report_cli_smoke>
        <Function test_generate_benchmark_report_emulated_with_dataset_requests>
        <Function test_generate_benchmark_report_cli_emulated_with_dataset_requests>
        <Function test_generate_benchmark_report_openai_limited_by_file_dataset[txt-Test prompt 1-1-constant-1.0]>
          Mock only a few functions to get the proper report result
          from the ``Backend.make_request``.
          
          Notes:
              All the results are collected in the `benchmark.errors``,
              since the most of the responses are mocked and can't be processed.
              But the ordering of the results is still the same for both collections.
          
              ``mock_benchmark_report`` and ``mock_benchmark_report_progress``
              are used for preventing working with IO bound tasks.
        <Function test_generate_benchmark_report_openai_limited_by_file_dataset[txt-Test prompt 1-1-sweep-1.0]>
          Mock only a few functions to get the proper report result
          from the ``Backend.make_request``.
          
          Notes:
              All the results are collected in the `benchmark.errors``,
              since the most of the responses are mocked and can't be processed.
              But the ordering of the results is still the same for both collections.
          
              ``mock_benchmark_report`` and ``mock_benchmark_report_progress``
              are used for preventing working with IO bound tasks.
        <Function test_generate_benchmark_report_openai_limited_by_file_dataset[txt-Test prompt 1\nTest prompt 2\nTest prompt 3\n-3-constant-1.0]>
          Mock only a few functions to get the proper report result
          from the ``Backend.make_request``.
          
          Notes:
              All the results are collected in the `benchmark.errors``,
              since the most of the responses are mocked and can't be processed.
              But the ordering of the results is still the same for both collections.
          
              ``mock_benchmark_report`` and ``mock_benchmark_report_progress``
              are used for preventing working with IO bound tasks.
        <Function test_generate_benchmark_report_openai_limited_by_file_dataset[txt-Test prompt 1\nTest prompt 2\nTest prompt 3\n-3-sweep-1.0]>
          Mock only a few functions to get the proper report result
          from the ``Backend.make_request``.
          
          Notes:
              All the results are collected in the `benchmark.errors``,
              since the most of the responses are mocked and can't be processed.
              But the ordering of the results is still the same for both collections.
          
              ``mock_benchmark_report`` and ``mock_benchmark_report_progress``
              are used for preventing working with IO bound tasks.
      <Package utils>
        <Module test_injector.py>
          <Function test_inject_data>
          <Function test_create_report_to_file>
          <Function test_create_report_to_directory>
        <Module test_progress.py>
          <Function test_initialization>
          <Function test_start_method>
          <Function test_update_benchmark>
          <Function test_finish_method>
          <Function test_error_on_update_completed_benchmark>
          <Function test_multiple_updates>
        <Module test_text.py>
          <Function test_filter_text[hello world-hello-world-hello ]>
          <Function test_filter_text[hello world-world-None-world]>
          <Function test_filter_text[hello world-None-hello-]>
          <Function test_filter_text[hello world-None-None-hello world]>
          <Function test_clean_text[This is\ta test.\n   New line.-True-True-False-False-This is a test.\nNew line.]>
          <Function test_clean_text[This is\ta test.\n   New line.-True-True-True-False-This is a test.\nNew line.]>
          <Function test_clean_text[This is a test. New line.-True-False-False-True-This is a test.\nNew line.]>
          <Function test_split_lines_by_punctuation>
          <Function test_is_url[https://example.com-True]>
          <Function test_is_url[ftp://example.com-True]>
          <Function test_is_url[not a url-False]>
          <Function test_is_path[/repo/tests/unit/utils/test_text.py-True]>
          <Function test_is_path[/non/existent/path-False]>
          <Function test_is_path_like[/repo/tests/unit/utils/test_text.py-True-True]>
          <Function test_is_path_like[/non/existent/path-False-True]>
          <Function test_is_path_like[https://example.com-False-False]>
          <Function test_split_text>
          <Function test_parse_text_objects[text\nline 1\nline 2-csv-expected0]>
          <Function test_parse_text_objects[{"text": "line 1"}\n{"text": "line 2"}-jsonl-expected1]>
          <Function test_load_text[https://example.com-Mock content]>
          <Function test_load_text[/repo/tests/unit/utils/test_text.py-from pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\nimport requests\n\nfrom guidellm.utils.text import (\n    clean_text,\n    filter_text,\n    is_path,\n    is_path_like,\n    is_url,\n    load_text,\n    load_text_lines,\n    parse_text_objects,\n    split_lines_by_punctuation,\n    split_text,\n)\n\n\n@pytest.fixture()\ndef sample_text():\n    return "This is a sample text.\\nThis is another line!"\n\n\n@pytest.fixture()\ndef sample_dict_data():\n    return [{"text": "line 1"}, {"text": "line 2"}, {"text": "line 3"}]\n\n\n@pytest.fixture()\ndef sample_csv_data():\n    return "text\\nline 1\\nline 2\\nline 3"\n\n\n@pytest.fixture()\ndef sample_jsonl_data():\n    return '{"text": "line 1"}\\n{"text": "line 2"}\\n{"text": "line 3"}'\n\n\n@pytest.fixture()\ndef sample_yaml_data():\n    return """\n    text:\n      - line 1\n      - line 2\n      - line 3\n    """\n\n\n@pytest.fixture()\ndef mock_response():\n    response = requests.Response()\n    response.status_code = 200\n    response._content = b"Mock content"\n    return response\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("text", "start", "end", "expected"),\n    [\n        ("hello world", "hello", "world", "hello "),\n        ("hello world", "world", None, "world"),\n        ("hello world", None, "hello", ""),\n        ("hello world", None, None, "hello world"),\n    ],\n)\ndef test_filter_text(text, start, end, expected):\n    assert filter_text(text, start, end) == expected\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    (\n        "text",\n        "fix_encoding",\n        "clean_whitespace",\n        "remove_empty_lines",\n        "force_new_line_punctuation",\n        "expected",\n    ),\n    [\n        (\n            "This is\\ta test.\\n   New line.",\n            True,\n            True,\n            False,\n            False,\n            "This is a test.\\nNew line.",\n        ),\n        (\n            "This is\\ta test.\\n   New line.",\n            True,\n            True,\n            True,\n            False,\n            "This is a test.\\nNew line.",\n        ),\n        (\n            "This is a test. New line.",\n            True,\n            False,\n            False,\n            True,\n            "This is a test.\\nNew line.",\n        ),\n    ],\n)\ndef test_clean_text(\n    text,\n    fix_encoding,\n    clean_whitespace,\n    remove_empty_lines,\n    force_new_line_punctuation,\n    expected,\n):\n    assert (\n        clean_text(\n            text,\n            fix_encoding,\n            clean_whitespace,\n            remove_empty_lines,\n            force_new_line_punctuation,\n        )\n        == expected\n    )\n\n\n@pytest.mark.smoke()\ndef test_split_lines_by_punctuation(sample_text):\n    expected = ["This is a sample text.", "This is another line!"]\n    assert split_lines_by_punctuation(sample_text) == expected\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("url", "expected"),\n    [\n        ("https://example.com", True),\n        ("ftp://example.com", True),\n        ("not a url", False),\n    ],\n)\ndef test_is_url(url, expected):\n    assert is_url(url) == expected\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("path", "expected"),\n    [\n        (str(Path(__file__)), True),\n        ("/non/existent/path", False),\n    ],\n)\ndef test_is_path(path, expected):\n    assert is_path(path) == expected\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("path", "enforce_file", "expected"),\n    [\n        (str(Path(__file__)), True, True),\n        ("/non/existent/path", False, True),\n        ("https://example.com", False, False),\n    ],\n)\ndef test_is_path_like(path, enforce_file, expected):\n    assert is_path_like(path, enforce_file) == expected\n\n\n@pytest.mark.smoke()\ndef test_split_text(sample_text):\n    words, separators, new_lines = split_text(sample_text)\n    assert words == [\n        "This",\n        "is",\n        "a",\n        "sample",\n        "text.",\n        "This",\n        "is",\n        "another",\n        "line!",\n    ]\n    assert separators == [" ", " ", " ", " ", "\\n", " ", " ", " ", " "]\n    assert new_lines == [0, 5]\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("data", "format_", "expected"),\n    [\n        ("text\\nline 1\\nline 2", "csv", [{"text": "line 1"}, {"text": "line 2"}]),\n        (\n            '{"text": "line 1"}\\n{"text": "line 2"}',\n            "jsonl",\n            [{"text": "line 1"}, {"text": "line 2"}],\n        ),\n    ],\n)\ndef test_parse_text_objects(data, format_, expected):\n    assert parse_text_objects(data, format_) == expected\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("data", "expected"),\n    [\n        ("https://example.com", "Mock content"),\n        (str(Path(__file__)), Path(__file__).read_text()),\n    ],\n)\ndef test_load_text(data, expected, mock_response):\n    with patch("requests.get", return_value=mock_response):\n        assert load_text(data) == expected\n\n\n@pytest.mark.regression()\ndef test_load_text_file_not_found():\n    with pytest.raises(FileNotFoundError):\n        load_text("/non/existent/file.txt")\n\n\n@pytest.mark.smoke()\n@pytest.mark.parametrize(\n    ("data", "format_", "filters", "expected"),\n    [\n        ("text\\nline 1\\nline 2", "csv", None, ["line 1", "line 2"]),\n        ('{"text": "line 1"}\\n{"text": "line 2"}', "jsonl", None, ["line 1", "line 2"]),\n        ("text\\nline 1\\nline 2", "txt", None, ["text", "line 1", "line 2"]),\n    ],\n)\ndef test_load_text_lines(data, format_, filters, expected):\n    assert load_text_lines(data, format_=format_, filters=filters) == expected\n\n\n@pytest.mark.regression()\ndef test_load_text_lines_invalid_data():\n    with pytest.raises(ValueError):\n        load_text_lines(123)  # type: ignore\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_invalid_format():\n    with pytest.raises(ValueError):\n        parse_text_objects("text", format_="unsupported")\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_invalid_data():\n    with pytest.raises(ValueError):\n        parse_text_objects(123)  # type: ignore\n\n\n@pytest.mark.regression()\n@pytest.mark.parametrize(\n    ("data", "format_", "filters", "expected"),\n    [\n        (\n            "text\\nline 1\\nline 2\\n",\n            "csv",\n            ["text"],\n            ["line 1", "line 2"],\n        ),\n    ],\n)\ndef test_load_text_lines_with_filters(data, format_, filters, expected):\n    assert load_text_lines(data, format_=format_, filters=filters) == expected\n\n\n@pytest.mark.regression()\ndef test_is_path_with_symlink(tmp_path):\n    # Create a symlink to a temporary file\n    target_file = tmp_path / "target_file.txt"\n    target_file.write_text("Sample content")\n    symlink_path = tmp_path / "symlink"\n    symlink_path.symlink_to(target_file)\n\n    assert is_path(str(symlink_path)) is True\n\n\n@pytest.mark.regression()\ndef test_is_path_like_with_symlink(tmp_path):\n    # Create a symlink to a temporary file\n    target_file = tmp_path / "target_file.txt"\n    target_file.write_text("Sample content")\n    symlink_path = tmp_path / "symlink.file"\n    symlink_path.symlink_to(target_file)\n\n    assert is_path_like(str(symlink_path), enforce_file=True) is True\n\n\n@pytest.mark.regression()\ndef test_load_text_lines_empty():\n    # Test loading text lines from an empty string\n    assert load_text_lines("") == []\n\n\n@pytest.mark.regression()\ndef test_split_text_with_empty_string():\n    words, separators, new_lines = split_text("")\n    assert words == []\n    assert separators == []\n    assert new_lines == []\n\n\n@pytest.mark.regression()\ndef test_split_lines_by_punctuation_with_no_punctuation():\n    text = "This is a test without punctuation"\n    assert split_lines_by_punctuation(text) == [text]\n\n\n@pytest.mark.regression()\ndef test_is_path_invalid_type():\n    assert not is_path(None)\n    assert not is_path(123)\n    assert not is_path(["not", "a", "path"])\n\n\n@pytest.mark.regression()\ndef test_is_path_like_invalid_type():\n    assert not is_path_like(None, enforce_file=False)\n    assert not is_path_like(123, enforce_file=True)\n    assert not is_path_like(["not", "a", "path"], enforce_file=False)\n\n\n@pytest.mark.regression()\ndef test_load_text_invalid_url():\n    with pytest.raises(requests.ConnectionError):\n        load_text("http://invalid.url")\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_empty_csv():\n    assert parse_text_objects("text\\n", "csv") == []\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_empty_jsonl():\n    assert parse_text_objects("", "jsonl") == []\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_invalid_jsonl():\n    with pytest.raises(ValueError):\n        parse_text_objects("{invalid_json}", "jsonl")\n\n\n@pytest.mark.regression()\ndef test_parse_text_objects_empty_yaml():\n    assert parse_text_objects("", "yaml") == []\n\n\n@pytest.mark.regression()\ndef test_clean_text_with_unicode():\n    text = "This is a test with unicode: \\u2013 \\u2014"\n    cleaned_text = clean_text(text, fix_encoding=True, clean_whitespace=True)\n    assert cleaned_text == "This is a test with unicode: \u2013 \u2014"\n\n\n@pytest.mark.regression()\ndef test_split_lines_by_punctuation_with_multiple_punctuations():\n    text = "First sentence. Second sentence? Third sentence!"\n    expected = ["First sentence.", "Second sentence?", "Third sentence!"]\n    assert split_lines_by_punctuation(text) == expected\n\n\n@pytest.mark.regression()\ndef test_is_url_empty_string():\n    assert not is_url("")\n\n\n@pytest.mark.regression()\ndef test_load_text_invalid_data():\n    with pytest.raises(TypeError):\n        load_text(123)  # type: ignore\n\n\n@pytest.mark.regression()\ndef test_load_text_lines_empty_format():\n    data = "text\\nline 1\\nline 2"\n    assert load_text_lines(data, format_="") == ["text", "line 1", "line 2"]\n\n\n@pytest.mark.regression()\ndef test_split_text_with_mixed_separators():\n    text = "This\\tis a test\\nwith mixed separators."\n    words, separators, new_lines = split_text(text)\n    assert words == ["This", "is", "a", "test", "with", "mixed", "separators."]\n    assert separators == ["\\t", " ", " ", "\\n", " ", " ", " "]\n    assert new_lines == [0, 4]\n]>
          <Function test_load_text_file_not_found>
          <Function test_load_text_lines[text\nline 1\nline 2-csv-None-expected0]>
          <Function test_load_text_lines[{"text": "line 1"}\n{"text": "line 2"}-jsonl-None-expected1]>
          <Function test_load_text_lines[text\nline 1\nline 2-txt-None-expected2]>
          <Function test_load_text_lines_invalid_data>
          <Function test_parse_text_objects_invalid_format>
          <Function test_parse_text_objects_invalid_data>
          <Function test_load_text_lines_with_filters[text\nline 1\nline 2\n-csv-filters0-expected0]>
          <Function test_is_path_with_symlink>
          <Function test_is_path_like_with_symlink>
          <Function test_load_text_lines_empty>
          <Function test_split_text_with_empty_string>
          <Function test_split_lines_by_punctuation_with_no_punctuation>
          <Function test_is_path_invalid_type>
          <Function test_is_path_like_invalid_type>
          <Function test_load_text_invalid_url>
          <Function test_parse_text_objects_empty_csv>
          <Function test_parse_text_objects_empty_jsonl>
          <Function test_parse_text_objects_invalid_jsonl>
          <Function test_parse_text_objects_empty_yaml>
          <Function test_clean_text_with_unicode>
          <Function test_split_lines_by_punctuation_with_multiple_punctuations>
          <Function test_is_url_empty_string>
          <Function test_load_text_invalid_data>
          <Function test_load_text_lines_empty_format>
          <Function test_split_text_with_mixed_separators>
        <Module test_transformers.py>
          <Function test_load_transformers_dataset[mock/directory/file.csv-dataset0-train-None-Dataset]>
          <Function test_load_transformers_dataset[mock/directory/file.json-dataset1-None-preferred_splits1-Dataset]>
          <Function test_load_transformers_dataset[mock/directory/file.py-dataset2-None-None-Dataset]>
          <Function test_load_transformers_dataset[dataset_arg3-None-val-None-Dataset]>
          <Function test_load_transformers_dataset[dataset_arg4-None-None-None-Dataset]>
          <Function test_load_transformers_dataset[dataset_arg5-None-None-None-IterableDataset]>
          <Function test_load_transformers_dataset[dataset_arg6-None-validation-None-IterableDataset]>
          <Function test_resolve_transformers_dataset[mock/directory/file.csv-dataset0-train-None-Dataset]>
          <Function test_resolve_transformers_dataset[mock/directory/file.json-dataset1-None-preferred_splits1-DatasetDict]>
          <Function test_resolve_transformers_dataset[mock/directory/file.py-dataset2-None-None-DatasetDict]>
          <Function test_resolve_transformers_dataset[mock/directory/file.unk-dataset3-None-None-DatasetDict]>
          <Function test_resolve_transformers_dataset[dataset_arg4-None-val-None-DatasetDict]>
          <Function test_resolve_transformers_dataset[dataset_arg5-None-None-None-Dataset]>
          <Function test_resolve_transformers_dataset[dataset_arg6-None-None-None-IterableDatasetDict]>
          <Function test_resolve_transformers_dataset[dataset_arg7-None-validation-None-IterableDataset]>
          <Function test_resolve_transformers_dataset_invalid>
          <Function test_resolve_transformers_dataset_split[dataset0-None-None-Dataset]>
          <Function test_resolve_transformers_dataset_split[dataset1-None-None-IterableDataset]>
          <Function test_resolve_transformers_dataset_split[dataset2-validation-None-IterableDataset]>
          <Function test_resolve_transformers_dataset_split_missing>
          <Function test_resolve_transformers_dataset_column[dataset0-None-None-text]>
          <Function test_resolve_transformers_dataset_column[dataset1-text-None-text]>
          <Function test_resolve_transformers_dataset_column[dataset2-None-preferred_columns2-text]>
          <Function test_resolve_transformers_dataset_column[dataset3-None-preferred_columns3-text]>
          <Function test_resolve_transformers_dataset_column[dataset4-None-None-text]>
          <Function test_resolve_transformers_dataset_column_missing>

========================= 345 tests collected in 2.62s =========================