
/repo/src/auralis/models/xttsv2/XTTSv2.py
<<<<<<< SEARCH
        self.init_vllm_engine(self.max_concurrency)
=======
        if torch.cuda.is_available():
            self.init_vllm_engine(self.max_concurrency)
        else:
            self.logger.warning("CUDA is not available. Skipping VLLM engine initialization.")
>>>>>>> REPLACE
