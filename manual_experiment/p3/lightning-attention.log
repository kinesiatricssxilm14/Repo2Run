2024-12-30T09:29:42.181900881Z [?2004hroot@f3ad0414bf1d:/# [7mexport http_proxy= https_proxy= no_proxy=.d.org[27m
[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexport http_proxy= https_proxy= no_proxy=.d.org
2024-12-30T09:29:45.642441956Z [?2004l
[?2004hroot@f3ad0414bf1d:/# [7mmkdir -p ~/.pip && touch ~/.pip/pip.conf[27m
2024-12-30T09:29:46.375154320Z 
[7mecho "[global]" >> ~/.pip/pip.conf && echo "index-url=/simple/" >> ~/.pip/pip.conf && echo "[install]" >> ~/.pip/pip.conf && echo "trusted-host=dpypi.d.org[27m[7m"[27m[7m >> ~/.pip/pip.conf[27m[A[A[Cmkdir -p ~/.pip && touch ~/.pip/pip.conf
2024-12-30T09:29:46.375180561Z 
echo "[global]" >> ~/.pip/pip.conf && echo "index-url=/simple/" >> ~/.pip/pip.conf && echo "[install]" >> ~/.pip/pip.conf && echo "trusted-host=dpypi.d.org" >> ~/.pip/pip.conf
2024-12-30T09:29:50.478737162Z [?2004l
[?2004hroot@f3ad0414bf1d:/# [7mapt-get update && apt-get install -y curl[27m
[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Capt-get update && apt-get install -y curl
2024-12-30T09:29:50.676145960Z [?2004l

0% [Working]
            
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
2024-12-30T09:29:51.006330184Z 
0% [1 InRelease 2048 B/151 kB 1%]
                                 
0% [Working]
0% [Waiting for headers]
                        
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
2024-12-30T09:29:51.096373126Z 
0% [2 InRelease 2051 B/55.4 kB 4%]
                                  
0% [Working]
0% [Waiting for headers]
                        
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
2024-12-30T09:29:51.187171498Z 
0% [3 InRelease 2012 B/48.0 kB 4%]
                                  
0% [Working]
16% [Waiting for headers]
                         
Get:4 http://deb.debian.org/debian bookworm/main amd64 Packages [8789 kB]
2024-12-30T09:29:51.672207917Z 
16% [4 Packages 2038 B/8789 kB 0%]
                                  
91% [Waiting for headers]
91% [4 Packages store 0 B] [Waiting for headers]
                                                
Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages [8856 B]
2024-12-30T09:29:51.811618681Z 
91% [4 Packages store 0 B] [5 Packages 8028 B/8856 B 91%]
                                                         
91% [4 Packages store 0 B] [Waiting for headers]
                                                
Get:6 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [240 kB]
2024-12-30T09:29:52.686249791Z 
92% [4 Packages store 0 B] [6 Packages 45.1 kB/240 kB 19%]
                                                          
93% [4 Packages store 0 B]
93% [4 Packages store 0 B]
                          
96% [Working]
96% [5 Packages store 0 B]
                          
98% [Working]
98% [6 Packages store 0 B]
                          
100% [Working]
              
Fetched 9292 kB in 2s (4285 kB/s)
2024-12-30T09:29:53.194617046Z 
Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 99%

Reading package lists... 99%

Reading package lists... Done

2024-12-30T09:29:53.732670596Z 
Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 0%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 96%

Reading package lists... 99%

Reading package lists... 99%

Reading package lists... Done

2024-12-30T09:29:53.901017790Z 
Building dependency tree... 0%

Building dependency tree... 0%

Building dependency tree... 50%

Building dependency tree... 50%

Building dependency tree... Done

2024-12-30T09:29:53.901890603Z 
Reading state information... 0% 

Reading state information... 0%

Reading state information... Done

2024-12-30T09:29:53.906670589Z curl is already the newest version (7.88.1-10+deb12u8).
2024-12-30T09:29:54.046054039Z 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
2024-12-30T09:29:55.436844506Z [?2004hroot@f3ad0414bf1d:/# [7mpip install pytest[27mpip install pytest
2024-12-30T09:29:57.077191538Z [?2004l
Looking in indexes: /simple/
2024-12-30T09:29:57.211330582Z Collecting pytest
2024-12-30T09:29:57.228591151Z   Downloading /packages/pytest/pytest-8.3.4-py3-none-any.whl (343 kB)
2024-12-30T09:29:57.375075441Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/343.1 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m337.9/343.1 kB[0m [31m25.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m337.9/343.1 kB[0m [31m25.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m337.9/343.1 kB[0m [31m25.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m337.9/343.1 kB[0m [31m25.5 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m343.1/343.1 kB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:29:57.431636705Z [?25hCollecting pluggy<2,>=1.5
2024-12-30T09:29:57.449176106Z   Downloading /packages/pluggy/pluggy-1.5.0-py3-none-any.whl (20 kB)
2024-12-30T09:29:57.549964731Z Collecting tomli>=1
2024-12-30T09:29:57.568748814Z   Downloading /packages/tomli/tomli-2.2.1-py3-none-any.whl (14 kB)
2024-12-30T09:29:57.717645282Z Collecting iniconfig
2024-12-30T09:29:57.736031566Z   Downloading /packages/iniconfig/iniconfig-2.0.0-py3-none-any.whl (5.9 kB)
2024-12-30T09:29:57.808515699Z Collecting exceptiongroup>=1.0.0rc8
2024-12-30T09:29:57.824406422Z   Downloading /packages/exceptiongroup/exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
2024-12-30T09:29:57.943026276Z Collecting packaging
2024-12-30T09:29:57.959961407Z   Downloading /packages/packaging/packaging-24.2-py3-none-any.whl (65 kB)
2024-12-30T09:29:58.051465475Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/65.5 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m61.4/65.5 kB[0m [31m12.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m61.4/65.5 kB[0m [31m12.0 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m65.5/65.5 kB[0m [31m635.0 kB/s[0m eta [36m0:00:00[0m
2024-12-30T09:29:58.138863172Z [?25hInstalling collected packages: tomli, pluggy, packaging, iniconfig, exceptiongroup, pytest
2024-12-30T09:29:59.752306783Z Successfully installed exceptiongroup-1.2.2 iniconfig-2.0.0 packaging-24.2 pluggy-1.5.0 pytest-8.3.4 tomli-2.2.1
2024-12-30T09:29:59.752579835Z [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
2024-12-30T09:29:59.930811626Z [0m
2024-12-30T09:29:59.930831513Z [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.0.1[0m[39;49m -> [0m[32;49m24.3.1[0m
2024-12-30T09:29:59.930835456Z [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m
2024-12-30T09:30:07.347065059Z [?2004hroot@f3ad0414bf1d:/# gti[K[K[Kgit clone [7mhttps://github.com/OpenNLPLab/lightning-attention.git[27m
[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chttps://github.com/OpenNLPLab/lightning-attention.git
2024-12-30T09:30:07.348628034Z [?2004l
Cloning into 'lightning-attention'...
2024-12-30T09:30:08.897656351Z remote: Enumerating objects: 256, done.[K
2024-12-30T09:30:08.903783614Z remote: Counting objects:   0% (1/256)[K
remote: Counting objects:   1% (3/256)[K
remote: Counting objects:   2% (6/256)[K
remote: Counting objects:   3% (8/256)[K
remote: Counting objects:   4% (11/256)[K
remote: Counting objects:   5% (13/256)[K
remote: Counting objects:   6% (16/256)[K
remote: Counting objects:   7% (18/256)[K
remote: Counting objects:   8% (21/256)[K
remote: Counting objects:   9% (24/256)[K
remote: Counting objects:  10% (26/256)[K
remote: Counting objects:  11% (29/256)[K
remote: Counting objects:  12% (31/256)[K
remote: Counting objects:  13% (34/256)[K
remote: Counting objects:  14% (36/256)[K
remote: Counting objects:  15% (39/256)[K
remote: Counting objects:  16% (41/256)[K
remote: Counting objects:  17% (44/256)[K
remote: Counting objects:  18% (47/256)[K
remote: Counting objects:  19% (49/256)[K
remote: Counting objects:  20% (52/256)[K
remote: Counting objects:  21% (54/256)[K
remote: Counting objects:  22% (57/256)[K
remote: Counting objects:  23% (59/256)[K
remote: Counting objects:  24% (62/256)[K
remote: Counting objects:  25% (64/256)[K
remote: Counting objects:  26% (67/256)[K
remote: Counting objects:  27% (70/256)[K
remote: Counting objects:  28% (72/256)[K
remote: Counting objects:  29% (75/256)[K
remote: Counting objects:  30% (77/256)[K
remote: Counting objects:  31% (80/256)[K
remote: Counting objects:  32% (82/256)[K
remote: Counting objects:  33% (85/256)[K
remote: Counting objects:  34% (88/256)[K
remote: Counting objects:  35% (90/256)[K
remote: Counting objects:  36% (93/256)[K
remote: Counting objects:  37% (95/256)[K
remote: Counting objects:  38% (98/256)[K
remote: Counting objects:  39% (100/256)[K
remote: Counting objects:  40% (103/256)[K
remote: Counting objects:  41% (105/256)[K
remote: Counting objects:  42% (108/256)[K
remote: Counting objects:  43% (111/256)[K
remote: Counting objects:  44% (113/256)[K
remote: Counting objects:  45% (116/256)[K
remote: Counting objects:  46% (118/256)[K
remote: Counting objects:  47% (121/256)[K
remote: Counting objects:  48% (123/256)[K
remote: Counting objects:  49% (126/256)[K
remote: Counting objects:  50% (128/256)[K
remote: Counting objects:  51% (131/256)[K
remote: Counting objects:  52% (134/256)[K
remote: Counting objects:  53% (136/256)[K
remote: Counting objects:  54% (139/256)[K
remote: Counting objects:  55% (141/256)[K
remote: Counting objects:  56% (144/256)[K
remote: Counting objects:  57% (146/256)[K
remote: Counting objects:  58% (149/256)[K
remote: Counting objects:  59% (152/256)[K
remote: Counting objects:  60% (154/256)[K
remote: Counting objects:  61% (157/256)[K
remote: Counting objects:  62% (159/256)[K
remote: Counting objects:  63% (162/256)[K
remote: Counting objects:  64% (164/256)[K
remote: Counting objects:  65% (167/256)[K
remote: Counting objects:  66% (169/256)[K
remote: Counting objects:  67% (172/256)[K
remote: Counting objects:  68% (175/256)[K
remote: Counting objects:  69% (177/256)[K
remote: Counting objects:  70% (180/256)[K
remote: Counting objects:  71% (182/256)[K
remote: Counting objects:  72% (185/256)[K
remote: Counting objects:  73% (187/256)[K
remote: Counting objects:  74% (190/256)[K
remote: Counting objects:  75% (192/256)[K
remote: Counting objects:  76% (195/256)[K
remote: Counting objects:  77% (198/256)[K
remote: Counting objects:  78% (200/256)[K
remote: Counting objects:  79% (203/256)[K
remote: Counting objects:  80% (205/256)[K
remote: Counting objects:  81% (208/256)[K
remote: Counting objects:  82% (210/256)[K
remote: Counting objects:  83% (213/256)[K
remote: Counting objects:  84% (216/256)[K
remote: Counting objects:  85% (218/256)[K
remote: Counting objects:  86% (221/256)[K
remote: Counting objects:  87% (223/256)[K
remote: Counting objects:  88% (226/256)[K
remote: Counting objects:  89% (228/256)[K
remote: Counting objects:  90% (231/256)[K
remote: Counting objects:  91% (233/256)[K
remote: Counting objects:  92% (236/256)[K
remote: Counting objects:  93% (239/256)[K
remote: Counting objects:  94% (241/256)[K
remote: Counting objects:  95% (244/256)[K
remote: Counting objects:  96% (246/256)[K
remote: Counting objects:  97% (249/256)[K
remote: Counting objects:  98% (251/256)[K
remote: Counting objects:  99% (254/256)[K
remote: Counting objects: 100% (256/256)[K
remote: Counting objects: 100% (256/256), done.[K
2024-12-30T09:30:08.908624178Z remote: Compressing objects:   0% (1/149)[K
remote: Compressing objects:   1% (2/149)[K
remote: Compressing objects:   2% (3/149)[K
remote: Compressing objects:   3% (5/149)[K
remote: Compressing objects:   4% (6/149)[K
remote: Compressing objects:   5% (8/149)[K
remote: Compressing objects:   6% (9/149)[K
remote: Compressing objects:   7% (11/149)[K
remote: Compressing objects:   8% (12/149)[K
remote: Compressing objects:   9% (14/149)[K
remote: Compressing objects:  10% (15/149)[K
remote: Compressing objects:  11% (17/149)[K
remote: Compressing objects:  12% (18/149)[K
remote: Compressing objects:  13% (20/149)[K
remote: Compressing objects:  14% (21/149)[K
remote: Compressing objects:  15% (23/149)[K
remote: Compressing objects:  16% (24/149)[K
remote: Compressing objects:  17% (26/149)[K
remote: Compressing objects:  18% (27/149)[K
remote: Compressing objects:  19% (29/149)[K
remote: Compressing objects:  20% (30/149)[K
remote: Compressing objects:  21% (32/149)[K
remote: Compressing objects:  22% (33/149)[K
remote: Compressing objects:  23% (35/149)[K
remote: Compressing objects:  24% (36/149)[K
remote: Compressing objects:  25% (38/149)[K
remote: Compressing objects:  26% (39/149)[K
remote: Compressing objects:  27% (41/149)[K
remote: Compressing objects:  28% (42/149)[K
remote: Compressing objects:  29% (44/149)[K
remote: Compressing objects:  30% (45/149)[K
remote: Compressing objects:  31% (47/149)[K
remote: Compressing objects:  32% (48/149)[K
remote: Compressing objects:  33% (50/149)[K
remote: Compressing objects:  34% (51/149)[K
remote: Compressing objects:  35% (53/149)[K
remote: Compressing objects:  36% (54/149)[K
remote: Compressing objects:  37% (56/149)[K
remote: Compressing objects:  38% (57/149)[K
remote: Compressing objects:  39% (59/149)[K
remote: Compressing objects:  40% (60/149)[K
remote: Compressing objects:  41% (62/149)[K
remote: Compressing objects:  42% (63/149)[K
remote: Compressing objects:  43% (65/149)[K
remote: Compressing objects:  44% (66/149)[K
remote: Compressing objects:  45% (68/149)[K
remote: Compressing objects:  46% (69/149)[K
remote: Compressing objects:  47% (71/149)[K
remote: Compressing objects:  48% (72/149)[K
remote: Compressing objects:  49% (74/149)[K
remote: Compressing objects:  50% (75/149)[K
remote: Compressing objects:  51% (76/149)[K
remote: Compressing objects:  52% (78/149)[K
remote: Compressing objects:  53% (79/149)[K
remote: Compressing objects:  54% (81/149)[K
remote: Compressing objects:  55% (82/149)[K
remote: Compressing objects:  56% (84/149)[K
remote: Compressing objects:  57% (85/149)[K
remote: Compressing objects:  58% (87/149)[K
remote: Compressing objects:  59% (88/149)[K
remote: Compressing objects:  60% (90/149)[K
remote: Compressing objects:  61% (91/149)[K
remote: Compressing objects:  62% (93/149)[K
remote: Compressing objects:  63% (94/149)[K
remote: Compressing objects:  64% (96/149)[K
remote: Compressing objects:  65% (97/149)[K
remote: Compressing objects:  66% (99/149)[K
remote: Compressing objects:  67% (100/149)[K
remote: Compressing objects:  68% (102/149)[K
remote: Compressing objects:  69% (103/149)[K
remote: Compressing objects:  70% (105/149)[K
remote: Compressing objects:  71% (106/149)[K
remote: Compressing objects:  72% (108/149)[K
remote: Compressing objects:  73% (109/149)[K
remote: Compressing objects:  74% (111/149)[K
remote: Compressing objects:  75% (112/149)[K
remote: Compressing objects:  76% (114/149)[K
remote: Compressing objects:  77% (115/149)[K
remote: Compressing objects:  78% (117/149)[K
remote: Compressing objects:  79% (118/149)[K
remote: Compressing objects:  80% (120/149)[K
remote: Compressing objects:  81% (121/149)[K
remote: Compressing objects:  82% (123/149)[K
remote: Compressing objects:  83% (124/149)[K
remote: Compressing objects:  84% (126/149)[K
remote: Compressing objects:  85% (127/149)[K
remote: Compressing objects:  86% (129/149)[K
remote: Compressing objects:  87% (130/149)[K
remote: Compressing objects:  88% (132/149)[K
remote: Compressing objects:  89% (133/149)[K
remote: Compressing objects:  90% (135/149)[K
remote: Compressing objects:  91% (136/149)[K
remote: Compressing objects:  92% (138/149)[K
remote: Compressing objects:  93% (139/149)[K
remote: Compressing objects:  94% (141/149)[K
remote: Compressing objects:  95% (142/149)[K
remote: Compressing objects:  96% (144/149)[K
remote: Compressing objects:  97% (145/149)[K
remote: Compressing objects:  98% (147/149)[K
remote: Compressing objects:  99% (148/149)[K
remote: Compressing objects: 100% (149/149)[K
remote: Compressing objects: 100% (149/149), done.[K
2024-12-30T09:30:09.122079378Z Receiving objects:   0% (1/256)
Receiving objects:   1% (3/256)
Receiving objects:   2% (6/256)
Receiving objects:   3% (8/256)
Receiving objects:   4% (11/256)
Receiving objects:   5% (13/256)
Receiving objects:   6% (16/256)
Receiving objects:   7% (18/256)
Receiving objects:   8% (21/256)
Receiving objects:   9% (24/256)
Receiving objects:  10% (26/256)
Receiving objects:  11% (29/256)
Receiving objects:  12% (31/256)
Receiving objects:  13% (34/256)
Receiving objects:  14% (36/256)
Receiving objects:  15% (39/256)
Receiving objects:  16% (41/256)
Receiving objects:  17% (44/256)
Receiving objects:  18% (47/256)
Receiving objects:  19% (49/256)
Receiving objects:  20% (52/256)
Receiving objects:  21% (54/256)
Receiving objects:  22% (57/256)
Receiving objects:  23% (59/256)
Receiving objects:  24% (62/256)
Receiving objects:  25% (64/256)
Receiving objects:  26% (67/256)
Receiving objects:  27% (70/256)
Receiving objects:  28% (72/256)
Receiving objects:  29% (75/256)
Receiving objects:  30% (77/256)
Receiving objects:  31% (80/256)
Receiving objects:  32% (82/256)
Receiving objects:  33% (85/256)
Receiving objects:  34% (88/256)
Receiving objects:  35% (90/256)
Receiving objects:  36% (93/256)
Receiving objects:  37% (95/256)
Receiving objects:  38% (98/256)
Receiving objects:  39% (100/256)
Receiving objects:  40% (103/256)
Receiving objects:  41% (105/256)
Receiving objects:  42% (108/256)
Receiving objects:  43% (111/256)
Receiving objects:  44% (113/256)
Receiving objects:  45% (116/256)
Receiving objects:  46% (118/256)
Receiving objects:  47% (121/256)
Receiving objects:  48% (123/256)
Receiving objects:  49% (126/256)
Receiving objects:  50% (128/256)
Receiving objects:  51% (131/256)
Receiving objects:  52% (134/256)
Receiving objects:  53% (136/256)
Receiving objects:  54% (139/256)
Receiving objects:  55% (141/256)
Receiving objects:  56% (144/256)
Receiving objects:  57% (146/256)
Receiving objects:  58% (149/256)
Receiving objects:  59% (152/256)
Receiving objects:  60% (154/256)
Receiving objects:  61% (157/256)
Receiving objects:  62% (159/256)
Receiving objects:  63% (162/256)
Receiving objects:  64% (164/256)
Receiving objects:  65% (167/256)
Receiving objects:  66% (169/256)
Receiving objects:  67% (172/256)
Receiving objects:  68% (175/256)
Receiving objects:  69% (177/256)
Receiving objects:  70% (180/256)
Receiving objects:  71% (182/256)
Receiving objects:  72% (185/256)
Receiving objects:  73% (187/256)
Receiving objects:  74% (190/256)
Receiving objects:  75% (192/256)
Receiving objects:  76% (195/256)
Receiving objects:  77% (198/256)
Receiving objects:  78% (200/256)
remote: Total 256 (delta 109), reused 231 (delta 86), pack-reused 0 (from 0)[K
2024-12-30T09:30:09.122331284Z Receiving objects:  79% (203/256)
Receiving objects:  80% (205/256)
Receiving objects:  81% (208/256)
Receiving objects:  82% (210/256)
Receiving objects:  83% (213/256)
Receiving objects:  84% (216/256)
Receiving objects:  85% (218/256)
Receiving objects:  86% (221/256)
Receiving objects:  87% (223/256)
Receiving objects:  88% (226/256)
Receiving objects:  89% (228/256)
Receiving objects:  90% (231/256)
Receiving objects:  91% (233/256)
Receiving objects:  92% (236/256)
Receiving objects:  93% (239/256)
Receiving objects:  94% (241/256)
Receiving objects:  95% (244/256)
Receiving objects:  96% (246/256)
Receiving objects:  97% (249/256)
Receiving objects:  98% (251/256)
Receiving objects:  99% (254/256)
Receiving objects: 100% (256/256)
Receiving objects: 100% (256/256), 43.14 KiB | 222.00 KiB/s, done.
2024-12-30T09:30:09.124544767Z Resolving deltas:   0% (0/109)
Resolving deltas:   1% (2/109)
Resolving deltas:   2% (3/109)
Resolving deltas:   3% (4/109)
Resolving deltas:   4% (5/109)
Resolving deltas:   5% (6/109)
Resolving deltas:   6% (7/109)
Resolving deltas:   7% (8/109)
Resolving deltas:   8% (9/109)
Resolving deltas:   9% (10/109)
Resolving deltas:  11% (12/109)
Resolving deltas:  12% (14/109)
Resolving deltas:  13% (15/109)
Resolving deltas:  15% (17/109)
Resolving deltas:  16% (18/109)
Resolving deltas:  17% (19/109)
Resolving deltas:  18% (20/109)
Resolving deltas:  20% (22/109)
Resolving deltas:  21% (23/109)
Resolving deltas:  23% (26/109)
Resolving deltas:  24% (27/109)
Resolving deltas:  25% (28/109)
Resolving deltas:  26% (29/109)
Resolving deltas:  27% (30/109)
Resolving deltas:  28% (31/109)
Resolving deltas:  30% (33/109)
Resolving deltas:  31% (34/109)
Resolving deltas:  32% (35/109)
Resolving deltas:  33% (36/109)
Resolving deltas:  36% (40/109)
Resolving deltas:  38% (42/109)
Resolving deltas:  40% (44/109)
Resolving deltas:  41% (45/109)
Resolving deltas:  43% (47/109)
Resolving deltas:  44% (48/109)
Resolving deltas:  47% (52/109)
Resolving deltas:  50% (55/109)
Resolving deltas:  51% (56/109)
Resolving deltas:  52% (57/109)
Resolving deltas:  53% (58/109)
Resolving deltas:  54% (59/109)
Resolving deltas:  55% (60/109)
Resolving deltas:  56% (62/109)
Resolving deltas:  57% (63/109)
Resolving deltas:  58% (64/109)
Resolving deltas:  59% (65/109)
Resolving deltas:  60% (66/109)
Resolving deltas:  62% (68/109)
Resolving deltas:  63% (69/109)
Resolving deltas:  64% (70/109)
Resolving deltas:  65% (71/109)
Resolving deltas:  66% (72/109)
Resolving deltas:  67% (74/109)
Resolving deltas:  68% (75/109)
Resolving deltas:  70% (77/109)
Resolving deltas:  71% (78/109)
Resolving deltas:  72% (79/109)
Resolving deltas:  74% (81/109)
Resolving deltas:  75% (82/109)
Resolving deltas:  76% (83/109)
Resolving deltas:  77% (84/109)
Resolving deltas:  78% (86/109)
Resolving deltas:  79% (87/109)
Resolving deltas:  81% (89/109)
Resolving deltas:  83% (91/109)
Resolving deltas:  84% (92/109)
Resolving deltas:  85% (93/109)
Resolving deltas:  86% (94/109)
Resolving deltas:  87% (95/109)
Resolving deltas:  88% (96/109)
Resolving deltas:  90% (99/109)
Resolving deltas:  92% (101/109)
Resolving deltas:  93% (102/109)
Resolving deltas:  94% (103/109)
Resolving deltas:  95% (104/109)
Resolving deltas:  96% (105/109)
Resolving deltas:  97% (106/109)
Resolving deltas:  98% (107/109)
Resolving deltas:  99% (108/109)
Resolving deltas: 100% (109/109)
Resolving deltas: 100% (109/109), done.
2024-12-30T09:30:16.925988808Z [?2004hroot@f3ad0414bf1d:/# cd li
2024-12-30T09:30:16.926015851Z lib/                 lib64/               lightning-attention/ 
2024-12-30T09:30:16.966617823Z root@f3ad0414bf1d:/# cd li
2024-12-30T09:30:16.966640790Z lib/                 lib64/               lightning-attention/ 
2024-12-30T09:30:19.371479632Z root@f3ad0414bf1d:/# cd lightning-attention/
2024-12-30T09:30:22.681083311Z [?2004l
[?2004hroot@f3ad0414bf1d:/lightning-attention# git checkout [7md7439519541e966084eeaaf3ffd63eecc216f414[27md7439519541e966084eeaaf3ffd63eecc216f414
2024-12-30T09:30:22.684692378Z [?2004l
Note: switching to 'd7439519541e966084eeaaf3ffd63eecc216f414'.
2024-12-30T09:30:22.684712673Z 
2024-12-30T09:30:22.684716604Z You are in 'detached HEAD' state. You can look around, make experimental
2024-12-30T09:30:22.684719945Z changes and commit them, and you can discard any commits you make in this
2024-12-30T09:30:22.684723318Z state without impacting any branches by switching back to a branch.
2024-12-30T09:30:22.684726578Z 
2024-12-30T09:30:22.684729993Z If you want to create a new branch to retain commits you create, you may
2024-12-30T09:30:22.684733162Z do so (now or later) by using -c with the switch command. Example:
2024-12-30T09:30:22.684736547Z 
2024-12-30T09:30:22.684739589Z   git switch -c <new-branch-name>
2024-12-30T09:30:22.684743033Z 
2024-12-30T09:30:22.684746785Z Or undo this operation with:
2024-12-30T09:30:22.684749041Z 
2024-12-30T09:30:22.684750898Z   git switch -
2024-12-30T09:30:22.684752836Z 
2024-12-30T09:30:22.684754684Z Turn off this advice by setting config variable advice.detachedHead to false
2024-12-30T09:30:22.684756713Z 
2024-12-30T09:30:22.684758611Z HEAD is now at d743951 update setup
2024-12-30T09:30:46.528336534Z [?2004hroot@f3ad0414bf1d:/lightning-attention# ls
2024-12-30T09:30:46.530529006Z [?2004l
LICENSE  README.md  benchmarks	examples  lightning_attn  setup.py  tests
2024-12-30T09:30:50.306681237Z [?2004hroot@f3ad0414bf1d:/lightning-attention# pip install .
2024-12-30T09:30:50.663560268Z [?2004l
Looking in indexes: /simple/
2024-12-30T09:30:50.670355368Z Processing /lightning-attention
2024-12-30T09:30:51.191460062Z   Preparing metadata (setup.py) ... [?25l- done
2024-12-30T09:30:51.354724110Z [?25hCollecting torch
2024-12-30T09:30:51.376015622Z   Downloading /packages/torch/torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)
2024-12-30T09:30:54.005368466Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/906.4 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.5/906.4 MB[0m [31m76.5 MB/s[0m eta [36m0:00:12[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.6/906.4 MB[0m [31m237.4 MB/s[0m eta [36m0:00:04[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.0/906.4 MB[0m [31m267.9 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m30.5/906.4 MB[0m [31m266.3 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m39.9/906.4 MB[0m [31m267.0 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.4/906.4 MB[0m [31m268.5 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m58.6/906.4 MB[0m [31m263.4 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m68.0/906.4 MB[0m [31m265.3 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m77.3/906.4 MB[0m [31m263.5 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m86.7/906.4 MB[0m [31m266.9 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m96.1/906.4 MB[0m [31m266.5 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m105.4/906.4 MB[0m [31m264.2 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m114.8/906.4 MB[0m [31m266.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m124.3/906.4 MB[0m [31m268.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m133.7/906.4 MB[0m [31m267.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m143.2/906.4 MB[0m [31m268.7 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m152.6/906.4 MB[0m [31m264.7 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m162.0/906.4 MB[0m [31m266.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m171.4/906.4 MB[0m [31m267.5 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m181.0/906.4 MB[0m [31m270.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m190.5/906.4 MB[0m [31m270.5 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.1/906.4 MB[0m [31m270.0 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m209.6/906.4 MB[0m [31m268.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m219.2/906.4 MB[0m [31m272.7 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m228.8/906.4 MB[0m [31m272.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m238.4/906.4 MB[0m [31m271.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m247.6/906.4 MB[0m [31m263.1 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m256.9/906.4 MB[0m [31m262.4 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m266.2/906.4 MB[0m [31m265.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m275.6/906.4 MB[0m [31m265.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m285.0/906.4 MB[0m [31m268.1 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m294.3/906.4 MB[0m [31m264.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m303.7/906.4 MB[0m [31m265.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m313.1/906.4 MB[0m [31m266.7 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m322.5/906.4 MB[0m [31m266.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m331.9/906.4 MB[0m [31m267.5 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m340.0/906.4 MB[0m [31m249.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m345.2/906.4 MB[0m [31m176.3 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m354.5/906.4 MB[0m [31m264.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m364.0/906.4 MB[0m [31m269.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m373.4/906.4 MB[0m [31m266.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m382.9/906.4 MB[0m [31m269.2 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m392.4/906.4 MB[0m [31m270.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m402.0/906.4 MB[0m [31m269.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m411.6/906.4 MB[0m [31m272.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m420.9/906.4 MB[0m [31m265.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m430.4/906.4 MB[0m [31m263.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m440.0/906.4 MB[0m [31m270.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m449.6/906.4 MB[0m [31m271.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m459.1/906.4 MB[0m [31m268.2 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m468.7/906.4 MB[0m [31m271.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m478.1/906.4 MB[0m [31m267.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m487.8/906.4 MB[0m [31m273.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m497.2/906.4 MB[0m [31m267.7 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m506.7/906.4 MB[0m [31m268.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m516.2/906.4 MB[0m [31m265.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m525.8/906.4 MB[0m [31m273.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m535.4/906.4 MB[0m [31m270.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m545.2/906.4 MB[0m [31m277.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m554.9/906.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m564.5/906.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m574.1/906.4 MB[0m [31m272.7 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m583.9/906.4 MB[0m [31m275.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m593.2/906.4 MB[0m [31m266.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m602.6/906.4 MB[0m [31m264.8 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m612.1/906.4 MB[0m [31m270.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m621.8/906.4 MB[0m [31m274.2 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m631.5/906.4 MB[0m [31m273.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m641.0/906.4 MB[0m [31m268.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m650.4/906.4 MB[0m [31m266.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m659.6/906.4 MB[0m [31m253.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m668.8/906.4 MB[0m [31m260.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m678.4/906.4 MB[0m [31m273.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━2024-12-30T09:30:54.005368466Z ━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m688.2/906.4 MB[0m [31m274.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m697.9/906.4 MB[0m [31m273.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m707.4/906.4 MB[0m [31m269.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m716.0/906.4 MB[0m [31m248.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m725.7/906.4 MB[0m [31m275.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m735.2/906.4 MB[0m [31m269.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m744.9/906.4 MB[0m [31m271.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━[0m [32m754.6/906.4 MB[0m [31m273.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m763.9/906.4 MB[0m [31m265.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m773.4/906.4 MB[0m [31m266.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m782.7/906.4 MB[0m [31m262.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m792.4/906.4 MB[0m [31m273.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m801.9/906.4 MB[0m [31m270.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m811.8/906.4 MB[0m [31m276.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m821.3/906.4 MB[0m [31m271.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m831.0/906.4 MB[0m [31m272.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m840.7/906.4 MB[0m [31m273.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m850.2/906.4 MB[0m [31m269.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m859.9/906.4 MB[0m [31m273.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m869.6/906.4 MB[0m [31m274.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m879.1/906.4 MB[0m [31m269.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m888.6/906.4 MB[0m [31m270.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m898.4/906.4 MB[0m [31m273.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━�2024-12-30T09:30:54.005368466Z ��━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━��2024-12-30T09:30:54.005368466Z �━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K 2024-12-30T09:30:54.005368466Z     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m2024-12-30T09:30:54.005368466Z 0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 M2024-12-30T09:30:54.005368466Z B/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m906.4/906.4 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m906.4/906.4 MB[0m [31m675.6 kB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:11.236806954Z [?25hCollecting einops
2024-12-30T09:31:11.253685611Z   Downloading /packages/einops/einops-0.8.0-py3-none-any.whl (43 kB)
2024-12-30T09:31:11.264127048Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/43.2 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m43.2/43.2 kB[0m [31m5.5 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:11.307248134Z [?25hCollecting triton
2024-12-30T09:31:11.326825768Z   Downloading /packages/triton/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)
2024-12-30T09:31:12.842504972Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/209.5 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.7/209.5 MB[0m [31m80.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m10.4/209.5 MB[0m [31m167.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m18.4/209.5 MB[0m [31m226.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m26.3/209.5 MB[0m [31m226.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m34.4/209.5 MB[0m [31m230.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m42.5/209.5 MB[0m [31m230.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m50.5/209.5 MB[0m [31m228.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m58.7/209.5 MB[0m [31m230.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m66.4/209.5 MB[0m [31m222.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m74.2/209.5 MB[0m [31m218.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m82.0/209.5 MB[0m [31m225.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m89.8/209.5 MB[0m [31m221.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m97.9/209.5 MB[0m [31m227.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m106.1/209.5 MB[0m [31m233.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m114.1/209.5 MB[0m [31m228.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m122.3/209.5 MB[0m [31m232.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m130.3/209.5 MB[0m [31m228.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m138.2/209.5 MB[0m [31m225.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m147.7/209.5 MB[0m [31m263.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m157.3/209.5 MB[0m [31m270.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m167.0/209.5 MB[0m [31m271.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m176.8/209.5 MB[0m [31m275.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m186.6/209.5 MB[0m [31m274.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m196.5/209.5 MB[0m [31m278.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m206.6/209.5 MB[0m [31m282.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m209.5/209.5 MB[0m [31m286.2 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m209.5/209.5 MB[0m [31m15.8 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:13.179263904Z [?25hCollecting nvidia-curand-cu12==10.3.5.147
2024-12-30T09:31:13.197815793Z   Downloading /packages/nvidia-curand-cu12/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
2024-12-30T09:31:13.581710164Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/56.3 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.1/56.3 MB[0m [31m153.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.7/56.3 MB[0m [31m271.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.5/56.3 MB[0m [31m274.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m34.3/56.3 MB[0m [31m274.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m44.0/56.3 MB[0m [31m273.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m54.0/56.3 MB[0m [31m280.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m285.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m285.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m285.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m285.9 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.3/56.3 MB[0m [31m53.0 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:13.688380351Z [?25hCollecting nvidia-cusparse-cu12==12.3.1.170
2024-12-30T09:31:13.704450808Z   Downloading /packages/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
2024-12-30T09:31:15.191501784Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/207.5 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.2/207.5 MB[0m [31m125.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.7/207.5 MB[0m [31m266.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m23.4/207.5 MB[0m [31m272.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m32.9/207.5 MB[0m [31m270.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m42.7/207.5 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m51.9/207.5 MB[0m [31m260.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m59.5/207.5 MB[0m [31m225.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m67.8/207.5 MB[0m [31m232.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m76.2/207.5 MB[0m [31m237.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m84.2/207.5 MB[0m [31m229.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m92.7/207.5 MB[0m [31m239.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m101.2/207.5 MB[0m [31m241.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m109.8/207.5 MB[0m [31m243.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m118.4/207.5 MB[0m [31m243.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m127.1/207.5 MB[0m [31m242.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m135.5/207.5 MB[0m [31m233.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m144.0/207.5 MB[0m [31m236.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m152.7/207.5 MB[0m [31m242.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m160.2/207.5 MB[0m [31m221.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m168.6/207.5 MB[0m [31m240.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m177.5/207.5 MB[0m [31m246.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m187.2/207.5 MB[0m [31m272.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m197.1/207.5 MB[0m [31m277.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.3/207.5 MB[0m [31m286.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m289.3 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m207.5/207.5 MB[0m [31m15.5 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:15.495826893Z [?25hCollecting sympy==1.13.1
2024-12-30T09:31:15.513062078Z   Downloading /packages/sympy/sympy-1.13.1-py3-none-any.whl (6.2 MB)
2024-12-30T09:31:15.580027433Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/6.2 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m4.2/6.2 MB[0m [31m125.6 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.2/6.2 MB[0m [31m98.4 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:15.637445124Z [?25hCollecting nvidia-cudnn-cu12==9.1.0.70
2024-12-30T09:31:15.670251016Z   Downloading /packages/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
2024-12-30T09:31:18.383390130Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/664.8 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.1/664.8 MB[0m [31m2.8 MB/s[0m eta [36m0:03:59[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.2/664.8 MB[0m [31m3.3 MB/s[0m eta [36m0:03:21[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.4/664.8 MB[0m [31m4.0 MB/s[0m eta [36m0:02:47[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.6/664.8 MB[0m [31m4.5 MB/s[0m eta [36m0:02:27[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.9/664.8 MB[0m [31m5.1 MB/s[0m eta [36m0:02:10[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.2/664.8 MB[0m [31m5.8 MB/s[0m eta [36m0:01:55[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.5/664.8 MB[0m [31m6.3 MB/s[0m eta [36m0:01:46[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.9/664.8 MB[0m [31m7.0 MB/s[0m eta [36m0:01:35[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.4/664.8 MB[0m [31m7.6 MB/s[0m eta [36m0:01:27[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.9/664.8 MB[0m [31m8.3 MB/s[0m eta [36m0:01:20[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.4/664.8 MB[0m [31m9.0 MB/s[0m eta [36m0:01:14[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.1/664.8 MB[0m [31m9.8 MB/s[0m eta [36m0:01:08[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.7/664.8 MB[0m [31m10.4 MB/s[0m eta [36m0:01:04[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.4/664.8 MB[0m [31m11.0 MB/s[0m eta [36m0:01:00[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.2/664.8 MB[0m [31m11.8 MB/s[0m eta [36m0:00:56[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.0/664.8 MB[0m [31m12.6 MB/s[0m eta [36m0:00:53[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.8/664.8 MB[0m [31m13.2 MB/s[0m eta [36m0:00:50[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.7/664.8 MB[0m [31m13.8 MB/s[0m eta [36m0:00:48[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m9.7/664.8 MB[0m [31m14.5 MB/s[0m eta [36m0:00:46[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m10.7/664.8 MB[0m [31m17.3 MB/s[0m eta [36m0:00:38[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.7/664.8 MB[0m [31m20.5 MB/s[0m eta [36m0:00:32[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m12.9/664.8 MB[0m [31m23.2 MB/s[0m eta [36m0:00:29[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.0/664.8 MB[0m [31m25.1 MB/s[0m eta [36m0:00:26[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m15.3/664.8 MB[0m [31m27.6 MB/s[0m eta [36m0:00:24[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m16.5/664.8 MB[0m [31m29.0 MB/s[0m eta [36m0:00:23[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m17.7/664.8 MB[0m [31m30.5 MB/s[0m eta [36m0:00:22[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m18.9/664.8 MB[0m [31m32.2 MB/s[0m eta [36m0:00:21[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m20.2/664.8 MB[0m [31m33.3 MB/s[0m eta [36m0:00:20[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.4/664.8 MB[0m [31m33.8 MB/s[0m eta [36m0:00:20[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m22.8/664.8 MB[0m [31m35.0 MB/s[0m eta [36m0:00:19[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.3/664.8 MB[0m [31m36.3 MB/s[0m eta [36m0:00:18[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m25.9/664.8 MB[0m [31m37.6 MB/s[0m eta [36m0:00:18[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m27.4/664.8 MB[0m [31m38.8 MB/s[0m eta [36m0:00:17[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m29.2/664.8 MB[0m [31m41.2 MB/s[0m eta [36m0:00:16[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m30.8/664.8 MB[0m [31m42.8 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m32.4/664.8 MB[0m [31m44.6 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m33.8/664.8 MB[0m [31m44.5 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m35.1/664.8 MB[0m [31m43.4 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m36.6/664.8 MB[0m [31m43.3 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m38.5/664.8 MB[0m [31m45.2 MB/s[0m eta [36m0:00:14[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m40.2/664.8 MB[0m [31m44.2 MB/s[0m eta [36m0:00:15[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m42.2/664.8 MB[0m [31m45.2 MB/s[0m eta [36m0:00:14[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m43.8/664.8 MB[0m [31m47.0 MB/s[0m eta [36m0:00:14[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.8/664.8 MB[0m [31m49.6 MB/s[0m eta [36m0:00:13[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m47.6/664.8 MB[0m [31m51.3 MB/s[0m eta [36m0:00:13[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.9/664.8 MB[0m [31m53.6 MB/s[0m eta [36m0:00:12[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m52.2/664.8 MB[0m [31m56.3 MB/s[0m eta [36m0:00:11[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m54.6/664.8 MB[0m [31m60.6 MB/s[0m eta [36m0:00:11[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.8/664.8 MB[0m [31m63.3 MB/s[0m eta [36m0:00:10[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m59.1/664.8 MB[0m [31m64.3 MB/s[0m eta [36m0:00:10[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m61.4/664.8 MB[0m [31m63.8 MB/s[0m eta [36m0:00:10[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m63.6/664.8 MB[0m [31m64.3 MB/s[0m eta [36m0:00:10[0m
[2K     [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m66.3/664.8 MB[0m [31m65.9 MB/s[0m eta [36m0:00:10[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m68.9/664.8 MB[0m [31m68.4 MB/s[0m eta [36m0:00:09[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m71.4/664.8 MB[0m [31m70.5 MB/s[0m eta [36m0:00:09[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m74.2/664.8 MB[0m [31m74.9 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m77.1/664.8 MB[0m [31m76.9 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m79.8/664.8 MB[0m [31m78.0 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m82.6/664.8 MB[0m [31m79.3 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m85.6/664.8 MB[0m [31m80.0 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m88.5/664.8 MB[0m [31m80.5 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m91.4/664.8 MB[0m [31m80.8 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m94.1/664.8 MB[0m [31m80.6 MB/s[0m eta [36m0:00:08[0m
[2K     [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m97.2/664.8 MB[0m [31m81.9 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m100.1/664.8 MB[0m [31m82.6 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m102.9/664.8 MB[0m [31m83.0 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m105.9/664.8 MB[0m [31m82.3 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m109.3/664.8 MB[0m [31m84.3 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m112.9/664.8 MB[0m [31m90.1 MB/s[0m eta [36m0:00:07[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m116.3/664.8 MB[0m [31m98.0 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m119.8/664.8 MB[0m [31m98.6 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m123.2/664.8 MB[0m [31m98.2 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m126.6/664.8 MB[0m [31m98.1 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m130.1/664.8 MB[0m [31m98.3 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m133.6/664.8 MB[0m [31m98.5 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━�2024-12-30T09:31:18.383390130Z ��━━━━━━━━━━━━━━━━━━━━━[0m [32m137.1/664.8 MB[0m [31m98.4 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m140.4/664.8 MB[0m [31m97.8 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m143.9/664.8 MB[0m [31m97.8 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m147.9/664.8 MB[0m [31m102.3 MB/s[0m eta [36m0:00:06[0m
[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m152.1/664.8 MB[0m [31m110.6 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m156.0/664.8 MB[0m [31m112.2 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m159.8/664.8 MB[0m [31m109.8 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m163.9/664.8 MB[0m [31m111.0 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m167.9/664.8 MB[0m [31m111.5 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m172.0/664.8 MB[0m [31m113.7 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m176.0/664.8 MB[0m [31m113.3 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m180.0/664.8 MB[0m [31m113.0 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m183.9/664.8 MB[0m [31m111.4 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m187.8/664.8 MB[0m [31m112.1 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m191.2/664.8 MB[0m [31m117.3 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m195.4/664.8 MB[0m [31m107.6 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m200.0/664.8 MB[0m [31m111.2 MB/s[0m eta [36m0:00:05[0m
[2K     [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m204.6/664.8 MB[0m [31m130.1 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m209.3/664.8 MB[0m [31m130.6 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m213.8/664.8 MB[0m [31m127.9 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m217.9/664.8 MB[0m [31m123.6 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m222.3/664.8 MB[0m [31m122.3 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m227.2/664.8 MB[0m [31m132.0 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m231.6/664.8 MB[0m [31m132.0 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m236.4/664.8 MB[0m [31m129.1 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m241.4/664.8 MB[0m [31m136.2 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m246.1/664.8 MB[0m [31m135.5 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m251.2/664.8 MB[0m [31m139.9 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m256.3/664.8 MB[0m [31m140.1 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m261.3/664.8 MB[0m [31m141.0 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m266.5/664.8 MB[0m [31m148.4 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m271.7/664.8 MB[0m [31m149.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m276.8/664.8 MB[0m [31m148.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m281.9/664.8 MB[0m [31m144.0 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m287.1/664.8 MB[0m [31m143.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m292.1/664.8 MB[0m [31m141.4 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m297.3/664.8 MB[0m [31m150.4 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m303.0/664.8 MB[0m [31m154.2 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m308.6/664.8 MB[0m [31m157.4 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m314.3/664.8 MB[0m [31m159.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m319.4/664.8 MB[0m [31m152.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m325.1/664.8 MB[0m [31m152.9 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m330.8/664.8 MB[0m [31m161.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m336.6/664.8 MB[0m [31m161.9 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m342.3/664.8 MB[0m [31m158.6 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m347.9/664.8 MB[0m [31m158.1 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m353.5/664.8 MB[0m [31m156.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m359.3/664.8 MB[0m [31m159.8 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m365.6/664.8 MB[0m [31m171.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m371.7/664.8 MB[0m [31m173.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m377.3/664.8 MB[0m [31m164.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m383.3/664.8 MB[0m [31m163.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m388.9/664.8 MB[0m [31m160.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m394.7/664.8 MB[0m [31m159.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m400.4/664.8 MB[0m [31m161.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m406.3/664.8 MB[0m [31m161.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m412.1/664.8 MB[0m [31m165.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m416.3/664.8 MB[0m [31m138.8 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m420.5/664.8 MB[0m [31m124.2 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m425.1/664.8 MB[0m [31m120.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m429.1/664.8 MB[0m [31m119.8 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m433.1/664.8 MB[0m [31m114.8 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m437.4/664.8 MB[0m [31m114.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m441.1/664.8 MB[0m [31m111.0 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m445.2/664.8 MB[0m [31m112.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m449.5/664.8 MB[0m [31m120.7 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m453.7/664.8 MB[0m [31m119.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m458.3/664.8 MB[0m [31m122.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m463.0/664.8 MB[0m [31m129.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m468.1/664.8 MB[0m [31m137.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m473.0/664.8 MB[0m [31m141.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m478.1/664.8 MB[0m [31m142.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m483.2/664.8 MB[0m [31m144.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m488.1/664.8 MB[0m [31m139.3 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━��2024-12-30T09:31:18.383390130Z �━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m493.5/664.8 MB[0m [31m149.1 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m498.5/664.8 MB[0m [31m150.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m503.6/664.8 MB[0m [31m140.8 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m508.3/664.8 MB[0m [31m135.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m513.8/664.8 MB[0m [31m143.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m519.5/664.8 MB[0m [31m158.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m525.3/664.8 MB[0m [31m162.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m531.0/664.8 MB[0m [31m162.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m536.7/664.8 MB[0m [31m159.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m542.5/664.8 MB[0m [31m159.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m548.1/664.8 MB[0m [31m159.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━[0m [32m553.6/664.8 MB[0m [31m158.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m559.4/664.8 MB[0m [31m159.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m565.1/664.8 MB[0m [31m158.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m570.8/664.8 MB[0m [31m160.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m577.0/664.8 MB[0m [31m170.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m583.1/664.8 MB[0m [31m171.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m589.4/664.8 MB[0m [31m171.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m595.6/664.8 MB[0m [31m174.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m601.7/664.8 MB[0m [31m171.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m607.8/664.8 MB[0m [31m170.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m614.2/664.8 MB[0m [31m176.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m620.2/664.8 MB[0m [31m173.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m626.8/664.8 MB[0m [31m188.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m632.9/664.8 MB[0m [31m179.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m639.2/664.8 MB[0m [31m176.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m646.0/664.8 MB[0m [31m188.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m653.0/664.8 MB[0m [31m194.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m659.7/664.8 MB[0m [31m192.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m2024-12-30T09:31:18.383390130Z 
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta2024-12-30T09:31:18.383390130Z  [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m180.3 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m664.8/664.8 MB[0m [31m1.8 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:28.589445968Z [?25hCollecting nvidia-cuda-runtime-cu12==12.4.127
2024-12-30T09:31:28.605983716Z   Downloading /packages/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
2024-12-30T09:31:28.637799510Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/883.7 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m883.7/883.7 kB[0m [31m32.2 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:28.678264544Z [?25hCollecting nvidia-cublas-cu12==12.4.5.8
2024-12-30T09:31:28.694933127Z   Downloading /packages/nvidia-cublas-cu12/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
2024-12-30T09:31:31.535701665Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/363.4 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.6/363.4 MB[0m [31m107.8 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m12.4/363.4 MB[0m [31m250.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.6/363.4 MB[0m [31m258.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m30.9/363.4 MB[0m [31m262.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m40.0/363.4 MB[0m [31m258.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.0/363.4 MB[0m [31m254.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m58.0/363.4 MB[0m [31m254.9 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m67.2/363.4 MB[0m [31m260.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m76.1/363.4 MB[0m [31m252.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m85.2/363.4 MB[0m [31m257.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m94.3/363.4 MB[0m [31m255.4 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m103.2/363.4 MB[0m [31m250.6 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m112.1/363.4 MB[0m [31m254.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m121.2/363.4 MB[0m [31m256.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m130.2/363.4 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m139.0/363.4 MB[0m [31m249.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m148.3/363.4 MB[0m [31m258.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m156.5/363.4 MB[0m [31m237.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m164.0/363.4 MB[0m [31m208.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m173.2/363.4 MB[0m [31m257.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m182.6/363.4 MB[0m [31m265.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m192.2/363.4 MB[0m [31m269.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m200.2/363.4 MB[0m [31m234.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m206.5/363.4 MB[0m [31m186.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m213.1/363.4 MB[0m [31m186.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m222.6/363.4 MB[0m [31m268.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m230.7/363.4 MB[0m [31m238.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m239.9/363.4 MB[0m [31m246.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m249.4/363.4 MB[0m [31m268.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m258.8/363.4 MB[0m [31m267.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m268.5/363.4 MB[0m [31m271.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m278.0/363.4 MB[0m [31m269.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m287.6/363.4 MB[0m [31m270.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m297.0/363.4 MB[0m [31m264.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m306.3/363.4 MB[0m [31m263.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m315.0/363.4 MB[0m [31m249.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m324.4/363.4 MB[0m [31m262.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m333.8/363.4 MB[0m [31m264.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m343.2/363.4 MB[0m [31m265.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m352.6/363.4 MB[0m [31m264.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m362.1/363.4 MB[0m [31m269.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━��2024-12-30T09:31:31.535701665Z �━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m272.5 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m363.4/363.4 MB[0m [31m2.8 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:34.250849739Z [?25hCollecting jinja2
2024-12-30T09:31:34.301096065Z   Downloading /packages/jinja2/jinja2-3.1.5-py3-none-any.whl (134 kB)
2024-12-30T09:31:34.469909696Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/134.6 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m133.1/134.6 kB[0m [31m31.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m133.1/134.6 kB[0m [31m31.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m133.1/134.6 kB[0m [31m31.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m133.1/134.6 kB[0m [31m31.7 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m134.6/134.6 kB[0m [31m755.3 kB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:34.496120491Z [?25hCollecting nvidia-nccl-cu12==2.21.5
2024-12-30T09:31:34.513427671Z   Downloading /packages/nvidia-nccl-cu12/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)
2024-12-30T09:31:36.897445793Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/188.7 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.4/188.7 MB[0m [31m42.3 MB/s[0m eta [36m0:00:05[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.0/188.7 MB[0m [31m58.4 MB/s[0m eta [36m0:00:04[0m
[2K     [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.3/188.7 MB[0m [31m79.3 MB/s[0m eta [36m0:00:03[0m
[2K     [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.1/188.7 MB[0m [31m137.5 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m23.1/188.7 MB[0m [31m239.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m32.3/188.7 MB[0m [31m257.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m41.6/188.7 MB[0m [31m257.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m50.9/188.7 MB[0m [31m261.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m60.2/188.7 MB[0m [31m261.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m69.6/188.7 MB[0m [31m260.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m79.1/188.7 MB[0m [31m262.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m88.7/188.7 MB[0m [31m268.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m98.3/188.7 MB[0m [31m266.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m107.9/188.7 MB[0m [31m266.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m117.3/188.7 MB[0m [31m263.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m126.6/188.7 MB[0m [31m259.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m136.0/188.7 MB[0m [31m260.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m145.6/188.7 MB[0m [31m267.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m155.2/188.7 MB[0m [31m270.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m164.7/188.7 MB[0m [31m264.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m174.1/188.7 MB[0m [31m263.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m183.8/188.7 MB[0m [31m270.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m188.7/188.7 MB[0m [31m282.6 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m188.7/188.7 MB[0m [31m6.4 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:37.169817190Z [?25hCollecting nvidia-cufft-cu12==11.2.1.3
2024-12-30T09:31:37.186110587Z   Downloading /packages/nvidia-cufft-cu12/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
2024-12-30T09:31:38.588961558Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/211.5 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.8/211.5 MB[0m [31m113.0 MB/s[0m eta [36m0:00:02[0m
[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m12.6/211.5 MB[0m [31m252.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.3/211.5 MB[0m [31m250.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m30.4/211.5 MB[0m [31m255.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m39.3/211.5 MB[0m [31m255.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m48.3/211.5 MB[0m [31m255.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m57.4/211.5 MB[0m [31m258.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m66.5/211.5 MB[0m [31m256.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m75.5/211.5 MB[0m [31m257.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m84.5/211.5 MB[0m [31m254.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m93.5/211.5 MB[0m [31m254.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m102.9/211.5 MB[0m [31m265.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m112.1/211.5 MB[0m [31m263.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m121.6/211.5 MB[0m [31m267.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m131.0/211.5 MB[0m [31m267.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m140.5/211.5 MB[0m [31m271.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m150.2/211.5 MB[0m [31m271.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m159.9/211.5 MB[0m [31m275.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m169.5/211.5 MB[0m [31m273.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m179.0/211.5 MB[0m [31m269.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m188.4/211.5 MB[0m [31m267.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m198.1/211.5 MB[0m [31m273.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m207.5/211.5 MB[0m [31m268.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m278.4 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m211.5/211.5 MB[0m [31m16.8 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:38.894618406Z [?25hCollecting nvidia-nvjitlink-cu12==12.4.127
2024-12-30T09:31:38.910541218Z   Downloading /packages/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
2024-12-30T09:31:39.086616793Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/21.1 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.9/21.1 MB[0m [31m118.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m12.3/21.1 MB[0m [31m233.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m20.6/21.1 MB[0m [31m238.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m21.1/21.1 MB[0m [31m226.9 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.1/21.1 MB[0m [31m93.9 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:39.167164711Z [?25hCollecting typing-extensions>=4.8.0
2024-12-30T09:31:39.183629479Z   Downloading /packages/typing-extensions/typing_extensions-4.12.2-py3-none-any.whl (37 kB)
2024-12-30T09:31:39.272327622Z Collecting networkx
2024-12-30T09:31:39.288755622Z   Downloading /packages/networkx/networkx-3.4.2-py3-none-any.whl (1.7 MB)
2024-12-30T09:31:39.326279668Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/1.7 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m53.3 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:39.404258427Z [?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127
2024-12-30T09:31:39.422780430Z   Downloading /packages/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
2024-12-30T09:31:39.615251714Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/24.6 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.5/24.6 MB[0m [31m134.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m13.8/24.6 MB[0m [31m265.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m23.2/24.6 MB[0m [31m265.7 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m24.6/24.6 MB[0m [31m271.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m24.6/24.6 MB[0m [31m271.3 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.6/24.6 MB[0m [31m89.2 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:39.683326993Z [?25hCollecting nvidia-cusolver-cu12==11.6.1.9
2024-12-30T09:31:39.700478968Z   Downloading /packages/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
2024-12-30T09:31:40.583219673Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/127.9 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.4/127.9 MB[0m [31m131.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.6/127.9 MB[0m [31m262.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m22.8/127.9 MB[0m [31m259.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m31.5/127.9 MB[0m [31m250.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m40.5/127.9 MB[0m [31m254.2 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.3/127.9 MB[0m [31m249.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m58.2/127.9 MB[0m [31m254.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m67.4/127.9 MB[0m [31m259.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m76.6/127.9 MB[0m [31m259.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m85.8/127.9 MB[0m [31m261.5 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m94.8/127.9 MB[0m [31m253.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m103.7/127.9 MB[0m [31m253.4 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m112.6/127.9 MB[0m [31m252.9 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m121.7/127.9 MB[0m [31m254.1 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m266.0 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m127.9/127.9 MB[0m [31m25.7 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:40.815491596Z [?25hCollecting filelock
2024-12-30T09:31:40.831837673Z   Downloading /packages/filelock/filelock-3.16.1-py3-none-any.whl (16 kB)
2024-12-30T09:31:40.868588863Z Collecting nvidia-nvtx-cu12==12.4.127
2024-12-30T09:31:40.885687704Z   Downloading /packages/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)
2024-12-30T09:31:40.902094551Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/99.1 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m99.1/99.1 kB[0m [31m7.9 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:40.964479712Z [?25hCollecting fsspec
2024-12-30T09:31:40.981221272Z   Downloading /packages/fsspec/fsspec-2024.12.0-py3-none-any.whl (183 kB)
2024-12-30T09:31:40.996387213Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/183.9 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m183.9/183.9 kB[0m [31m15.8 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:41.031009641Z [?25hCollecting nvidia-cuda-cupti-cu12==12.4.127
2024-12-30T09:31:41.048208655Z   Downloading /packages/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
2024-12-30T09:31:41.166668540Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/13.8 MB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.6/13.8 MB[0m [31m107.3 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m12.9/13.8 MB[0m [31m262.0 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m13.8/13.8 MB[0m [31m262.5 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.8/13.8 MB[0m [31m129.9 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:41.258973786Z [?25hCollecting mpmath<1.4,>=1.1.0
2024-12-30T09:31:41.277057398Z   Downloading /packages/mpmath/mpmath-1.3.0-py3-none-any.whl (536 kB)
2024-12-30T09:31:41.301766091Z [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/536.2 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m536.2/536.2 kB[0m [31m25.6 MB/s[0m eta [36m0:00:00[0m
2024-12-30T09:31:41.647818555Z [?25hCollecting MarkupSafe>=2.0
2024-12-30T09:31:41.665170360Z   Downloading /packages/markupsafe/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)
2024-12-30T09:31:41.721646930Z Building wheels for collected packages: lightning-attn
2024-12-30T09:31:42.261865830Z   Building wheel for lightning-attn (setup.py) ... [?25l- \ done
2024-12-30T09:31:42.262606485Z [?25h  Created wheel for lightning-attn: filename=lightning_attn-0.0.5-py3-none-any.whl size=13072 sha256=c2154322910e4c093402755aef5a793a1f360b77ef8023fc4917854803d63323
2024-12-30T09:31:42.262828823Z   Stored in directory: /root/.cache/pip/wheels/c3/16/69/cd88bf81d1072f60ae67d772ffefdee4bfa969cb3279e20d99
2024-12-30T09:31:42.265321507Z Successfully built lightning-attn
2024-12-30T09:31:42.737024631Z Installing collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, einops, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, lightning-attn
2024-12-30T09:32:43.319692758Z Successfully installed MarkupSafe-3.0.2 einops-0.8.0 filelock-3.16.1 fsspec-2024.12.0 jinja2-3.1.5 lightning-attn-0.0.5 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0 typing-extensions-4.12.2
2024-12-30T09:32:43.319958247Z [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
2024-12-30T09:32:43.328613022Z [0m
2024-12-30T09:32:43.328639652Z [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.0.1[0m[39;49m -> [0m[32;49m24.3.1[0m
2024-12-30T09:32:43.328643971Z [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m
2024-12-30T09:34:54.831384840Z [?2004hroot@f3ad0414bf1d:/lightning-attention# [7mpytest --collect-only -q[27mpytest --collect-only -q
2024-12-30T09:34:56.823772451Z [?2004l
tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-256-128-64]
2024-12-30T09:34:56.823799423Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-512-128-64]
2024-12-30T09:34:56.823802494Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-1024-128-64]
2024-12-30T09:34:56.823804934Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-128-64]
2024-12-30T09:34:56.823807275Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-4096-128-64]
2024-12-30T09:34:56.823809426Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-8192-128-64]
2024-12-30T09:34:56.823812238Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-32-64]
2024-12-30T09:34:56.823814387Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-64-64]
2024-12-30T09:34:56.823816507Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-12-2048-128-64]
2024-12-30T09:34:56.823818658Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-16-2048-128-64]
2024-12-30T09:34:56.823821104Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-20-2048-128-64]
2024-12-30T09:34:56.823823233Z tests/ops/test_lightning2.py::test_lightning2[dtype0-1-8-2048-128-64]
2024-12-30T09:34:56.823825357Z tests/ops/test_lightning2.py::test_lightning2[dtype0-2-8-2048-128-64]
2024-12-30T09:34:56.823839468Z tests/ops/test_lightning2.py::test_lightning2[dtype0-3-8-2048-128-64]
2024-12-30T09:34:56.823841821Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-913-128-64]
2024-12-30T09:34:56.823843882Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-513-128-64]
2024-12-30T09:34:56.823846169Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-1213-128-64]
2024-12-30T09:34:56.823848496Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-16-64]
2024-12-30T09:34:56.823850515Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-256-128-64]
2024-12-30T09:34:56.823852445Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-512-128-64]
2024-12-30T09:34:56.823854490Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-1024-128-64]
2024-12-30T09:34:56.823856821Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-128-64]
2024-12-30T09:34:56.823859361Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-4096-128-64]
2024-12-30T09:34:56.823861373Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-8192-128-64]
2024-12-30T09:34:56.823867204Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-32-64]
2024-12-30T09:34:56.823869324Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-64-64]
2024-12-30T09:34:56.823871280Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-12-2048-128-64]
2024-12-30T09:34:56.823873692Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-16-2048-128-64]
2024-12-30T09:34:56.823875927Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-20-2048-128-64]
2024-12-30T09:34:56.823877867Z tests/ops/test_lightning2.py::test_lightning2[dtype1-1-8-2048-128-64]
2024-12-30T09:34:56.823879832Z tests/ops/test_lightning2.py::test_lightning2[dtype1-2-8-2048-128-64]
2024-12-30T09:34:56.823881841Z tests/ops/test_lightning2.py::test_lightning2[dtype1-3-8-2048-128-64]
2024-12-30T09:34:56.823883880Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-913-128-64]
2024-12-30T09:34:56.823885831Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-513-128-64]
2024-12-30T09:34:56.823921946Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-1213-128-64]
2024-12-30T09:34:56.823948517Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-16-64]
2024-12-30T09:34:56.823951217Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-256-128-64]
2024-12-30T09:34:56.823953642Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-512-128-64]
2024-12-30T09:34:56.823955925Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-1024-128-64]
2024-12-30T09:34:56.823957999Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-128-64]
2024-12-30T09:34:56.823960182Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-4096-128-64]
2024-12-30T09:34:56.823962664Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-8192-128-64]
2024-12-30T09:34:56.823978242Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-32-64]
2024-12-30T09:34:56.823983488Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-64-64]
2024-12-30T09:34:56.823985791Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-12-2048-128-64]
2024-12-30T09:34:56.823987857Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-16-2048-128-64]
2024-12-30T09:34:56.823989821Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-20-2048-128-64]
2024-12-30T09:34:56.823991768Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-1-8-2048-128-64]
2024-12-30T09:34:56.823993802Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-2-8-2048-128-64]
2024-12-30T09:34:56.823996595Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-3-8-2048-128-64]
2024-12-30T09:34:56.823999172Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-913-128-64]
2024-12-30T09:34:56.824001211Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-513-128-64]
2024-12-30T09:34:56.824030598Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-1213-128-64]
2024-12-30T09:34:56.824048589Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-16-64]
2024-12-30T09:34:56.824051266Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-256-128-64]
2024-12-30T09:34:56.824054554Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-512-128-64]
2024-12-30T09:34:56.824056869Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-1024-128-64]
2024-12-30T09:34:56.824059774Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-128-64]
2024-12-30T09:34:56.824065145Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-4096-128-64]
2024-12-30T09:34:56.824068703Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-8192-128-64]
2024-12-30T09:34:56.824106644Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-32-64]
2024-12-30T09:34:56.824113818Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-64-64]
2024-12-30T09:34:56.824116076Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-12-2048-128-64]
2024-12-30T09:34:56.824119514Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-16-2048-128-64]
2024-12-30T09:34:56.824121668Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-20-2048-128-64]
2024-12-30T09:34:56.824140495Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-1-8-2048-128-64]
2024-12-30T09:34:56.824145210Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-2-8-2048-128-64]
2024-12-30T09:34:56.824147443Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-3-8-2048-128-64]
2024-12-30T09:34:56.824149487Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-913-128-64]
2024-12-30T09:34:56.824176955Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-513-128-64]
2024-12-30T09:34:56.824181444Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-1213-128-64]
2024-12-30T09:34:56.824183472Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-16-64]
2024-12-30T09:34:56.824185529Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-127-768]
2024-12-30T09:34:56.824187695Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-127-1024]
2024-12-30T09:34:56.824190488Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-128-768]
2024-12-30T09:34:56.824205952Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-128-1024]
2024-12-30T09:34:56.824210381Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-256-768]
2024-12-30T09:34:56.824212831Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-256-1024]
2024-12-30T09:34:56.824224898Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-257-768]
2024-12-30T09:34:56.824249854Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-257-1024]
2024-12-30T09:34:56.824254019Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1024-768]
2024-12-30T09:34:56.824256010Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1024-1024]
2024-12-30T09:34:56.824289777Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1025-768]
2024-12-30T09:34:56.824293260Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1025-1024]
2024-12-30T09:34:56.824295488Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-127-768]
2024-12-30T09:34:56.824297474Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-127-1024]
2024-12-30T09:34:56.824299437Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-128-768]
2024-12-30T09:34:56.824301407Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-128-1024]
2024-12-30T09:34:56.824332810Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-256-768]
2024-12-30T09:34:56.824346917Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-256-1024]
2024-12-30T09:34:56.824349732Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-257-768]
2024-12-30T09:34:56.824352123Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-257-1024]
2024-12-30T09:34:56.824354189Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1024-768]
2024-12-30T09:34:56.824356795Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1024-1024]
2024-12-30T09:34:56.824367244Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1025-768]
2024-12-30T09:34:56.824369347Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1025-1024]
2024-12-30T09:34:56.824405724Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-127-768]
2024-12-30T09:34:56.824410054Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-127-1024]
2024-12-30T09:34:56.824412273Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-128-768]
2024-12-30T09:34:56.824426293Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-128-1024]
2024-12-30T09:34:56.824428358Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-256-768]
2024-12-30T09:34:56.824438338Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-256-1024]
2024-12-30T09:34:56.824441885Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-257-768]
2024-12-30T09:34:56.824443836Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-257-1024]
2024-12-30T09:34:56.824445952Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1024-768]
2024-12-30T09:34:56.824447955Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1024-1024]
2024-12-30T09:34:56.824480255Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1025-768]
2024-12-30T09:34:56.824493217Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1025-1024]
2024-12-30T09:34:56.824497128Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-127-768]
2024-12-30T09:34:56.824500217Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-127-1024]
2024-12-30T09:34:56.824502960Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-128-768]
2024-12-30T09:34:56.824523605Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-128-1024]
2024-12-30T09:34:56.824526932Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-256-768]
2024-12-30T09:34:56.824528939Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-256-1024]
2024-12-30T09:34:56.824530943Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-257-768]
2024-12-30T09:34:56.824533012Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-257-1024]
2024-12-30T09:34:56.824540023Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1024-768]
2024-12-30T09:34:56.824544013Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1024-1024]
2024-12-30T09:34:56.824586082Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1025-768]
2024-12-30T09:34:56.824592009Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1025-1024]
2024-12-30T09:34:56.824596225Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-127-768]
2024-12-30T09:34:56.824598250Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-127-1024]
2024-12-30T09:34:56.824632161Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-128-768]
2024-12-30T09:34:56.824638357Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-128-1024]
2024-12-30T09:34:56.824640533Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-256-768]
2024-12-30T09:34:56.824642531Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-256-1024]
2024-12-30T09:34:56.824644541Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-257-768]
2024-12-30T09:34:56.824657219Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-257-1024]
2024-12-30T09:34:56.824660325Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1024-768]
2024-12-30T09:34:56.824663304Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1024-1024]
2024-12-30T09:34:56.824671193Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1025-768]
2024-12-30T09:34:56.824682373Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1025-1024]
2024-12-30T09:34:56.824686152Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-127-768]
2024-12-30T09:34:56.824746524Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-127-1024]
2024-12-30T09:34:56.824750364Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-128-768]
2024-12-30T09:34:56.824752419Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-128-1024]
2024-12-30T09:34:56.824754358Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-256-768]
2024-12-30T09:34:56.824756328Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-256-1024]
2024-12-30T09:34:56.824758377Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-257-768]
2024-12-30T09:34:56.824760417Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-257-1024]
2024-12-30T09:34:56.824762422Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1024-768]
2024-12-30T09:34:56.824764371Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1024-1024]
2024-12-30T09:34:56.824779097Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1025-768]
2024-12-30T09:34:56.824782573Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1025-1024]
2024-12-30T09:34:56.824784710Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-127-768]
2024-12-30T09:34:56.824786667Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-127-1024]
2024-12-30T09:34:56.824788718Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-128-768]
2024-12-30T09:34:56.824806621Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-128-1024]
2024-12-30T09:34:56.824810579Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-256-768]
2024-12-30T09:34:56.824812616Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-256-1024]
2024-12-30T09:34:56.824814603Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-257-768]
2024-12-30T09:34:56.824831473Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-257-1024]
2024-12-30T09:34:56.824851762Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1024-768]
2024-12-30T09:34:56.824854866Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1024-1024]
2024-12-30T09:34:56.824856795Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1025-768]
2024-12-30T09:34:56.824858909Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1025-1024]
2024-12-30T09:34:56.824886670Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-127-768]
2024-12-30T09:34:56.824889831Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-127-1024]
2024-12-30T09:34:56.824891901Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-128-768]
2024-12-30T09:34:56.824893930Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-128-1024]
2024-12-30T09:34:56.824896280Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-256-768]
2024-12-30T09:34:56.824921716Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-256-1024]
2024-12-30T09:34:56.824925161Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-257-768]
2024-12-30T09:34:56.824927183Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-257-1024]
2024-12-30T09:34:56.824940600Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1024-768]
2024-12-30T09:34:56.824942824Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1024-1024]
2024-12-30T09:34:56.824944804Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1025-768]
2024-12-30T09:34:56.824957270Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1025-1024]
2024-12-30T09:34:56.824969930Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-127-768]
2024-12-30T09:34:56.824973378Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-127-1024]
2024-12-30T09:34:56.824975330Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-128-768]
2024-12-30T09:34:56.824986865Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-128-1024]
2024-12-30T09:34:56.824989055Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-256-768]
2024-12-30T09:34:56.824999614Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-256-1024]
2024-12-30T09:34:56.825009036Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-257-768]
2024-12-30T09:34:56.825029651Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-257-1024]
2024-12-30T09:34:56.825032140Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1024-768]
2024-12-30T09:34:56.825034061Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1024-1024]
2024-12-30T09:34:56.825047776Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1025-768]
2024-12-30T09:34:56.825055533Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1025-1024]
2024-12-30T09:34:56.825059469Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-127-768]
2024-12-30T09:34:56.825088097Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-127-1024]
2024-12-30T09:34:56.825090732Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-128-768]
2024-12-30T09:34:56.825092714Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-128-1024]
2024-12-30T09:34:56.825094651Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-256-768]
2024-12-30T09:34:56.825107319Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-256-1024]
2024-12-30T09:34:56.825109552Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-257-768]
2024-12-30T09:34:56.825111583Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-257-1024]
2024-12-30T09:34:56.825132496Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1024-768]
2024-12-30T09:34:56.825135036Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1024-1024]
2024-12-30T09:34:56.825136976Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1025-768]
2024-12-30T09:34:56.825159107Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1025-1024]
2024-12-30T09:34:56.825179339Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-127-768]
2024-12-30T09:34:56.825182592Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-127-1024]
2024-12-30T09:34:56.825184695Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-128-768]
2024-12-30T09:34:56.825187327Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-128-1024]
2024-12-30T09:34:56.825189360Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-256-768]
2024-12-30T09:34:56.825191987Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-256-1024]
2024-12-30T09:34:56.825193986Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-257-768]
2024-12-30T09:34:56.825211747Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-257-1024]
2024-12-30T09:34:56.825216623Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1024-768]
2024-12-30T09:34:56.825239130Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1024-1024]
2024-12-30T09:34:56.825258328Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1025-768]
2024-12-30T09:34:56.825260976Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1025-1024]
2024-12-30T09:34:56.825263048Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-127-768]
2024-12-30T09:34:56.825265053Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-127-1024]
2024-12-30T09:34:56.825267107Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-128-768]
2024-12-30T09:34:56.825269127Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-128-1024]
2024-12-30T09:34:56.825271698Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-256-768]
2024-12-30T09:34:56.825273651Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-256-1024]
2024-12-30T09:34:56.825295531Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-257-768]
2024-12-30T09:34:56.825298523Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-257-1024]
2024-12-30T09:34:56.825300494Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1024-768]
2024-12-30T09:34:56.825302496Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1024-1024]
2024-12-30T09:34:56.825312395Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1025-768]
2024-12-30T09:34:56.825314738Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1025-1024]
2024-12-30T09:34:56.826407815Z 
2024-12-30T09:34:56.826505170Z [33m===================================================================================== warnings summary ======================================================================================[0m
2024-12-30T09:34:56.826596891Z ../usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295
2024-12-30T09:34:56.826601111Z   /usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
2024-12-30T09:34:56.826603844Z     cpu = _conversion_method_template(device=torch.device("cpu"))
2024-12-30T09:34:56.826613187Z 
2024-12-30T09:34:56.826615325Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2024-12-30T09:34:56.826749827Z [32m[32m216 tests collected[0m[32m in 1.73s[0m[0m
2024-12-30T09:35:07.538069570Z [?2004hroot@f3ad0414bf1d:/lightning-attention# pytest
2024-12-30T09:35:07.703514094Z [?2004l
[1m==================================================================================== test session starts ====================================================================================[0m
2024-12-30T09:35:07.703546099Z platform linux -- Python 3.10.16, pytest-8.3.4, pluggy-1.5.0
2024-12-30T09:35:07.703561365Z rootdir: /lightning-attention
2024-12-30T09:35:09.327986519Z [1mcollecting ... [0m[1m
collecting 36 items                                                                                                                                                                         [0m[1m
collected 216 items                                                                                                                                                                         [0m
2024-12-30T09:35:09.328572439Z 
2024-12-30T09:35:10.360945834Z tests/ops/test_lightning2.py [31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31m                                                                                                                     [ 16%][0m
2024-12-30T09:35:11.418228441Z tests/ops/test_lightning2_no_decay.py [31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31m                                                                                                            [ 33%][0m
2024-12-30T09:35:15.415006482Z tests/ops/test_srmsnorm.py [31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31mF[0m[31m           [100%][0m
2024-12-30T09:35:15.415088294Z 
2024-12-30T09:35:15.415103667Z ========================================================================================= FAILURES ==========================================================================================
2024-12-30T09:35:15.415125955Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-256-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.415130963Z 
2024-12-30T09:35:15.415193319Z b = 6, h = 8, n = 256, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.415196111Z 
2024-12-30T09:35:15.415475168Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.415478791Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.415481144Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.415483183Z         torch.manual_seed(2024)
2024-12-30T09:35:15.415503177Z         device = torch.device("cuda")
2024-12-30T09:35:15.415518995Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.415529979Z 
2024-12-30T09:35:15.415535057Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.415537497Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.415540243Z 
2024-12-30T09:35:15.415864909Z     def _lazy_init():
2024-12-30T09:35:15.415893567Z         global _initialized, _queued_calls
2024-12-30T09:35:15.415896271Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.415899049Z             return
2024-12-30T09:35:15.415901194Z         with _initialization_lock:
2024-12-30T09:35:15.415903398Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.415906135Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.415908255Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.415910386Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.415912545Z             # find there is nothing left to do.
2024-12-30T09:35:15.415930772Z             if is_initialized():
2024-12-30T09:35:15.415937389Z                 return
2024-12-30T09:35:15.415939498Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.415942022Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.415944168Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.415946403Z             if _is_in_bad_fork():
2024-12-30T09:35:15.415948282Z                 raise RuntimeError(
2024-12-30T09:35:15.415950222Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.415952402Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.415954607Z                 )
2024-12-30T09:35:15.415956478Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.415960167Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.415962258Z             if _cudart is None:
2024-12-30T09:35:15.415964956Z                 raise AssertionError(
2024-12-30T09:35:15.415966870Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.415969087Z                 )
2024-12-30T09:35:15.415971596Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.416016769Z             # are found or any other error occurs
2024-12-30T09:35:15.416029295Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.416032396Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.416034772Z >           torch._C._cuda_init()
2024-12-30T09:35:15.416036954Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.416046998Z 
2024-12-30T09:35:15.416078002Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.416404178Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-512-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.416412359Z 
2024-12-30T09:35:15.416414587Z b = 6, h = 8, n = 512, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.416416749Z 
2024-12-30T09:35:15.416601223Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.416607869Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.416610735Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.416612734Z         torch.manual_seed(2024)
2024-12-30T09:35:15.416614728Z         device = torch.device("cuda")
2024-12-30T09:35:15.416616828Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.416629725Z 
2024-12-30T09:35:15.416632004Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.416680289Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.416686370Z 
2024-12-30T09:35:15.416914766Z     def _lazy_init():
2024-12-30T09:35:15.416933140Z         global _initialized, _queued_calls
2024-12-30T09:35:15.416937400Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.416941040Z             return
2024-12-30T09:35:15.416944158Z         with _initialization_lock:
2024-12-30T09:35:15.416947741Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.416951617Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.416964941Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.416967106Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.416969115Z             # find there is nothing left to do.
2024-12-30T09:35:15.416982061Z             if is_initialized():
2024-12-30T09:35:15.416984401Z                 return
2024-12-30T09:35:15.416986588Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.416988872Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.416990964Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.416992952Z             if _is_in_bad_fork():
2024-12-30T09:35:15.416995027Z                 raise RuntimeError(
2024-12-30T09:35:15.416997998Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.417000499Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.417028645Z                 )
2024-12-30T09:35:15.417036339Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.417038793Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.417040944Z             if _cudart is None:
2024-12-30T09:35:15.417043005Z                 raise AssertionError(
2024-12-30T09:35:15.417045113Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.417047398Z                 )
2024-12-30T09:35:15.417049326Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.417062245Z             # are found or any other error occurs
2024-12-30T09:35:15.417064517Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.417066873Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.417069216Z >           torch._C._cuda_init()
2024-12-30T09:35:15.417081845Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.417092774Z 
2024-12-30T09:35:15.417118489Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.417310075Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-1024-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.417314168Z 
2024-12-30T09:35:15.417370567Z b = 6, h = 8, n = 1024, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.417376123Z 
2024-12-30T09:35:15.417596737Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.417601809Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.417615667Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.417617927Z         torch.manual_seed(2024)
2024-12-30T09:35:15.417619869Z         device = torch.device("cuda")
2024-12-30T09:35:15.417621905Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.417624081Z 
2024-12-30T09:35:15.417631542Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.417649071Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.417651853Z 
2024-12-30T09:35:15.417859698Z     def _lazy_init():
2024-12-30T09:35:15.417864242Z         global _initialized, _queued_calls
2024-12-30T09:35:15.417866501Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.417876948Z             return
2024-12-30T09:35:15.417879859Z         with _initialization_lock:
2024-12-30T09:35:15.417881888Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.417888563Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.417894036Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.417896915Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.417898962Z             # find there is nothing left to do.
2024-12-30T09:35:15.417955091Z             if is_initialized():
2024-12-30T09:35:15.417959845Z                 return
2024-12-30T09:35:15.417961858Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.417964554Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.417987640Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.417990401Z             if _is_in_bad_fork():
2024-12-30T09:35:15.417992583Z                 raise RuntimeError(
2024-12-30T09:35:15.417994582Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.417996871Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.417998972Z                 )
2024-12-30T09:35:15.418000846Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.418002952Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.418005654Z             if _cudart is None:
2024-12-30T09:35:15.418008878Z                 raise AssertionError(
2024-12-30T09:35:15.418019364Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.418025516Z                 )
2024-12-30T09:35:15.418031854Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.418034208Z             # are found or any other error occurs
2024-12-30T09:35:15.418048163Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.418050569Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.418062270Z >           torch._C._cuda_init()
2024-12-30T09:35:15.418064850Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.418067487Z 
2024-12-30T09:35:15.418110930Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.418267943Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.418273833Z 
2024-12-30T09:35:15.418307064Z b = 6, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.418309922Z 
2024-12-30T09:35:15.418497448Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.418500416Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.418502560Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.418504749Z         torch.manual_seed(2024)
2024-12-30T09:35:15.418507078Z         device = torch.device("cuda")
2024-12-30T09:35:15.418509106Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.418511278Z 
2024-12-30T09:35:15.418535466Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.418576508Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.418581506Z 
2024-12-30T09:35:15.418741206Z     def _lazy_init():
2024-12-30T09:35:15.418744188Z         global _initialized, _queued_calls
2024-12-30T09:35:15.418746155Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.418748320Z             return
2024-12-30T09:35:15.418768734Z         with _initialization_lock:
2024-12-30T09:35:15.418773407Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.418776646Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.418784803Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.418791035Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.418794497Z             # find there is nothing left to do.
2024-12-30T09:35:15.418798957Z             if is_initialized():
2024-12-30T09:35:15.418813175Z                 return
2024-12-30T09:35:15.418820677Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.418823326Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.418833812Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.418838618Z             if _is_in_bad_fork():
2024-12-30T09:35:15.418840650Z                 raise RuntimeError(
2024-12-30T09:35:15.418859100Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.418884170Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.418905501Z                 )
2024-12-30T09:35:15.418909157Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.418911508Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.418913747Z             if _cudart is None:
2024-12-30T09:35:15.418915890Z                 raise AssertionError(
2024-12-30T09:35:15.418918566Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.418920872Z                 )
2024-12-30T09:35:15.418926187Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.418928695Z             # are found or any other error occurs
2024-12-30T09:35:15.418930714Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.418952997Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.418959410Z >           torch._C._cuda_init()
2024-12-30T09:35:15.418989377Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.418995260Z 
2024-12-30T09:35:15.418997286Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.419171770Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-4096-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.419177161Z 
2024-12-30T09:35:15.419184516Z b = 6, h = 8, n = 4096, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.419193483Z 
2024-12-30T09:35:15.419375345Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.419381627Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.419384464Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.419386831Z         torch.manual_seed(2024)
2024-12-30T09:35:15.419389077Z         device = torch.device("cuda")
2024-12-30T09:35:15.419391765Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.419428010Z 
2024-12-30T09:35:15.419434287Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.419437096Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.419439878Z 
2024-12-30T09:35:15.419646680Z     def _lazy_init():
2024-12-30T09:35:15.419651155Z         global _initialized, _queued_calls
2024-12-30T09:35:15.419653146Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.419655330Z             return
2024-12-30T09:35:15.419657244Z         with _initialization_lock:
2024-12-30T09:35:15.419659383Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.419661829Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.419663930Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.419697049Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.419702651Z             # find there is nothing left to do.
2024-12-30T09:35:15.419704819Z             if is_initialized():
2024-12-30T09:35:15.419706917Z                 return
2024-12-30T09:35:15.419709018Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.419712402Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.419740206Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.419745722Z             if _is_in_bad_fork():
2024-12-30T09:35:15.419747757Z                 raise RuntimeError(
2024-12-30T09:35:15.419749882Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.419752090Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.419758014Z                 )
2024-12-30T09:35:15.419760603Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.419782328Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.419788377Z             if _cudart is None:
2024-12-30T09:35:15.419821257Z                 raise AssertionError(
2024-12-30T09:35:15.419831940Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.419835105Z                 )
2024-12-30T09:35:15.419837942Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.419841241Z             # are found or any other error occurs
2024-12-30T09:35:15.419853658Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.419857007Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.419860186Z >           torch._C._cuda_init()
2024-12-30T09:35:15.419877697Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.419882541Z 
2024-12-30T09:35:15.419884987Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.420040628Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-8192-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.420045959Z 
2024-12-30T09:35:15.420085016Z b = 6, h = 8, n = 8192, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.420098796Z 
2024-12-30T09:35:15.420268975Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.420274213Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.420276428Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.420278586Z         torch.manual_seed(2024)
2024-12-30T09:35:15.420281122Z         device = torch.device("cuda")
2024-12-30T09:35:15.420283384Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.420286074Z 
2024-12-30T09:35:15.420332670Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.420342908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.420347020Z 
2024-12-30T09:35:15.420563091Z     def _lazy_init():
2024-12-30T09:35:15.420577672Z         global _initialized, _queued_calls
2024-12-30T09:35:15.420580205Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.420582691Z             return
2024-12-30T09:35:15.420596656Z         with _initialization_lock:
2024-12-30T09:35:15.420598920Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.420601148Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.420603305Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.420607017Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.420609153Z             # find there is nothing left to do.
2024-12-30T09:35:15.420631744Z             if is_initialized():
2024-12-30T09:35:15.420637367Z                 return
2024-12-30T09:35:15.420639425Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.420641544Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.420643644Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.420679669Z             if _is_in_bad_fork():
2024-12-30T09:35:15.420682667Z                 raise RuntimeError(
2024-12-30T09:35:15.420684621Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.420686894Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.420689005Z                 )
2024-12-30T09:35:15.420690917Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.420719306Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.420733792Z             if _cudart is None:
2024-12-30T09:35:15.420736433Z                 raise AssertionError(
2024-12-30T09:35:15.420738578Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.420740867Z                 )
2024-12-30T09:35:15.420755226Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.420765509Z             # are found or any other error occurs
2024-12-30T09:35:15.420768190Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.420770730Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.420774068Z >           torch._C._cuda_init()
2024-12-30T09:35:15.420805433Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.420813688Z 
2024-12-30T09:35:15.420816694Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.420982027Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-32-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.420995233Z 
2024-12-30T09:35:15.421009074Z b = 6, h = 8, n = 2048, d = 32, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.421013449Z 
2024-12-30T09:35:15.421198548Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.421205070Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.421207740Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.421209832Z         torch.manual_seed(2024)
2024-12-30T09:35:15.421212237Z         device = torch.device("cuda")
2024-12-30T09:35:15.421214345Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.421216442Z 
2024-12-30T09:35:15.421247613Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.421253458Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.421255859Z 
2024-12-30T09:35:15.421460064Z     def _lazy_init():
2024-12-30T09:35:15.421463205Z         global _initialized, _queued_calls
2024-12-30T09:35:15.421465452Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.421467577Z             return
2024-12-30T09:35:15.421469484Z         with _initialization_lock:
2024-12-30T09:35:15.421471444Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.421473545Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.421490100Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.421495314Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.421498324Z             # find there is nothing left to do.
2024-12-30T09:35:15.421500561Z             if is_initialized():
2024-12-30T09:35:15.421513416Z                 return
2024-12-30T09:35:15.421518058Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.421520309Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.421522469Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.421538870Z             if _is_in_bad_fork():
2024-12-30T09:35:15.421542847Z                 raise RuntimeError(
2024-12-30T09:35:15.421545978Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.421585961Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.421592992Z                 )
2024-12-30T09:35:15.421595114Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.421607892Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.421613890Z             if _cudart is None:
2024-12-30T09:35:15.421616015Z                 raise AssertionError(
2024-12-30T09:35:15.421618131Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.421620353Z                 )
2024-12-30T09:35:15.421647403Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.421652638Z             # are found or any other error occurs
2024-12-30T09:35:15.421654794Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.421656994Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.421676669Z >           torch._C._cuda_init()
2024-12-30T09:35:15.421688001Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.421716269Z 
2024-12-30T09:35:15.421721100Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.421873733Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-64-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.421879119Z 
2024-12-30T09:35:15.421890981Z b = 6, h = 8, n = 2048, d = 64, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.421893344Z 
2024-12-30T09:35:15.422072879Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.422075887Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.422078016Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.422080336Z         torch.manual_seed(2024)
2024-12-30T09:35:15.422082491Z         device = torch.device("cuda")
2024-12-30T09:35:15.422084602Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.422119917Z 
2024-12-30T09:35:15.422128640Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.422159687Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.422176970Z 
2024-12-30T09:35:15.422325488Z     def _lazy_init():
2024-12-30T09:35:15.422331667Z         global _initialized, _queued_calls
2024-12-30T09:35:15.422333790Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.422336257Z             return
2024-12-30T09:35:15.422338404Z         with _initialization_lock:
2024-12-30T09:35:15.422340431Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.422342524Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.422346368Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.422348962Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.422350925Z             # find there is nothing left to do.
2024-12-30T09:35:15.422393203Z             if is_initialized():
2024-12-30T09:35:15.422402175Z                 return
2024-12-30T09:35:15.422405448Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.422446425Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.422451473Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.422454834Z             if _is_in_bad_fork():
2024-12-30T09:35:15.422457954Z                 raise RuntimeError(
2024-12-30T09:35:15.422460976Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.422464334Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.422467945Z                 )
2024-12-30T09:35:15.422471337Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.422474773Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.422478442Z             if _cudart is None:
2024-12-30T09:35:15.422483385Z                 raise AssertionError(
2024-12-30T09:35:15.422486779Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.422513172Z                 )
2024-12-30T09:35:15.422524180Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.422527521Z             # are found or any other error occurs
2024-12-30T09:35:15.422545148Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.422559526Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.422561984Z >           torch._C._cuda_init()
2024-12-30T09:35:15.422564116Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.422578359Z 
2024-12-30T09:35:15.422580574Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.422784094Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-12-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.422790803Z 
2024-12-30T09:35:15.422792980Z b = 6, h = 12, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.422809604Z 
2024-12-30T09:35:15.423011003Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.423020026Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.423022914Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.423024918Z         torch.manual_seed(2024)
2024-12-30T09:35:15.423026834Z         device = torch.device("cuda")
2024-12-30T09:35:15.423029009Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.423031198Z 
2024-12-30T09:35:15.423035141Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.423065241Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.423073050Z 
2024-12-30T09:35:15.423318164Z     def _lazy_init():
2024-12-30T09:35:15.423322218Z         global _initialized, _queued_calls
2024-12-30T09:35:15.423325646Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.423329295Z             return
2024-12-30T09:35:15.423332413Z         with _initialization_lock:
2024-12-30T09:35:15.423335635Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.423338994Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.423342121Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.423345244Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.423348420Z             # find there is nothing left to do.
2024-12-30T09:35:15.423351474Z             if is_initialized():
2024-12-30T09:35:15.423354504Z                 return
2024-12-30T09:35:15.423367149Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.423370543Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.423374303Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.423386064Z             if _is_in_bad_fork():
2024-12-30T09:35:15.423389538Z                 raise RuntimeError(
2024-12-30T09:35:15.423392576Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.423395953Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.423399228Z                 )
2024-12-30T09:35:15.423402242Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.423405399Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.423447714Z             if _cudart is None:
2024-12-30T09:35:15.423454942Z                 raise AssertionError(
2024-12-30T09:35:15.423457672Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.423460247Z                 )
2024-12-30T09:35:15.423462224Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.423464539Z             # are found or any other error occurs
2024-12-30T09:35:15.423466537Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.423468957Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.423471175Z >           torch._C._cuda_init()
2024-12-30T09:35:15.423475151Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.423477982Z 
2024-12-30T09:35:15.423499666Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.423671285Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-16-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.423676261Z 
2024-12-30T09:35:15.423683898Z b = 6, h = 16, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.423686168Z 
2024-12-30T09:35:15.423870577Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.423876494Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.423879133Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.423881358Z         torch.manual_seed(2024)
2024-12-30T09:35:15.423888544Z         device = torch.device("cuda")
2024-12-30T09:35:15.423893358Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.423895616Z 
2024-12-30T09:35:15.423919856Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.423933276Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.423937546Z 
2024-12-30T09:35:15.424135425Z     def _lazy_init():
2024-12-30T09:35:15.424141003Z         global _initialized, _queued_calls
2024-12-30T09:35:15.424143061Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.424145541Z             return
2024-12-30T09:35:15.424147562Z         with _initialization_lock:
2024-12-30T09:35:15.424149576Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.424181166Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.424187090Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.424190362Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.424193736Z             # find there is nothing left to do.
2024-12-30T09:35:15.424197007Z             if is_initialized():
2024-12-30T09:35:15.424228905Z                 return
2024-12-30T09:35:15.424235264Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.424237596Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.424239699Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.424241835Z             if _is_in_bad_fork():
2024-12-30T09:35:15.424243768Z                 raise RuntimeError(
2024-12-30T09:35:15.424245780Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.424249535Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.424251702Z                 )
2024-12-30T09:35:15.424253696Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.424256535Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.424304263Z             if _cudart is None:
2024-12-30T09:35:15.424306849Z                 raise AssertionError(
2024-12-30T09:35:15.424308815Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.424310940Z                 )
2024-12-30T09:35:15.424312831Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.424315814Z             # are found or any other error occurs
2024-12-30T09:35:15.424317826Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.424319881Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.424349892Z >           torch._C._cuda_init()
2024-12-30T09:35:15.424356004Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.424368265Z 
2024-12-30T09:35:15.424408394Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.424539461Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-20-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.424544639Z 
2024-12-30T09:35:15.424583386Z b = 6, h = 20, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.424587097Z 
2024-12-30T09:35:15.424762577Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.424766432Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.424768533Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.424770522Z         torch.manual_seed(2024)
2024-12-30T09:35:15.424775817Z         device = torch.device("cuda")
2024-12-30T09:35:15.424781579Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.424785153Z 
2024-12-30T09:35:15.424798088Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.424844552Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.424847538Z 
2024-12-30T09:35:15.425004884Z     def _lazy_init():
2024-12-30T09:35:15.425008667Z         global _initialized, _queued_calls
2024-12-30T09:35:15.425010832Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.425013102Z             return
2024-12-30T09:35:15.425034565Z         with _initialization_lock:
2024-12-30T09:35:15.425041749Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.425046515Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.425049911Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.425056876Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.425069774Z             # find there is nothing left to do.
2024-12-30T09:35:15.425072541Z             if is_initialized():
2024-12-30T09:35:15.425075430Z                 return
2024-12-30T09:35:15.425083903Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.425087651Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.425089780Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.425091825Z             if _is_in_bad_fork():
2024-12-30T09:35:15.425121867Z                 raise RuntimeError(
2024-12-30T09:35:15.425125814Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.425127965Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.425130015Z                 )
2024-12-30T09:35:15.425179299Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.425184760Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.425188376Z             if _cudart is None:
2024-12-30T09:35:15.425191399Z                 raise AssertionError(
2024-12-30T09:35:15.425194416Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.425197698Z                 )
2024-12-30T09:35:15.425200613Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.425204930Z             # are found or any other error occurs
2024-12-30T09:35:15.425208348Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.425211674Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.425260179Z >           torch._C._cuda_init()
2024-12-30T09:35:15.425263477Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.425265854Z 
2024-12-30T09:35:15.425267684Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.425422646Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-1-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.425425998Z 
2024-12-30T09:35:15.425475854Z b = 1, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.425483617Z 
2024-12-30T09:35:15.425657670Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.425663586Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.425665756Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.425667757Z         torch.manual_seed(2024)
2024-12-30T09:35:15.425669826Z         device = torch.device("cuda")
2024-12-30T09:35:15.425677387Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.425679570Z 
2024-12-30T09:35:15.425691294Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.425712523Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.425715833Z 
2024-12-30T09:35:15.425905776Z     def _lazy_init():
2024-12-30T09:35:15.425908577Z         global _initialized, _queued_calls
2024-12-30T09:35:15.425910548Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.425912766Z             return
2024-12-30T09:35:15.425914792Z         with _initialization_lock:
2024-12-30T09:35:15.425953373Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.425962004Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.425964107Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.425966090Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.425968107Z             # find there is nothing left to do.
2024-12-30T09:35:15.425970122Z             if is_initialized():
2024-12-30T09:35:15.425983072Z                 return
2024-12-30T09:35:15.425985272Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.425987371Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.425989452Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.426025428Z             if _is_in_bad_fork():
2024-12-30T09:35:15.426028322Z                 raise RuntimeError(
2024-12-30T09:35:15.426037783Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.426040857Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.426043046Z                 )
2024-12-30T09:35:15.426080585Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.426087805Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.426091263Z             if _cudart is None:
2024-12-30T09:35:15.426094231Z                 raise AssertionError(
2024-12-30T09:35:15.426098442Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.426102091Z                 )
2024-12-30T09:35:15.426105310Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.426114379Z             # are found or any other error occurs
2024-12-30T09:35:15.426118906Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.426122468Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.426125563Z >           torch._C._cuda_init()
2024-12-30T09:35:15.426158383Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.426162950Z 
2024-12-30T09:35:15.426167530Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.426329584Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-2-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.426337402Z 
2024-12-30T09:35:15.426371203Z b = 2, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.426376232Z 
2024-12-30T09:35:15.426564147Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.426569234Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.426572995Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.426575036Z         torch.manual_seed(2024)
2024-12-30T09:35:15.426577013Z         device = torch.device("cuda")
2024-12-30T09:35:15.426579088Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.426595336Z 
2024-12-30T09:35:15.426599915Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.426625752Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.426629046Z 
2024-12-30T09:35:15.426826986Z     def _lazy_init():
2024-12-30T09:35:15.426832596Z         global _initialized, _queued_calls
2024-12-30T09:35:15.426835883Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.426839365Z             return
2024-12-30T09:35:15.426842527Z         with _initialization_lock:
2024-12-30T09:35:15.426845640Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.426848811Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.426852874Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.426856906Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.426864422Z             # find there is nothing left to do.
2024-12-30T09:35:15.426868422Z             if is_initialized():
2024-12-30T09:35:15.426871713Z                 return
2024-12-30T09:35:15.426885980Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.426890154Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.426895212Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.426898578Z             if _is_in_bad_fork():
2024-12-30T09:35:15.426902083Z                 raise RuntimeError(
2024-12-30T09:35:15.426918000Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.426922276Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.426924419Z                 )
2024-12-30T09:35:15.426940515Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.426942874Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.426944954Z             if _cudart is None:
2024-12-30T09:35:15.426971963Z                 raise AssertionError(
2024-12-30T09:35:15.426974812Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.426977077Z                 )
2024-12-30T09:35:15.426979048Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.427007286Z             # are found or any other error occurs
2024-12-30T09:35:15.427012047Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.427015475Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.427019256Z >           torch._C._cuda_init()
2024-12-30T09:35:15.427023501Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.427028238Z 
2024-12-30T09:35:15.427086390Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.427199238Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-3-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.427202326Z 
2024-12-30T09:35:15.427244652Z b = 3, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.427247441Z 
2024-12-30T09:35:15.427446311Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.427455286Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.427457571Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.427459629Z         torch.manual_seed(2024)
2024-12-30T09:35:15.427461605Z         device = torch.device("cuda")
2024-12-30T09:35:15.427471651Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.427475576Z 
2024-12-30T09:35:15.427490395Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.427503740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.427506378Z 
2024-12-30T09:35:15.427711540Z     def _lazy_init():
2024-12-30T09:35:15.427715142Z         global _initialized, _queued_calls
2024-12-30T09:35:15.427717193Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.427719436Z             return
2024-12-30T09:35:15.427753415Z         with _initialization_lock:
2024-12-30T09:35:15.427766765Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.427769586Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.427771754Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.427773872Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.427775880Z             # find there is nothing left to do.
2024-12-30T09:35:15.427795386Z             if is_initialized():
2024-12-30T09:35:15.427797870Z                 return
2024-12-30T09:35:15.427800001Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.427802102Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.427804117Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.427815949Z             if _is_in_bad_fork():
2024-12-30T09:35:15.427818374Z                 raise RuntimeError(
2024-12-30T09:35:15.427855678Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.427864248Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.427866617Z                 )
2024-12-30T09:35:15.427868525Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.427870732Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.427873735Z             if _cudart is None:
2024-12-30T09:35:15.427883416Z                 raise AssertionError(
2024-12-30T09:35:15.427887258Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.427889499Z                 )
2024-12-30T09:35:15.427891390Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.427905773Z             # are found or any other error occurs
2024-12-30T09:35:15.427910274Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.427918374Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.427922005Z >           torch._C._cuda_init()
2024-12-30T09:35:15.427953997Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.427957596Z 
2024-12-30T09:35:15.427959592Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.428132565Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-913-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.428136977Z 
2024-12-30T09:35:15.428198207Z b = 6, h = 8, n = 913, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.428201093Z 
2024-12-30T09:35:15.428341013Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.428344757Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.428347140Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.428349201Z         torch.manual_seed(2024)
2024-12-30T09:35:15.428351158Z         device = torch.device("cuda")
2024-12-30T09:35:15.428353219Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.428387330Z 
2024-12-30T09:35:15.428393072Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.428421990Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.428426146Z 
2024-12-30T09:35:15.428597546Z     def _lazy_init():
2024-12-30T09:35:15.428601664Z         global _initialized, _queued_calls
2024-12-30T09:35:15.428603921Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.428621143Z             return
2024-12-30T09:35:15.428626777Z         with _initialization_lock:
2024-12-30T09:35:15.428630192Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.428640358Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.428644047Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.428647112Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.428654801Z             # find there is nothing left to do.
2024-12-30T09:35:15.428663770Z             if is_initialized():
2024-12-30T09:35:15.428682143Z                 return
2024-12-30T09:35:15.428685383Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.428687398Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.428690013Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.428692088Z             if _is_in_bad_fork():
2024-12-30T09:35:15.428701590Z                 raise RuntimeError(
2024-12-30T09:35:15.428703753Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.428772690Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.428776774Z                 )
2024-12-30T09:35:15.428778798Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.428780964Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.428783103Z             if _cudart is None:
2024-12-30T09:35:15.428785083Z                 raise AssertionError(
2024-12-30T09:35:15.428787079Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.428789186Z                 )
2024-12-30T09:35:15.428802808Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.428807762Z             # are found or any other error occurs
2024-12-30T09:35:15.428811106Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.428815225Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.428818803Z >           torch._C._cuda_init()
2024-12-30T09:35:15.428847152Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.428850849Z 
2024-12-30T09:35:15.428852759Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.429015858Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-513-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.429022766Z 
2024-12-30T09:35:15.429050868Z b = 6, h = 8, n = 513, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.429053926Z 
2024-12-30T09:35:15.429259736Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.429262962Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.429265150Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.429267191Z         torch.manual_seed(2024)
2024-12-30T09:35:15.429269189Z         device = torch.device("cuda")
2024-12-30T09:35:15.429271208Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.429295859Z 
2024-12-30T09:35:15.429301133Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.429363221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.429368266Z 
2024-12-30T09:35:15.429558493Z     def _lazy_init():
2024-12-30T09:35:15.429564046Z         global _initialized, _queued_calls
2024-12-30T09:35:15.429567344Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.429570768Z             return
2024-12-30T09:35:15.429573834Z         with _initialization_lock:
2024-12-30T09:35:15.429577002Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.429604538Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.429612519Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.429614632Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.429616703Z             # find there is nothing left to do.
2024-12-30T09:35:15.429637179Z             if is_initialized():
2024-12-30T09:35:15.429639799Z                 return
2024-12-30T09:35:15.429641859Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.429657499Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.429660083Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.429674230Z             if _is_in_bad_fork():
2024-12-30T09:35:15.429676594Z                 raise RuntimeError(
2024-12-30T09:35:15.429695579Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.429698310Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.429711050Z                 )
2024-12-30T09:35:15.429731622Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.429735496Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.429749255Z             if _cudart is None:
2024-12-30T09:35:15.429752919Z                 raise AssertionError(
2024-12-30T09:35:15.429773033Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.429777939Z                 )
2024-12-30T09:35:15.429780635Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.429783102Z             # are found or any other error occurs
2024-12-30T09:35:15.429802796Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.429805674Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.429821148Z >           torch._C._cuda_init()
2024-12-30T09:35:15.429823465Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.429853931Z 
2024-12-30T09:35:15.429856548Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.430076309Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-1213-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.430079050Z 
2024-12-30T09:35:15.430141229Z b = 6, h = 8, n = 1213, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.430146663Z 
2024-12-30T09:35:15.430339675Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.430344493Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.430347931Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.430351020Z         torch.manual_seed(2024)
2024-12-30T09:35:15.430354069Z         device = torch.device("cuda")
2024-12-30T09:35:15.430363866Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.430367454Z 
2024-12-30T09:35:15.430387723Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.430432433Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.430435826Z 
2024-12-30T09:35:15.430646530Z     def _lazy_init():
2024-12-30T09:35:15.430650360Z         global _initialized, _queued_calls
2024-12-30T09:35:15.430652410Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.430661855Z             return
2024-12-30T09:35:15.430663883Z         with _initialization_lock:
2024-12-30T09:35:15.430665792Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.430668007Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.430670045Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.430672828Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.430674917Z             # find there is nothing left to do.
2024-12-30T09:35:15.430698424Z             if is_initialized():
2024-12-30T09:35:15.430708754Z                 return
2024-12-30T09:35:15.430711169Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.430724715Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.430729938Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.430733540Z             if _is_in_bad_fork():
2024-12-30T09:35:15.430736785Z                 raise RuntimeError(
2024-12-30T09:35:15.430759481Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.430763370Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.430765653Z                 )
2024-12-30T09:35:15.430789289Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.430791707Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.430793916Z             if _cudart is None:
2024-12-30T09:35:15.430796122Z                 raise AssertionError(
2024-12-30T09:35:15.430809053Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.430811431Z                 )
2024-12-30T09:35:15.430830490Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.430832895Z             # are found or any other error occurs
2024-12-30T09:35:15.430834845Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.430876200Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.430880471Z >           torch._C._cuda_init()
2024-12-30T09:35:15.430895690Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.430900951Z 
2024-12-30T09:35:15.430924394Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.431089667Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-16-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.431093312Z 
2024-12-30T09:35:15.431137078Z b = 6, h = 8, n = 2048, d = 16, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.431141055Z 
2024-12-30T09:35:15.431324609Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.431329038Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.431331217Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.431333183Z         torch.manual_seed(2024)
2024-12-30T09:35:15.431338057Z         device = torch.device("cuda")
2024-12-30T09:35:15.431342504Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.431378032Z 
2024-12-30T09:35:15.431383039Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.431413593Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.431417516Z 
2024-12-30T09:35:15.431588809Z     def _lazy_init():
2024-12-30T09:35:15.431591787Z         global _initialized, _queued_calls
2024-12-30T09:35:15.431610356Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.431614450Z             return
2024-12-30T09:35:15.431616852Z         with _initialization_lock:
2024-12-30T09:35:15.431620002Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.431638243Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.431641262Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.431654601Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.431656842Z             # find there is nothing left to do.
2024-12-30T09:35:15.431724883Z             if is_initialized():
2024-12-30T09:35:15.431727442Z                 return
2024-12-30T09:35:15.431729319Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.431731323Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.431733343Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.431735477Z             if _is_in_bad_fork():
2024-12-30T09:35:15.431737420Z                 raise RuntimeError(
2024-12-30T09:35:15.431743520Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.431760954Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.431765326Z                 )
2024-12-30T09:35:15.431768424Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.431771804Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.431781969Z             if _cudart is None:
2024-12-30T09:35:15.431785419Z                 raise AssertionError(
2024-12-30T09:35:15.431787403Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.431801038Z                 )
2024-12-30T09:35:15.431803165Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.431821072Z             # are found or any other error occurs
2024-12-30T09:35:15.431823599Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.431825693Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.431843985Z >           torch._C._cuda_init()
2024-12-30T09:35:15.431854269Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.431864149Z 
2024-12-30T09:35:15.431888030Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.432060683Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-256-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.432066407Z 
2024-12-30T09:35:15.432108112Z b = 6, h = 8, n = 256, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.432111223Z 
2024-12-30T09:35:15.432299793Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.432302925Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.432305012Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.432306981Z         torch.manual_seed(2024)
2024-12-30T09:35:15.432309402Z         device = torch.device("cuda")
2024-12-30T09:35:15.432311894Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.432341310Z 
2024-12-30T09:35:15.432345194Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.432391542Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.432401826Z 
2024-12-30T09:35:15.432619944Z     def _lazy_init():
2024-12-30T09:35:15.432627988Z         global _initialized, _queued_calls
2024-12-30T09:35:15.432630067Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.432632255Z             return
2024-12-30T09:35:15.432634289Z         with _initialization_lock:
2024-12-30T09:35:15.432636324Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.432696864Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.432700378Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.432702502Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.432704497Z             # find there is nothing left to do.
2024-12-30T09:35:15.432706428Z             if is_initialized():
2024-12-30T09:35:15.432708382Z                 return
2024-12-30T09:35:15.432710310Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.432712366Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.432714425Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.432717039Z             if _is_in_bad_fork():
2024-12-30T09:35:15.432719135Z                 raise RuntimeError(
2024-12-30T09:35:15.432721068Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.432746174Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.432750718Z                 )
2024-12-30T09:35:15.432752711Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.432754792Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.432770260Z             if _cudart is None:
2024-12-30T09:35:15.432772493Z                 raise AssertionError(
2024-12-30T09:35:15.432774601Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.432813480Z                 )
2024-12-30T09:35:15.432820346Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.432823883Z             # are found or any other error occurs
2024-12-30T09:35:15.432827189Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.432830952Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.432835292Z >           torch._C._cuda_init()
2024-12-30T09:35:15.432857016Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.432865767Z 
2024-12-30T09:35:15.432879403Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.433056103Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-512-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.433063058Z 
2024-12-30T09:35:15.433085746Z b = 6, h = 8, n = 512, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.433089686Z 
2024-12-30T09:35:15.433299769Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.433305431Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.433308628Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.433311823Z         torch.manual_seed(2024)
2024-12-30T09:35:15.433315082Z         device = torch.device("cuda")
2024-12-30T09:35:15.433318412Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.433324361Z 
2024-12-30T09:35:15.433355907Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.433381674Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.433385239Z 
2024-12-30T09:35:15.433602236Z     def _lazy_init():
2024-12-30T09:35:15.433606087Z         global _initialized, _queued_calls
2024-12-30T09:35:15.433608044Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.433610328Z             return
2024-12-30T09:35:15.433655809Z         with _initialization_lock:
2024-12-30T09:35:15.433660161Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.433662273Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.433664294Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.433666306Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.433668882Z             # find there is nothing left to do.
2024-12-30T09:35:15.433671244Z             if is_initialized():
2024-12-30T09:35:15.433673215Z                 return
2024-12-30T09:35:15.433675198Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.433678961Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.433685133Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.433715526Z             if _is_in_bad_fork():
2024-12-30T09:35:15.433722639Z                 raise RuntimeError(
2024-12-30T09:35:15.433724687Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.433727002Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.433729266Z                 )
2024-12-30T09:35:15.433734729Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.433738427Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.433765307Z             if _cudart is None:
2024-12-30T09:35:15.433768802Z                 raise AssertionError(
2024-12-30T09:35:15.433770725Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.433772914Z                 )
2024-12-30T09:35:15.433784745Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.433786964Z             # are found or any other error occurs
2024-12-30T09:35:15.433800522Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.433811458Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.433813766Z >           torch._C._cuda_init()
2024-12-30T09:35:15.433834595Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.433846779Z 
2024-12-30T09:35:15.433868022Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.434091398Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-1024-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.434097405Z 
2024-12-30T09:35:15.434131586Z b = 6, h = 8, n = 1024, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.434135680Z 
2024-12-30T09:35:15.434371723Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.434375631Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.434377817Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.434379997Z         torch.manual_seed(2024)
2024-12-30T09:35:15.434381960Z         device = torch.device("cuda")
2024-12-30T09:35:15.434383971Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.434406557Z 
2024-12-30T09:35:15.434416196Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.434457873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.434467340Z 
2024-12-30T09:35:15.434710227Z     def _lazy_init():
2024-12-30T09:35:15.434714143Z         global _initialized, _queued_calls
2024-12-30T09:35:15.434716132Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.434718354Z             return
2024-12-30T09:35:15.434741751Z         with _initialization_lock:
2024-12-30T09:35:15.434744404Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.434746519Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.434760245Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.434767340Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.434792154Z             # find there is nothing left to do.
2024-12-30T09:35:15.434795705Z             if is_initialized():
2024-12-30T09:35:15.434797648Z                 return
2024-12-30T09:35:15.434825329Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.434827960Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.434829999Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.434869761Z             if _is_in_bad_fork():
2024-12-30T09:35:15.434874917Z                 raise RuntimeError(
2024-12-30T09:35:15.434878377Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.434882155Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.434886681Z                 )
2024-12-30T09:35:15.434890120Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.434911160Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.434915528Z             if _cudart is None:
2024-12-30T09:35:15.434926498Z                 raise AssertionError(
2024-12-30T09:35:15.434928810Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.434954443Z                 )
2024-12-30T09:35:15.434957398Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.434959421Z             # are found or any other error occurs
2024-12-30T09:35:15.434979505Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.434987755Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.435006746Z >           torch._C._cuda_init()
2024-12-30T09:35:15.435029767Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.435033972Z 
2024-12-30T09:35:15.435095935Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.435247878Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.435252651Z 
2024-12-30T09:35:15.435309512Z b = 6, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.435322215Z 
2024-12-30T09:35:15.435485154Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.435489620Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.435491872Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.435494035Z         torch.manual_seed(2024)
2024-12-30T09:35:15.435496098Z         device = torch.device("cuda")
2024-12-30T09:35:15.435509630Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.435515091Z 
2024-12-30T09:35:15.435539631Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.435557504Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.435563177Z 
2024-12-30T09:35:15.435757695Z     def _lazy_init():
2024-12-30T09:35:15.435762829Z         global _initialized, _queued_calls
2024-12-30T09:35:15.435765251Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.435768000Z             return
2024-12-30T09:35:15.435770064Z         with _initialization_lock:
2024-12-30T09:35:15.435772293Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.435774572Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.435787441Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.435793797Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.435801355Z             # find there is nothing left to do.
2024-12-30T09:35:15.435807733Z             if is_initialized():
2024-12-30T09:35:15.435817259Z                 return
2024-12-30T09:35:15.435821609Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.435825307Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.435830018Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.435833609Z             if _is_in_bad_fork():
2024-12-30T09:35:15.435844960Z                 raise RuntimeError(
2024-12-30T09:35:15.435850181Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.435886318Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.435890125Z                 )
2024-12-30T09:35:15.435892119Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.435894306Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.435896408Z             if _cudart is None:
2024-12-30T09:35:15.435923508Z                 raise AssertionError(
2024-12-30T09:35:15.435928881Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.435932549Z                 )
2024-12-30T09:35:15.435935792Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.435939235Z             # are found or any other error occurs
2024-12-30T09:35:15.435942954Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.435947174Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.435951014Z >           torch._C._cuda_init()
2024-12-30T09:35:15.435988730Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.435992380Z 
2024-12-30T09:35:15.436011255Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.436180080Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-4096-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.436186005Z 
2024-12-30T09:35:15.436215248Z b = 6, h = 8, n = 4096, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.436219492Z 
2024-12-30T09:35:15.436408782Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.436413028Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.436415205Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.436417326Z         torch.manual_seed(2024)
2024-12-30T09:35:15.436426703Z         device = torch.device("cuda")
2024-12-30T09:35:15.436428894Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.436439706Z 
2024-12-30T09:35:15.436444277Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.436459605Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.436462553Z 
2024-12-30T09:35:15.436664406Z     def _lazy_init():
2024-12-30T09:35:15.436667870Z         global _initialized, _queued_calls
2024-12-30T09:35:15.436669830Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.436672029Z             return
2024-12-30T09:35:15.436674193Z         with _initialization_lock:
2024-12-30T09:35:15.436694883Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.436698139Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.436700275Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.436718940Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.436721458Z             # find there is nothing left to do.
2024-12-30T09:35:15.436723474Z             if is_initialized():
2024-12-30T09:35:15.436736874Z                 return
2024-12-30T09:35:15.436739011Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.436766631Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.436769184Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.436771162Z             if _is_in_bad_fork():
2024-12-30T09:35:15.436773175Z                 raise RuntimeError(
2024-12-30T09:35:15.436781516Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.436787783Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.436819795Z                 )
2024-12-30T09:35:15.436824172Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.436827751Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.436831345Z             if _cudart is None:
2024-12-30T09:35:15.436834812Z                 raise AssertionError(
2024-12-30T09:35:15.436857891Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.436864757Z                 )
2024-12-30T09:35:15.436875783Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.436880955Z             # are found or any other error occurs
2024-12-30T09:35:15.436883410Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.436886390Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.436888715Z >           torch._C._cuda_init()
2024-12-30T09:35:15.436920184Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.436926896Z 
2024-12-30T09:35:15.436937187Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.437107764Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-8192-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.437112848Z 
2024-12-30T09:35:15.437122426Z b = 6, h = 8, n = 8192, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.437124747Z 
2024-12-30T09:35:15.437303768Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.437307594Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.437309749Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.437316382Z         torch.manual_seed(2024)
2024-12-30T09:35:15.437320714Z         device = torch.device("cuda")
2024-12-30T09:35:15.437332135Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.437337190Z 
2024-12-30T09:35:15.437404938Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.437409599Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.437411975Z 
2024-12-30T09:35:15.437596209Z     def _lazy_init():
2024-12-30T09:35:15.437599380Z         global _initialized, _queued_calls
2024-12-30T09:35:15.437601479Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.437603645Z             return
2024-12-30T09:35:15.437605771Z         with _initialization_lock:
2024-12-30T09:35:15.437622085Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.437625895Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.437693425Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.437704535Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.437706799Z             # find there is nothing left to do.
2024-12-30T09:35:15.437708760Z             if is_initialized():
2024-12-30T09:35:15.437710715Z                 return
2024-12-30T09:35:15.437712757Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.437714930Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.437717406Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.437719469Z             if _is_in_bad_fork():
2024-12-30T09:35:15.437722382Z                 raise RuntimeError(
2024-12-30T09:35:15.437724448Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.437735821Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.437741647Z                 )
2024-12-30T09:35:15.437743693Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.437762246Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.437764702Z             if _cudart is None:
2024-12-30T09:35:15.437766760Z                 raise AssertionError(
2024-12-30T09:35:15.437769216Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.437788374Z                 )
2024-12-30T09:35:15.437793764Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.437796104Z             # are found or any other error occurs
2024-12-30T09:35:15.437817780Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.437821287Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.437823369Z >           torch._C._cuda_init()
2024-12-30T09:35:15.437836817Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.437842839Z 
2024-12-30T09:35:15.437868383Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.438014682Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-32-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.438018209Z 
2024-12-30T09:35:15.438052385Z b = 6, h = 8, n = 2048, d = 32, e = 64, dtype = torch.float16
2024-12-30T09:35:15.438054907Z 
2024-12-30T09:35:15.438242979Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.438251056Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.438253625Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.438255627Z         torch.manual_seed(2024)
2024-12-30T09:35:15.438257594Z         device = torch.device("cuda")
2024-12-30T09:35:15.438259731Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.438264201Z 
2024-12-30T09:35:15.438266330Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.438320030Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.438324564Z 
2024-12-30T09:35:15.438533839Z     def _lazy_init():
2024-12-30T09:35:15.438547445Z         global _initialized, _queued_calls
2024-12-30T09:35:15.438560574Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.438563185Z             return
2024-12-30T09:35:15.438565650Z         with _initialization_lock:
2024-12-30T09:35:15.438567662Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.438569736Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.438572145Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.438575344Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.438577487Z             # find there is nothing left to do.
2024-12-30T09:35:15.438579482Z             if is_initialized():
2024-12-30T09:35:15.438581452Z                 return
2024-12-30T09:35:15.438584375Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.438586480Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.438588513Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.438610049Z             if _is_in_bad_fork():
2024-12-30T09:35:15.438616710Z                 raise RuntimeError(
2024-12-30T09:35:15.438618837Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.438637245Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.438641309Z                 )
2024-12-30T09:35:15.438643286Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.438645274Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.438650943Z             if _cudart is None:
2024-12-30T09:35:15.438653143Z                 raise AssertionError(
2024-12-30T09:35:15.438692198Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.438695886Z                 )
2024-12-30T09:35:15.438697760Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.438699778Z             # are found or any other error occurs
2024-12-30T09:35:15.438701704Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.438704417Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.438751497Z >           torch._C._cuda_init()
2024-12-30T09:35:15.438754995Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.438757499Z 
2024-12-30T09:35:15.438759376Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.438945629Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-64-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.438960980Z 
2024-12-30T09:35:15.438965108Z b = 6, h = 8, n = 2048, d = 64, e = 64, dtype = torch.float16
2024-12-30T09:35:15.438968178Z 
2024-12-30T09:35:15.439140174Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.439144154Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.439146351Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.439148317Z         torch.manual_seed(2024)
2024-12-30T09:35:15.439150418Z         device = torch.device("cuda")
2024-12-30T09:35:15.439152591Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.439178339Z 
2024-12-30T09:35:15.439187735Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.439214442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.439232016Z 
2024-12-30T09:35:15.439402176Z     def _lazy_init():
2024-12-30T09:35:15.439407855Z         global _initialized, _queued_calls
2024-12-30T09:35:15.439410142Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.439412650Z             return
2024-12-30T09:35:15.439416086Z         with _initialization_lock:
2024-12-30T09:35:15.439424311Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.439434238Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.439439893Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.439442034Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.439463993Z             # find there is nothing left to do.
2024-12-30T09:35:15.439466447Z             if is_initialized():
2024-12-30T09:35:15.439468514Z                 return
2024-12-30T09:35:15.439470887Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.439482799Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.439485138Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.439487310Z             if _is_in_bad_fork():
2024-12-30T09:35:15.439581157Z                 raise RuntimeError(
2024-12-30T09:35:15.439587170Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.439589933Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.439592099Z                 )
2024-12-30T09:35:15.439594518Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.439596962Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.439599411Z             if _cudart is None:
2024-12-30T09:35:15.439601367Z                 raise AssertionError(
2024-12-30T09:35:15.439603420Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.439605668Z                 )
2024-12-30T09:35:15.439609356Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.439611911Z             # are found or any other error occurs
2024-12-30T09:35:15.439614093Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.439616153Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.439618397Z >           torch._C._cuda_init()
2024-12-30T09:35:15.439661646Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.439668196Z 
2024-12-30T09:35:15.439670283Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.439838933Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-12-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.439849308Z 
2024-12-30T09:35:15.439851833Z b = 6, h = 12, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.439854249Z 
2024-12-30T09:35:15.440041555Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.440046937Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.440049702Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.440052149Z         torch.manual_seed(2024)
2024-12-30T09:35:15.440054579Z         device = torch.device("cuda")
2024-12-30T09:35:15.440056696Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.440066556Z 
2024-12-30T09:35:15.440073975Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.440108288Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.440112689Z 
2024-12-30T09:35:15.440264385Z     def _lazy_init():
2024-12-30T09:35:15.440267840Z         global _initialized, _queued_calls
2024-12-30T09:35:15.440269965Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.440290493Z             return
2024-12-30T09:35:15.440293801Z         with _initialization_lock:
2024-12-30T09:35:15.440296031Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.440298529Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.440327517Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.440333046Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.440335283Z             # find there is nothing left to do.
2024-12-30T09:35:15.440355394Z             if is_initialized():
2024-12-30T09:35:15.440370400Z                 return
2024-12-30T09:35:15.440372576Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.440374654Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.440378562Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.440380742Z             if _is_in_bad_fork():
2024-12-30T09:35:15.440382694Z                 raise RuntimeError(
2024-12-30T09:35:15.440402293Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.440406972Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.440415217Z                 )
2024-12-30T09:35:15.440417987Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.440420265Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.440422358Z             if _cudart is None:
2024-12-30T09:35:15.440453248Z                 raise AssertionError(
2024-12-30T09:35:15.440456553Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.440459029Z                 )
2024-12-30T09:35:15.440460943Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.440488728Z             # are found or any other error occurs
2024-12-30T09:35:15.440493913Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.440497208Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.440500086Z >           torch._C._cuda_init()
2024-12-30T09:35:15.440517051Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.440520604Z 
2024-12-30T09:35:15.440527443Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.440699725Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-16-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.440704262Z 
2024-12-30T09:35:15.440734157Z b = 6, h = 16, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.440738206Z 
2024-12-30T09:35:15.440911112Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.440914347Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.440916409Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.440918445Z         torch.manual_seed(2024)
2024-12-30T09:35:15.440920365Z         device = torch.device("cuda")
2024-12-30T09:35:15.440966789Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.440971929Z 
2024-12-30T09:35:15.440973960Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.440976139Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.440978533Z 
2024-12-30T09:35:15.441163670Z     def _lazy_init():
2024-12-30T09:35:15.441167043Z         global _initialized, _queued_calls
2024-12-30T09:35:15.441176190Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.441178582Z             return
2024-12-30T09:35:15.441180634Z         with _initialization_lock:
2024-12-30T09:35:15.441182676Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.441185488Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.441188219Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.441202606Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.441208598Z             # find there is nothing left to do.
2024-12-30T09:35:15.441221881Z             if is_initialized():
2024-12-30T09:35:15.441226533Z                 return
2024-12-30T09:35:15.441229373Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.441257383Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.441260773Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.441262841Z             if _is_in_bad_fork():
2024-12-30T09:35:15.441264811Z                 raise RuntimeError(
2024-12-30T09:35:15.441266731Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.441288805Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.441292177Z                 )
2024-12-30T09:35:15.441294118Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.441296383Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.441306732Z             if _cudart is None:
2024-12-30T09:35:15.441310522Z                 raise AssertionError(
2024-12-30T09:35:15.441338175Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.441342269Z                 )
2024-12-30T09:35:15.441344466Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.441346589Z             # are found or any other error occurs
2024-12-30T09:35:15.441348660Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.441386295Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.441390713Z >           torch._C._cuda_init()
2024-12-30T09:35:15.441392855Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.441395518Z 
2024-12-30T09:35:15.441415004Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.441567029Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-20-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.441570975Z 
2024-12-30T09:35:15.441602819Z b = 6, h = 20, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.441605594Z 
2024-12-30T09:35:15.441787673Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.441790388Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.441792526Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.441794506Z         torch.manual_seed(2024)
2024-12-30T09:35:15.441796451Z         device = torch.device("cuda")
2024-12-30T09:35:15.441798473Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.441810952Z 
2024-12-30T09:35:15.441909129Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.441918552Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.441920922Z 
2024-12-30T09:35:15.442034282Z     def _lazy_init():
2024-12-30T09:35:15.442037831Z         global _initialized, _queued_calls
2024-12-30T09:35:15.442039797Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.442041971Z             return
2024-12-30T09:35:15.442043842Z         with _initialization_lock:
2024-12-30T09:35:15.442103257Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.442106052Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.442108238Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.442110334Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.442112293Z             # find there is nothing left to do.
2024-12-30T09:35:15.442114274Z             if is_initialized():
2024-12-30T09:35:15.442116440Z                 return
2024-12-30T09:35:15.442118330Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.442120729Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.442123956Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.442128384Z             if _is_in_bad_fork():
2024-12-30T09:35:15.442131342Z                 raise RuntimeError(
2024-12-30T09:35:15.442162394Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.442165318Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.442167458Z                 )
2024-12-30T09:35:15.442200236Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.442204238Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.442206406Z             if _cudart is None:
2024-12-30T09:35:15.442208523Z                 raise AssertionError(
2024-12-30T09:35:15.442210525Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.442212864Z                 )
2024-12-30T09:35:15.442215591Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.442230761Z             # are found or any other error occurs
2024-12-30T09:35:15.442241487Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.442243876Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.442249924Z >           torch._C._cuda_init()
2024-12-30T09:35:15.442273277Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.442276819Z 
2024-12-30T09:35:15.442283138Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.442443647Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-1-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.442447070Z 
2024-12-30T09:35:15.442480921Z b = 1, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.442483739Z 
2024-12-30T09:35:15.442667098Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.442671133Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.442673266Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.442675216Z         torch.manual_seed(2024)
2024-12-30T09:35:15.442690680Z         device = torch.device("cuda")
2024-12-30T09:35:15.442694915Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.442697102Z 
2024-12-30T09:35:15.442713133Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.442749329Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.442759463Z 
2024-12-30T09:35:15.442916149Z     def _lazy_init():
2024-12-30T09:35:15.442921250Z         global _initialized, _queued_calls
2024-12-30T09:35:15.442923298Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.442925544Z             return
2024-12-30T09:35:15.442927744Z         with _initialization_lock:
2024-12-30T09:35:15.442950471Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.442954211Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.442956272Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.442958261Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.442980810Z             # find there is nothing left to do.
2024-12-30T09:35:15.442984298Z             if is_initialized():
2024-12-30T09:35:15.442986319Z                 return
2024-12-30T09:35:15.442988268Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.443003356Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.443005641Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.443007866Z             if _is_in_bad_fork():
2024-12-30T09:35:15.443026076Z                 raise RuntimeError(
2024-12-30T09:35:15.443029314Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.443032371Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.443034536Z                 )
2024-12-30T09:35:15.443069550Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.443072935Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.443075006Z             if _cudart is None:
2024-12-30T09:35:15.443077039Z                 raise AssertionError(
2024-12-30T09:35:15.443092591Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.443095945Z                 )
2024-12-30T09:35:15.443109072Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.443112596Z             # are found or any other error occurs
2024-12-30T09:35:15.443114537Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.443126714Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.443129060Z >           torch._C._cuda_init()
2024-12-30T09:35:15.443186106Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.443189885Z 
2024-12-30T09:35:15.443191745Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.443327301Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-2-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.443330506Z 
2024-12-30T09:35:15.443380311Z b = 2, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.443383355Z 
2024-12-30T09:35:15.443574388Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.443577901Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.443579954Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.443581982Z         torch.manual_seed(2024)
2024-12-30T09:35:15.443584027Z         device = torch.device("cuda")
2024-12-30T09:35:15.443586054Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.443588131Z 
2024-12-30T09:35:15.443607699Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.443612786Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.443615246Z 
2024-12-30T09:35:15.443813354Z     def _lazy_init():
2024-12-30T09:35:15.443818755Z         global _initialized, _queued_calls
2024-12-30T09:35:15.443828974Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.443831296Z             return
2024-12-30T09:35:15.443833222Z         with _initialization_lock:
2024-12-30T09:35:15.443835172Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.443842096Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.443845370Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.443847568Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.443854688Z             # find there is nothing left to do.
2024-12-30T09:35:15.443870192Z             if is_initialized():
2024-12-30T09:35:15.443872919Z                 return
2024-12-30T09:35:15.443875050Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.443883476Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.443891027Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.443907782Z             if _is_in_bad_fork():
2024-12-30T09:35:15.443911182Z                 raise RuntimeError(
2024-12-30T09:35:15.443926323Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.443928809Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.443930917Z                 )
2024-12-30T09:35:15.443952446Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.443955951Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.443975739Z             if _cudart is None:
2024-12-30T09:35:15.443978738Z                 raise AssertionError(
2024-12-30T09:35:15.443980750Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.443982984Z                 )
2024-12-30T09:35:15.443984915Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.444033103Z             # are found or any other error occurs
2024-12-30T09:35:15.444036916Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.444039136Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.444041233Z >           torch._C._cuda_init()
2024-12-30T09:35:15.444043244Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.444059958Z 
2024-12-30T09:35:15.444062149Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.444238410Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-3-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.444242978Z 
2024-12-30T09:35:15.444283433Z b = 3, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.444286293Z 
2024-12-30T09:35:15.444484656Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.444488681Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.444490949Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.444492911Z         torch.manual_seed(2024)
2024-12-30T09:35:15.444494884Z         device = torch.device("cuda")
2024-12-30T09:35:15.444496995Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.444518181Z 
2024-12-30T09:35:15.444521738Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.444556230Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.444559824Z 
2024-12-30T09:35:15.444735795Z     def _lazy_init():
2024-12-30T09:35:15.444738832Z         global _initialized, _queued_calls
2024-12-30T09:35:15.444740897Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.444743066Z             return
2024-12-30T09:35:15.444756092Z         with _initialization_lock:
2024-12-30T09:35:15.444760758Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.444787172Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.444797585Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.444799936Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.444801947Z             # find there is nothing left to do.
2024-12-30T09:35:15.444804596Z             if is_initialized():
2024-12-30T09:35:15.444806730Z                 return
2024-12-30T09:35:15.444824324Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.444827956Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.444830024Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.444843260Z             if _is_in_bad_fork():
2024-12-30T09:35:15.444845494Z                 raise RuntimeError(
2024-12-30T09:35:15.444854032Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.444856419Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.444875004Z                 )
2024-12-30T09:35:15.444882801Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.444908951Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.444912545Z             if _cudart is None:
2024-12-30T09:35:15.444914603Z                 raise AssertionError(
2024-12-30T09:35:15.444916594Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.444918709Z                 )
2024-12-30T09:35:15.444921192Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.444923358Z             # are found or any other error occurs
2024-12-30T09:35:15.444992306Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.444996303Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.444998463Z >           torch._C._cuda_init()
2024-12-30T09:35:15.445000705Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.445003300Z 
2024-12-30T09:35:15.445005351Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.445165512Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-913-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.445170033Z 
2024-12-30T09:35:15.445210816Z b = 6, h = 8, n = 913, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.445214483Z 
2024-12-30T09:35:15.445416962Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.445421553Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.445423756Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.445425887Z         torch.manual_seed(2024)
2024-12-30T09:35:15.445427831Z         device = torch.device("cuda")
2024-12-30T09:35:15.445429820Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.445432018Z 
2024-12-30T09:35:15.445474371Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.445477625Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.445479871Z 
2024-12-30T09:35:15.445668165Z     def _lazy_init():
2024-12-30T09:35:15.445671479Z         global _initialized, _queued_calls
2024-12-30T09:35:15.445673541Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.445675598Z             return
2024-12-30T09:35:15.445697896Z         with _initialization_lock:
2024-12-30T09:35:15.445700170Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.445702236Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.445713474Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.445719597Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.445727254Z             # find there is nothing left to do.
2024-12-30T09:35:15.445751721Z             if is_initialized():
2024-12-30T09:35:15.445756324Z                 return
2024-12-30T09:35:15.445758323Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.445760417Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.445788875Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.445792427Z             if _is_in_bad_fork():
2024-12-30T09:35:15.445794393Z                 raise RuntimeError(
2024-12-30T09:35:15.445796337Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.445798438Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.445850309Z                 )
2024-12-30T09:35:15.445853549Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.445855614Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.445857703Z             if _cudart is None:
2024-12-30T09:35:15.445859709Z                 raise AssertionError(
2024-12-30T09:35:15.445868505Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.445870603Z                 )
2024-12-30T09:35:15.445873866Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.445876051Z             # are found or any other error occurs
2024-12-30T09:35:15.445878201Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.445880819Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.445909750Z >           torch._C._cuda_init()
2024-12-30T09:35:15.445915673Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.445919873Z 
2024-12-30T09:35:15.445952228Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.446101124Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-513-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.446106612Z 
2024-12-30T09:35:15.446137015Z b = 6, h = 8, n = 513, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.446150271Z 
2024-12-30T09:35:15.446346229Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.446352026Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.446354568Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.446373171Z         torch.manual_seed(2024)
2024-12-30T09:35:15.446375852Z         device = torch.device("cuda")
2024-12-30T09:35:15.446378072Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.446381497Z 
2024-12-30T09:35:15.446398208Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.446404963Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.446419930Z 
2024-12-30T09:35:15.446592828Z     def _lazy_init():
2024-12-30T09:35:15.446596583Z         global _initialized, _queued_calls
2024-12-30T09:35:15.446598666Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.446610661Z             return
2024-12-30T09:35:15.446612891Z         with _initialization_lock:
2024-12-30T09:35:15.446651157Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.446666626Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.446668969Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.446671084Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.446683728Z             # find there is nothing left to do.
2024-12-30T09:35:15.446686047Z             if is_initialized():
2024-12-30T09:35:15.446688189Z                 return
2024-12-30T09:35:15.446690415Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.446692613Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.446704321Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.446707440Z             if _is_in_bad_fork():
2024-12-30T09:35:15.446717666Z                 raise RuntimeError(
2024-12-30T09:35:15.446720891Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.446750186Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.446754790Z                 )
2024-12-30T09:35:15.446757086Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.446759181Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.446762935Z             if _cudart is None:
2024-12-30T09:35:15.446767427Z                 raise AssertionError(
2024-12-30T09:35:15.446785942Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.446794353Z                 )
2024-12-30T09:35:15.446796370Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.446806394Z             # are found or any other error occurs
2024-12-30T09:35:15.446808606Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.446857702Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.446861551Z >           torch._C._cuda_init()
2024-12-30T09:35:15.446863612Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.446866338Z 
2024-12-30T09:35:15.446890331Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.447047562Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-1213-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.447057039Z 
2024-12-30T09:35:15.447062930Z b = 6, h = 8, n = 1213, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.447066242Z 
2024-12-30T09:35:15.447256445Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.447262365Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.447265774Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.447268925Z         torch.manual_seed(2024)
2024-12-30T09:35:15.447272173Z         device = torch.device("cuda")
2024-12-30T09:35:15.447275420Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.447278967Z 
2024-12-30T09:35:15.447282509Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.447303090Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.447307954Z 
2024-12-30T09:35:15.447526848Z     def _lazy_init():
2024-12-30T09:35:15.447538308Z         global _initialized, _queued_calls
2024-12-30T09:35:15.447541324Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.447547110Z             return
2024-12-30T09:35:15.447569390Z         with _initialization_lock:
2024-12-30T09:35:15.447572790Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.447575674Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.447585891Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.447593726Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.447599547Z             # find there is nothing left to do.
2024-12-30T09:35:15.447601602Z             if is_initialized():
2024-12-30T09:35:15.447603720Z                 return
2024-12-30T09:35:15.447605650Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.447627809Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.447632660Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.447634755Z             if _is_in_bad_fork():
2024-12-30T09:35:15.447636700Z                 raise RuntimeError(
2024-12-30T09:35:15.447638718Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.447640940Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.447657423Z                 )
2024-12-30T09:35:15.447661287Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.447663438Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.447679364Z             if _cudart is None:
2024-12-30T09:35:15.447683405Z                 raise AssertionError(
2024-12-30T09:35:15.447685421Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.447688387Z                 )
2024-12-30T09:35:15.447708912Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.447712065Z             # are found or any other error occurs
2024-12-30T09:35:15.447714074Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.447734932Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.447737288Z >           torch._C._cuda_init()
2024-12-30T09:35:15.447739265Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.447753082Z 
2024-12-30T09:35:15.447781402Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.447954700Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-16-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.447960324Z 
2024-12-30T09:35:15.448002647Z b = 6, h = 8, n = 2048, d = 16, e = 64, dtype = torch.float16
2024-12-30T09:35:15.448033429Z 
2024-12-30T09:35:15.448160124Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.448164437Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.448166712Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.448168815Z         torch.manual_seed(2024)
2024-12-30T09:35:15.448170877Z         device = torch.device("cuda")
2024-12-30T09:35:15.448172889Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.448179646Z 
2024-12-30T09:35:15.448183841Z [1m[31mtests/ops/test_lightning2.py[0m:39: 
2024-12-30T09:35:15.448201328Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.448204643Z 
2024-12-30T09:35:15.448418680Z     def _lazy_init():
2024-12-30T09:35:15.448424497Z         global _initialized, _queued_calls
2024-12-30T09:35:15.448427465Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.448430475Z             return
2024-12-30T09:35:15.448433329Z         with _initialization_lock:
2024-12-30T09:35:15.448436177Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.448439203Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.448442076Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.448445134Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.448448828Z             # find there is nothing left to do.
2024-12-30T09:35:15.448451480Z             if is_initialized():
2024-12-30T09:35:15.448454246Z                 return
2024-12-30T09:35:15.448482124Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.448487441Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.448489562Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.448491629Z             if _is_in_bad_fork():
2024-12-30T09:35:15.448505788Z                 raise RuntimeError(
2024-12-30T09:35:15.448510448Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.448512717Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.448514850Z                 )
2024-12-30T09:35:15.448529852Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.448532366Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.448575567Z             if _cudart is None:
2024-12-30T09:35:15.448580214Z                 raise AssertionError(
2024-12-30T09:35:15.448582178Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.448584443Z                 )
2024-12-30T09:35:15.448586399Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.448598566Z             # are found or any other error occurs
2024-12-30T09:35:15.448602586Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.448604831Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.448624559Z >           torch._C._cuda_init()
2024-12-30T09:35:15.448627600Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.448649420Z 
2024-12-30T09:35:15.448658030Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.448817401Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-256-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.448821308Z 
2024-12-30T09:35:15.448857424Z b = 6, h = 8, n = 256, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.448860416Z 
2024-12-30T09:35:15.449033701Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.449036996Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.449039235Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.449043186Z         torch.manual_seed(2024)
2024-12-30T09:35:15.449045386Z         device = torch.device("cuda")
2024-12-30T09:35:15.449047566Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.449049727Z 
2024-12-30T09:35:15.449083441Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.449091853Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.449095947Z 
2024-12-30T09:35:15.449283158Z     def _lazy_init():
2024-12-30T09:35:15.449286793Z         global _initialized, _queued_calls
2024-12-30T09:35:15.449288859Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.449291071Z             return
2024-12-30T09:35:15.449297579Z         with _initialization_lock:
2024-12-30T09:35:15.449299635Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.449322959Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.449325780Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.449327858Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.449329977Z             # find there is nothing left to do.
2024-12-30T09:35:15.449332025Z             if is_initialized():
2024-12-30T09:35:15.449342784Z                 return
2024-12-30T09:35:15.449347034Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.449386032Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.449391446Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.449393619Z             if _is_in_bad_fork():
2024-12-30T09:35:15.449395568Z                 raise RuntimeError(
2024-12-30T09:35:15.449397616Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.449430752Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.449434406Z                 )
2024-12-30T09:35:15.449436351Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.449438319Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.449440553Z             if _cudart is None:
2024-12-30T09:35:15.449442447Z                 raise AssertionError(
2024-12-30T09:35:15.449454515Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.449458737Z                 )
2024-12-30T09:35:15.449472807Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.449475143Z             # are found or any other error occurs
2024-12-30T09:35:15.449479275Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.449493330Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.449496689Z >           torch._C._cuda_init()
2024-12-30T09:35:15.449506725Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.449509496Z 
2024-12-30T09:35:15.449559862Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.449700056Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-512-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.449707477Z 
2024-12-30T09:35:15.449745097Z b = 6, h = 8, n = 512, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.449752143Z 
2024-12-30T09:35:15.449913132Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.449916676Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.449918789Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.449920736Z         torch.manual_seed(2024)
2024-12-30T09:35:15.449923845Z         device = torch.device("cuda")
2024-12-30T09:35:15.449936697Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.449940631Z 
2024-12-30T09:35:15.449965427Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.449968419Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.449970882Z 
2024-12-30T09:35:15.450158133Z     def _lazy_init():
2024-12-30T09:35:15.450161922Z         global _initialized, _queued_calls
2024-12-30T09:35:15.450163818Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.450165906Z             return
2024-12-30T09:35:15.450167745Z         with _initialization_lock:
2024-12-30T09:35:15.450169739Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.450192393Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.450196954Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.450200362Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.450210156Z             # find there is nothing left to do.
2024-12-30T09:35:15.450214221Z             if is_initialized():
2024-12-30T09:35:15.450230078Z                 return
2024-12-30T09:35:15.450244480Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.450247319Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.450250600Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.450252837Z             if _is_in_bad_fork():
2024-12-30T09:35:15.450271060Z                 raise RuntimeError(
2024-12-30T09:35:15.450274057Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.450276305Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.450307269Z                 )
2024-12-30T09:35:15.450311195Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.450313449Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.450315675Z             if _cudart is None:
2024-12-30T09:35:15.450324127Z                 raise AssertionError(
2024-12-30T09:35:15.450328015Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.450330173Z                 )
2024-12-30T09:35:15.450342253Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.450347898Z             # are found or any other error occurs
2024-12-30T09:35:15.450370889Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.450374154Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.450376243Z >           torch._C._cuda_init()
2024-12-30T09:35:15.450392508Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.450396486Z 
2024-12-30T09:35:15.450405146Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.450565248Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-1024-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.450569698Z 
2024-12-30T09:35:15.450607415Z b = 6, h = 8, n = 1024, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.450610173Z 
2024-12-30T09:35:15.450783374Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.450786838Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.450789006Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.450791051Z         torch.manual_seed(2024)
2024-12-30T09:35:15.450792984Z         device = torch.device("cuda")
2024-12-30T09:35:15.450808725Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.450812072Z 
2024-12-30T09:35:15.450831572Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.450847867Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.450850578Z 
2024-12-30T09:35:15.451048103Z     def _lazy_init():
2024-12-30T09:35:15.451060037Z         global _initialized, _queued_calls
2024-12-30T09:35:15.451062447Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.451064724Z             return
2024-12-30T09:35:15.451066717Z         with _initialization_lock:
2024-12-30T09:35:15.451069632Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.451071775Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.451073788Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.451079226Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.451082823Z             # find there is nothing left to do.
2024-12-30T09:35:15.451087175Z             if is_initialized():
2024-12-30T09:35:15.451138579Z                 return
2024-12-30T09:35:15.451141548Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.451143638Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.451145660Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.451147682Z             if _is_in_bad_fork():
2024-12-30T09:35:15.451149642Z                 raise RuntimeError(
2024-12-30T09:35:15.451151713Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.451153753Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.451178386Z                 )
2024-12-30T09:35:15.451181897Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.451184041Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.451186148Z             if _cudart is None:
2024-12-30T09:35:15.451188057Z                 raise AssertionError(
2024-12-30T09:35:15.451231502Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.451236222Z                 )
2024-12-30T09:35:15.451238174Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.451240250Z             # are found or any other error occurs
2024-12-30T09:35:15.451242345Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.451246982Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.451254664Z >           torch._C._cuda_init()
2024-12-30T09:35:15.451257003Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.451263614Z 
2024-12-30T09:35:15.451296683Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.451441758Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.451447703Z 
2024-12-30T09:35:15.451482301Z b = 6, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.451487202Z 
2024-12-30T09:35:15.451660999Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.451665741Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.451668027Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.451670035Z         torch.manual_seed(2024)
2024-12-30T09:35:15.451672096Z         device = torch.device("cuda")
2024-12-30T09:35:15.451674290Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.451685443Z 
2024-12-30T09:35:15.451724662Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.451727873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.451730168Z 
2024-12-30T09:35:15.451914931Z     def _lazy_init():
2024-12-30T09:35:15.451919704Z         global _initialized, _queued_calls
2024-12-30T09:35:15.451921685Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.451923861Z             return
2024-12-30T09:35:15.451925812Z         with _initialization_lock:
2024-12-30T09:35:15.451927815Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.451952253Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.451957084Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.451960341Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.451963171Z             # find there is nothing left to do.
2024-12-30T09:35:15.451988607Z             if is_initialized():
2024-12-30T09:35:15.451993153Z                 return
2024-12-30T09:35:15.451995622Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.451997824Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.451999886Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.452029823Z             if _is_in_bad_fork():
2024-12-30T09:35:15.452033198Z                 raise RuntimeError(
2024-12-30T09:35:15.452035182Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.452037379Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.452058124Z                 )
2024-12-30T09:35:15.452061552Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.452063665Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.452065708Z             if _cudart is None:
2024-12-30T09:35:15.452081298Z                 raise AssertionError(
2024-12-30T09:35:15.452083648Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.452085785Z                 )
2024-12-30T09:35:15.452087787Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.452099217Z             # are found or any other error occurs
2024-12-30T09:35:15.452101380Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.452124460Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.452127217Z >           torch._C._cuda_init()
2024-12-30T09:35:15.452154680Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.452157502Z 
2024-12-30T09:35:15.452176192Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.452329580Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-4096-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.452334308Z 
2024-12-30T09:35:15.452369886Z b = 6, h = 8, n = 4096, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.452373062Z 
2024-12-30T09:35:15.452544543Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.452547533Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.452559348Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.452562018Z         torch.manual_seed(2024)
2024-12-30T09:35:15.452564000Z         device = torch.device("cuda")
2024-12-30T09:35:15.452566091Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.452589447Z 
2024-12-30T09:35:15.452593787Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.452620988Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.452625605Z 
2024-12-30T09:35:15.452813596Z     def _lazy_init():
2024-12-30T09:35:15.452818053Z         global _initialized, _queued_calls
2024-12-30T09:35:15.452820096Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.452822379Z             return
2024-12-30T09:35:15.452824386Z         with _initialization_lock:
2024-12-30T09:35:15.452826547Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.452829310Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.452831372Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.452854080Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.452860282Z             # find there is nothing left to do.
2024-12-30T09:35:15.452889392Z             if is_initialized():
2024-12-30T09:35:15.452893299Z                 return
2024-12-30T09:35:15.452895359Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.452897458Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.452899468Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.452901532Z             if _is_in_bad_fork():
2024-12-30T09:35:15.452926746Z                 raise RuntimeError(
2024-12-30T09:35:15.452930023Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.452932284Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.452934339Z                 )
2024-12-30T09:35:15.452936227Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.452960047Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.452963328Z             if _cudart is None:
2024-12-30T09:35:15.452965680Z                 raise AssertionError(
2024-12-30T09:35:15.452984784Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.452987994Z                 )
2024-12-30T09:35:15.452989921Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.453023029Z             # are found or any other error occurs
2024-12-30T09:35:15.453026493Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.453028833Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.453041533Z >           torch._C._cuda_init()
2024-12-30T09:35:15.453044253Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.453046792Z 
2024-12-30T09:35:15.453067198Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.453211719Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-8192-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.453215440Z 
2024-12-30T09:35:15.453259164Z b = 6, h = 8, n = 8192, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.453262372Z 
2024-12-30T09:35:15.453440956Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.453446241Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.453448412Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.453450414Z         torch.manual_seed(2024)
2024-12-30T09:35:15.453452400Z         device = torch.device("cuda")
2024-12-30T09:35:15.453454585Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.453481493Z 
2024-12-30T09:35:15.453492713Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.453503405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.453510226Z 
2024-12-30T09:35:15.453718743Z     def _lazy_init():
2024-12-30T09:35:15.453727986Z         global _initialized, _queued_calls
2024-12-30T09:35:15.453730108Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.453732301Z             return
2024-12-30T09:35:15.453734294Z         with _initialization_lock:
2024-12-30T09:35:15.453736236Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.453738239Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.453740249Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.453801255Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.453804695Z             # find there is nothing left to do.
2024-12-30T09:35:15.453806763Z             if is_initialized():
2024-12-30T09:35:15.453808807Z                 return
2024-12-30T09:35:15.453810853Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.453817242Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.453819313Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.453821293Z             if _is_in_bad_fork():
2024-12-30T09:35:15.453823333Z                 raise RuntimeError(
2024-12-30T09:35:15.453825269Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.453837344Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.453842488Z                 )
2024-12-30T09:35:15.453844464Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.453846573Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.453869612Z             if _cudart is None:
2024-12-30T09:35:15.453872905Z                 raise AssertionError(
2024-12-30T09:35:15.453874866Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.453877063Z                 )
2024-12-30T09:35:15.453885014Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.453887294Z             # are found or any other error occurs
2024-12-30T09:35:15.453895950Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.453898169Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.453963211Z >           torch._C._cuda_init()
2024-12-30T09:35:15.453966091Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.453968478Z 
2024-12-30T09:35:15.453970335Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.454110960Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-32-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.454115546Z 
2024-12-30T09:35:15.454150381Z b = 6, h = 8, n = 2048, d = 32, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.454153293Z 
2024-12-30T09:35:15.454327918Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.454334741Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.454338123Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.454341428Z         torch.manual_seed(2024)
2024-12-30T09:35:15.454344434Z         device = torch.device("cuda")
2024-12-30T09:35:15.454353708Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.454356446Z 
2024-12-30T09:35:15.454372883Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.454392808Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.454396376Z 
2024-12-30T09:35:15.454596653Z     def _lazy_init():
2024-12-30T09:35:15.454605421Z         global _initialized, _queued_calls
2024-12-30T09:35:15.454607874Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.454610139Z             return
2024-12-30T09:35:15.454639022Z         with _initialization_lock:
2024-12-30T09:35:15.454642233Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.454644262Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.454646282Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.454648276Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.454676792Z             # find there is nothing left to do.
2024-12-30T09:35:15.454679571Z             if is_initialized():
2024-12-30T09:35:15.454682621Z                 return
2024-12-30T09:35:15.454684565Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.454686726Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.454702275Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.454707376Z             if _is_in_bad_fork():
2024-12-30T09:35:15.454709412Z                 raise RuntimeError(
2024-12-30T09:35:15.454733142Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.454736377Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.454738567Z                 )
2024-12-30T09:35:15.454740584Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.454764608Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.454767834Z             if _cudart is None:
2024-12-30T09:35:15.454769794Z                 raise AssertionError(
2024-12-30T09:35:15.454771805Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.454797481Z                 )
2024-12-30T09:35:15.454800515Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.454808010Z             # are found or any other error occurs
2024-12-30T09:35:15.454810078Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.454820600Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.454824266Z >           torch._C._cuda_init()
2024-12-30T09:35:15.454842315Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.454849086Z 
2024-12-30T09:35:15.454858057Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.455017284Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-64-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.455021120Z 
2024-12-30T09:35:15.455054730Z b = 6, h = 8, n = 2048, d = 64, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.455057650Z 
2024-12-30T09:35:15.455220648Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.455224237Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.455226484Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.455233317Z         torch.manual_seed(2024)
2024-12-30T09:35:15.455235471Z         device = torch.device("cuda")
2024-12-30T09:35:15.455261034Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.455265639Z 
2024-12-30T09:35:15.455304535Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.455307998Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.455310371Z 
2024-12-30T09:35:15.455525234Z     def _lazy_init():
2024-12-30T09:35:15.455528582Z         global _initialized, _queued_calls
2024-12-30T09:35:15.455530562Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.455532723Z             return
2024-12-30T09:35:15.455534669Z         with _initialization_lock:
2024-12-30T09:35:15.455585220Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.455593816Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.455596888Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.455599913Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.455609987Z             # find there is nothing left to do.
2024-12-30T09:35:15.455612042Z             if is_initialized():
2024-12-30T09:35:15.455614334Z                 return
2024-12-30T09:35:15.455617103Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.455619210Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.455621356Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.455624041Z             if _is_in_bad_fork():
2024-12-30T09:35:15.455626040Z                 raise RuntimeError(
2024-12-30T09:35:15.455642922Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.455652298Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.455663990Z                 )
2024-12-30T09:35:15.455667478Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.455705699Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.455708123Z             if _cudart is None:
2024-12-30T09:35:15.455710224Z                 raise AssertionError(
2024-12-30T09:35:15.455712160Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.455714327Z                 )
2024-12-30T09:35:15.455716324Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.455719639Z             # are found or any other error occurs
2024-12-30T09:35:15.455721725Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.455724431Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.455754827Z >           torch._C._cuda_init()
2024-12-30T09:35:15.455759013Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.455761659Z 
2024-12-30T09:35:15.455814575Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.455941692Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-12-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.455945252Z 
2024-12-30T09:35:15.455977686Z b = 6, h = 12, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.455980683Z 
2024-12-30T09:35:15.456146971Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.456157318Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.456160507Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.456163661Z         torch.manual_seed(2024)
2024-12-30T09:35:15.456191070Z         device = torch.device("cuda")
2024-12-30T09:35:15.456196784Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.456199046Z 
2024-12-30T09:35:15.456200893Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.456234302Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.456239621Z 
2024-12-30T09:35:15.456448345Z     def _lazy_init():
2024-12-30T09:35:15.456452092Z         global _initialized, _queued_calls
2024-12-30T09:35:15.456454111Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.456456296Z             return
2024-12-30T09:35:15.456458184Z         with _initialization_lock:
2024-12-30T09:35:15.456491563Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.456500883Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.456504033Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.456507241Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.456510478Z             # find there is nothing left to do.
2024-12-30T09:35:15.456535126Z             if is_initialized():
2024-12-30T09:35:15.456537523Z                 return
2024-12-30T09:35:15.456539496Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.456541565Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.456543582Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.456557560Z             if _is_in_bad_fork():
2024-12-30T09:35:15.456561325Z                 raise RuntimeError(
2024-12-30T09:35:15.456563404Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.456565530Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.456577860Z                 )
2024-12-30T09:35:15.456581365Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.456603330Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.456609834Z             if _cudart is None:
2024-12-30T09:35:15.456618532Z                 raise AssertionError(
2024-12-30T09:35:15.456630427Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.456632861Z                 )
2024-12-30T09:35:15.456634803Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.456636887Z             # are found or any other error occurs
2024-12-30T09:35:15.456674018Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.456691303Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.456693782Z >           torch._C._cuda_init()
2024-12-30T09:35:15.456696740Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.456699480Z 
2024-12-30T09:35:15.456743710Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.456881422Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-16-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.456885028Z 
2024-12-30T09:35:15.456910879Z b = 6, h = 16, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.456913635Z 
2024-12-30T09:35:15.457154991Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.457158804Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.457161078Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.457163080Z         torch.manual_seed(2024)
2024-12-30T09:35:15.457165190Z         device = torch.device("cuda")
2024-12-30T09:35:15.457167337Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.457169538Z 
2024-12-30T09:35:15.457171387Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.457173485Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.457175771Z 
2024-12-30T09:35:15.457371348Z     def _lazy_init():
2024-12-30T09:35:15.457374935Z         global _initialized, _queued_calls
2024-12-30T09:35:15.457376984Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.457379281Z             return
2024-12-30T09:35:15.457381334Z         with _initialization_lock:
2024-12-30T09:35:15.457383460Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.457390901Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.457410330Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.457416012Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.457418952Z             # find there is nothing left to do.
2024-12-30T09:35:15.457421629Z             if is_initialized():
2024-12-30T09:35:15.457424918Z                 return
2024-12-30T09:35:15.457428542Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.457432373Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.457460456Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.457463879Z             if _is_in_bad_fork():
2024-12-30T09:35:15.457465888Z                 raise RuntimeError(
2024-12-30T09:35:15.457467843Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.457471036Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.457492805Z                 )
2024-12-30T09:35:15.457497904Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.457500911Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.457504318Z             if _cudart is None:
2024-12-30T09:35:15.457507984Z                 raise AssertionError(
2024-12-30T09:35:15.457510631Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.457526173Z                 )
2024-12-30T09:35:15.457559822Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.457622560Z             # are found or any other error occurs
2024-12-30T09:35:15.457626850Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.457629035Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.457631085Z >           torch._C._cuda_init()
2024-12-30T09:35:15.457633181Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.457635565Z 
2024-12-30T09:35:15.457637403Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.457783848Z [31m[1m_________________________________________________________________________ test_lightning2[dtype0-6-20-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.457793090Z 
2024-12-30T09:35:15.457820495Z b = 6, h = 20, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.457827495Z 
2024-12-30T09:35:15.458019145Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.458023496Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.458025607Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.458027679Z         torch.manual_seed(2024)
2024-12-30T09:35:15.458029581Z         device = torch.device("cuda")
2024-12-30T09:35:15.458031693Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.458034276Z 
2024-12-30T09:35:15.458050514Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.458065613Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.458069058Z 
2024-12-30T09:35:15.458275837Z     def _lazy_init():
2024-12-30T09:35:15.458279944Z         global _initialized, _queued_calls
2024-12-30T09:35:15.458281978Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.458284331Z             return
2024-12-30T09:35:15.458286348Z         with _initialization_lock:
2024-12-30T09:35:15.458288316Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.458290421Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.458308970Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.458315314Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.458317421Z             # find there is nothing left to do.
2024-12-30T09:35:15.458319584Z             if is_initialized():
2024-12-30T09:35:15.458321538Z                 return
2024-12-30T09:35:15.458324169Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.458333005Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.458336410Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.458371044Z             if _is_in_bad_fork():
2024-12-30T09:35:15.458375432Z                 raise RuntimeError(
2024-12-30T09:35:15.458378332Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.458381330Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.458402954Z                 )
2024-12-30T09:35:15.458409756Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.458411876Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.458413969Z             if _cudart is None:
2024-12-30T09:35:15.458444296Z                 raise AssertionError(
2024-12-30T09:35:15.458452024Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.458454377Z                 )
2024-12-30T09:35:15.458456307Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.458458431Z             # are found or any other error occurs
2024-12-30T09:35:15.458538315Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.458543803Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.458546979Z >           torch._C._cuda_init()
2024-12-30T09:35:15.458555638Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.458558924Z 
2024-12-30T09:35:15.458560813Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.458684901Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-1-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.458690372Z 
2024-12-30T09:35:15.458722199Z b = 1, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.458726375Z 
2024-12-30T09:35:15.458910918Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.458914265Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.458916429Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.458918504Z         torch.manual_seed(2024)
2024-12-30T09:35:15.458920528Z         device = torch.device("cuda")
2024-12-30T09:35:15.458922525Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.458924637Z 
2024-12-30T09:35:15.458948906Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.458953679Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.458964008Z 
2024-12-30T09:35:15.459175486Z     def _lazy_init():
2024-12-30T09:35:15.459178811Z         global _initialized, _queued_calls
2024-12-30T09:35:15.459189022Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.459191264Z             return
2024-12-30T09:35:15.459193176Z         with _initialization_lock:
2024-12-30T09:35:15.459195186Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.459197178Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.459199293Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.459201284Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.459222868Z             # find there is nothing left to do.
2024-12-30T09:35:15.459227752Z             if is_initialized():
2024-12-30T09:35:15.459230054Z                 return
2024-12-30T09:35:15.459232114Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.459234533Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.459236590Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.459258888Z             if _is_in_bad_fork():
2024-12-30T09:35:15.459262009Z                 raise RuntimeError(
2024-12-30T09:35:15.459264039Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.459266401Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.459295547Z                 )
2024-12-30T09:35:15.459298736Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.459300889Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.459302943Z             if _cudart is None:
2024-12-30T09:35:15.459304923Z                 raise AssertionError(
2024-12-30T09:35:15.459306935Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.459309520Z                 )
2024-12-30T09:35:15.459354919Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.459369918Z             # are found or any other error occurs
2024-12-30T09:35:15.459372153Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.459374332Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.459376467Z >           torch._C._cuda_init()
2024-12-30T09:35:15.459396856Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.459400384Z 
2024-12-30T09:35:15.459402333Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.459568697Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-2-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.459574605Z 
2024-12-30T09:35:15.459598081Z b = 2, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.459601438Z 
2024-12-30T09:35:15.459786117Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.459793877Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.459795998Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.459797927Z         torch.manual_seed(2024)
2024-12-30T09:35:15.459799845Z         device = torch.device("cuda")
2024-12-30T09:35:15.459801934Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.459804110Z 
2024-12-30T09:35:15.459812383Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.459816664Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.459837818Z 
2024-12-30T09:35:15.460037032Z     def _lazy_init():
2024-12-30T09:35:15.460040189Z         global _initialized, _queued_calls
2024-12-30T09:35:15.460042186Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.460044463Z             return
2024-12-30T09:35:15.460046520Z         with _initialization_lock:
2024-12-30T09:35:15.460048434Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.460050470Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.460052448Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.460055935Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.460058091Z             # find there is nothing left to do.
2024-12-30T09:35:15.460060114Z             if is_initialized():
2024-12-30T09:35:15.460080983Z                 return
2024-12-30T09:35:15.460085748Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.460100470Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.460103841Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.460105848Z             if _is_in_bad_fork():
2024-12-30T09:35:15.460108369Z                 raise RuntimeError(
2024-12-30T09:35:15.460142864Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.460145936Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.460147981Z                 )
2024-12-30T09:35:15.460149954Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.460152081Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.460227356Z             if _cudart is None:
2024-12-30T09:35:15.460231135Z                 raise AssertionError(
2024-12-30T09:35:15.460233137Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.460235362Z                 )
2024-12-30T09:35:15.460237281Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.460239346Z             # are found or any other error occurs
2024-12-30T09:35:15.460241233Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.460243222Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.460245311Z >           torch._C._cuda_init()
2024-12-30T09:35:15.460259499Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.460266828Z 
2024-12-30T09:35:15.460278903Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.460437418Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-3-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.460441697Z 
2024-12-30T09:35:15.460471321Z b = 3, h = 8, n = 2048, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.460474351Z 
2024-12-30T09:35:15.460661863Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.460666075Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.460668350Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.460670489Z         torch.manual_seed(2024)
2024-12-30T09:35:15.460672462Z         device = torch.device("cuda")
2024-12-30T09:35:15.460674564Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.460694223Z 
2024-12-30T09:35:15.460699312Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.460713039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.460723411Z 
2024-12-30T09:35:15.460918050Z     def _lazy_init():
2024-12-30T09:35:15.460922977Z         global _initialized, _queued_calls
2024-12-30T09:35:15.460925103Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.460927343Z             return
2024-12-30T09:35:15.460929426Z         with _initialization_lock:
2024-12-30T09:35:15.460931341Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.460934087Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.460936346Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.460954348Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.460958095Z             # find there is nothing left to do.
2024-12-30T09:35:15.460960189Z             if is_initialized():
2024-12-30T09:35:15.460974452Z                 return
2024-12-30T09:35:15.460978990Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.460986944Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.460991454Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.461009246Z             if _is_in_bad_fork():
2024-12-30T09:35:15.461012358Z                 raise RuntimeError(
2024-12-30T09:35:15.461021934Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.461024231Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.461041571Z                 )
2024-12-30T09:35:15.461044653Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.461068185Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.461070791Z             if _cudart is None:
2024-12-30T09:35:15.461072736Z                 raise AssertionError(
2024-12-30T09:35:15.461074647Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.461084209Z                 )
2024-12-30T09:35:15.461087231Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.461107904Z             # are found or any other error occurs
2024-12-30T09:35:15.461111203Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.461113385Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.461139681Z >           torch._C._cuda_init()
2024-12-30T09:35:15.461148253Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.461150961Z 
2024-12-30T09:35:15.461166269Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.461310365Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-913-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.461314102Z 
2024-12-30T09:35:15.461369946Z b = 6, h = 8, n = 913, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.461374362Z 
2024-12-30T09:35:15.461533499Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.461541991Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.461544443Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.461546474Z         torch.manual_seed(2024)
2024-12-30T09:35:15.461557455Z         device = torch.device("cuda")
2024-12-30T09:35:15.461560555Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.461563276Z 
2024-12-30T09:35:15.461596408Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.461601323Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.461603735Z 
2024-12-30T09:35:15.461780752Z     def _lazy_init():
2024-12-30T09:35:15.461785593Z         global _initialized, _queued_calls
2024-12-30T09:35:15.461787637Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.461789794Z             return
2024-12-30T09:35:15.461815756Z         with _initialization_lock:
2024-12-30T09:35:15.461820888Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.461824286Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.461827548Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.461830773Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.461833904Z             # find there is nothing left to do.
2024-12-30T09:35:15.461886939Z             if is_initialized():
2024-12-30T09:35:15.461890596Z                 return
2024-12-30T09:35:15.461892541Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.461894578Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.461901842Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.461903946Z             if _is_in_bad_fork():
2024-12-30T09:35:15.461905833Z                 raise RuntimeError(
2024-12-30T09:35:15.461907715Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.461910410Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.461912643Z                 )
2024-12-30T09:35:15.461914543Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.461920466Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.461924023Z             if _cudart is None:
2024-12-30T09:35:15.461982826Z                 raise AssertionError(
2024-12-30T09:35:15.461985738Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.461988052Z                 )
2024-12-30T09:35:15.461990023Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.461992041Z             # are found or any other error occurs
2024-12-30T09:35:15.461994009Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.461996098Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.461998197Z >           torch._C._cuda_init()
2024-12-30T09:35:15.462035670Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.462041612Z 
2024-12-30T09:35:15.462044683Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.462176910Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-513-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.462180608Z 
2024-12-30T09:35:15.462220889Z b = 6, h = 8, n = 513, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.462223358Z 
2024-12-30T09:35:15.462389936Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.462393318Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.462395508Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.462397446Z         torch.manual_seed(2024)
2024-12-30T09:35:15.462421962Z         device = torch.device("cuda")
2024-12-30T09:35:15.462424423Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.462433341Z 
2024-12-30T09:35:15.462460724Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.462476380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.462480171Z 
2024-12-30T09:35:15.462682374Z     def _lazy_init():
2024-12-30T09:35:15.462689554Z         global _initialized, _queued_calls
2024-12-30T09:35:15.462692885Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.462696337Z             return
2024-12-30T09:35:15.462699517Z         with _initialization_lock:
2024-12-30T09:35:15.462702815Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.462710631Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.462714906Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.462717707Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.462719788Z             # find there is nothing left to do.
2024-12-30T09:35:15.462721845Z             if is_initialized():
2024-12-30T09:35:15.462739083Z                 return
2024-12-30T09:35:15.462744229Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.462746400Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.462753914Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.462756077Z             if _is_in_bad_fork():
2024-12-30T09:35:15.462780283Z                 raise RuntimeError(
2024-12-30T09:35:15.462783026Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.462785288Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.462802388Z                 )
2024-12-30T09:35:15.462805470Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.462807638Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.462835430Z             if _cudart is None:
2024-12-30T09:35:15.462847080Z                 raise AssertionError(
2024-12-30T09:35:15.462849190Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.462851895Z                 )
2024-12-30T09:35:15.462854503Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.462856647Z             # are found or any other error occurs
2024-12-30T09:35:15.462881319Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.462884985Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.462887079Z >           torch._C._cuda_init()
2024-12-30T09:35:15.462941771Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.462947120Z 
2024-12-30T09:35:15.462949063Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.463081587Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-1213-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.463085047Z 
2024-12-30T09:35:15.463115424Z b = 6, h = 8, n = 1213, d = 128, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.463118017Z 
2024-12-30T09:35:15.463297904Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.463303759Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.463309058Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.463312195Z         torch.manual_seed(2024)
2024-12-30T09:35:15.463315371Z         device = torch.device("cuda")
2024-12-30T09:35:15.463318767Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.463322779Z 
2024-12-30T09:35:15.463339276Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.463389909Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.463394023Z 
2024-12-30T09:35:15.463585031Z     def _lazy_init():
2024-12-30T09:35:15.463589961Z         global _initialized, _queued_calls
2024-12-30T09:35:15.463593261Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.463596324Z             return
2024-12-30T09:35:15.463598970Z         with _initialization_lock:
2024-12-30T09:35:15.463601937Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.463605161Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.463607989Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.463611487Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.463625281Z             # find there is nothing left to do.
2024-12-30T09:35:15.463635730Z             if is_initialized():
2024-12-30T09:35:15.463637887Z                 return
2024-12-30T09:35:15.463640029Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.463660190Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.463664272Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.463666270Z             if _is_in_bad_fork():
2024-12-30T09:35:15.463668316Z                 raise RuntimeError(
2024-12-30T09:35:15.463670276Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.463692085Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.463699665Z                 )
2024-12-30T09:35:15.463701728Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.463703819Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.463705905Z             if _cudart is None:
2024-12-30T09:35:15.463721561Z                 raise AssertionError(
2024-12-30T09:35:15.463723920Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.463726125Z                 )
2024-12-30T09:35:15.463758009Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.463764599Z             # are found or any other error occurs
2024-12-30T09:35:15.463766607Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.463768656Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.463770822Z >           torch._C._cuda_init()
2024-12-30T09:35:15.463817283Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.463826262Z 
2024-12-30T09:35:15.463828249Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.463963022Z [31m[1m__________________________________________________________________________ test_lightning2[dtype0-6-8-2048-16-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.463972068Z 
2024-12-30T09:35:15.464001512Z b = 6, h = 8, n = 2048, d = 16, e = 64, dtype = torch.bfloat16
2024-12-30T09:35:15.464004247Z 
2024-12-30T09:35:15.464159271Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.464161976Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.464164141Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.464176122Z         torch.manual_seed(2024)
2024-12-30T09:35:15.464178156Z         device = torch.device("cuda")
2024-12-30T09:35:15.464180122Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.464199683Z 
2024-12-30T09:35:15.464204221Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.464239600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.464244734Z 
2024-12-30T09:35:15.464421586Z     def _lazy_init():
2024-12-30T09:35:15.464425296Z         global _initialized, _queued_calls
2024-12-30T09:35:15.464427296Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.464429488Z             return
2024-12-30T09:35:15.464431449Z         with _initialization_lock:
2024-12-30T09:35:15.464443539Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.464448809Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.464452961Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.464455794Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.464473208Z             # find there is nothing left to do.
2024-12-30T09:35:15.464476229Z             if is_initialized():
2024-12-30T09:35:15.464478107Z                 return
2024-12-30T09:35:15.464514284Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.464526909Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.464529522Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.464531994Z             if _is_in_bad_fork():
2024-12-30T09:35:15.464534060Z                 raise RuntimeError(
2024-12-30T09:35:15.464562171Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.464566747Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.464569991Z                 )
2024-12-30T09:35:15.464571943Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.464573951Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.464576143Z             if _cudart is None:
2024-12-30T09:35:15.464592781Z                 raise AssertionError(
2024-12-30T09:35:15.464595888Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.464601195Z                 )
2024-12-30T09:35:15.464610079Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.464612389Z             # are found or any other error occurs
2024-12-30T09:35:15.464614703Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.464622391Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.464632509Z >           torch._C._cuda_init()
2024-12-30T09:35:15.464649387Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.464652522Z 
2024-12-30T09:35:15.464670200Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.464816369Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-256-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.464819616Z 
2024-12-30T09:35:15.464860383Z b = 6, h = 8, n = 256, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.464862949Z 
2024-12-30T09:35:15.465032361Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.465035048Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.465037042Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.465039094Z         torch.manual_seed(2024)
2024-12-30T09:35:15.465041129Z         device = torch.device("cuda")
2024-12-30T09:35:15.465108082Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.465112135Z 
2024-12-30T09:35:15.465114355Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.465116490Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.465118772Z 
2024-12-30T09:35:15.465288217Z     def _lazy_init():
2024-12-30T09:35:15.465291317Z         global _initialized, _queued_calls
2024-12-30T09:35:15.465293243Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.465295505Z             return
2024-12-30T09:35:15.465297406Z         with _initialization_lock:
2024-12-30T09:35:15.465322269Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.465325376Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.465334587Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.465336677Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.465356091Z             # find there is nothing left to do.
2024-12-30T09:35:15.465369637Z             if is_initialized():
2024-12-30T09:35:15.465372557Z                 return
2024-12-30T09:35:15.465375817Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.465378923Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.465396867Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.465400244Z             if _is_in_bad_fork():
2024-12-30T09:35:15.465402277Z                 raise RuntimeError(
2024-12-30T09:35:15.465417503Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.465422789Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.465434380Z                 )
2024-12-30T09:35:15.465438098Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.465440662Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.465472833Z             if _cudart is None:
2024-12-30T09:35:15.465478256Z                 raise AssertionError(
2024-12-30T09:35:15.465480228Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.465506829Z                 )
2024-12-30T09:35:15.465509802Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.465511796Z             # are found or any other error occurs
2024-12-30T09:35:15.465513788Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.465515814Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.465537403Z >           torch._C._cuda_init()
2024-12-30T09:35:15.465542275Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.465577294Z 
2024-12-30T09:35:15.465586335Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.465739634Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-512-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.465743895Z 
2024-12-30T09:35:15.465789055Z b = 6, h = 8, n = 512, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.465798178Z 
2024-12-30T09:35:15.465969575Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.465974651Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.465976884Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.465979053Z         torch.manual_seed(2024)
2024-12-30T09:35:15.465981021Z         device = torch.device("cuda")
2024-12-30T09:35:15.465983138Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.465985848Z 
2024-12-30T09:35:15.466016847Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.466019805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.466022256Z 
2024-12-30T09:35:15.466230898Z     def _lazy_init():
2024-12-30T09:35:15.466235317Z         global _initialized, _queued_calls
2024-12-30T09:35:15.466237352Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.466239615Z             return
2024-12-30T09:35:15.466241517Z         with _initialization_lock:
2024-12-30T09:35:15.466243590Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.466245666Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.466273335Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.466276284Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.466278278Z             # find there is nothing left to do.
2024-12-30T09:35:15.466280190Z             if is_initialized():
2024-12-30T09:35:15.466282097Z                 return
2024-12-30T09:35:15.466305808Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.466311268Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.466314654Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.466317854Z             if _is_in_bad_fork():
2024-12-30T09:35:15.466320929Z                 raise RuntimeError(
2024-12-30T09:35:15.466324015Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.466329039Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.466332120Z                 )
2024-12-30T09:35:15.466351819Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.466355118Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.466375357Z             if _cudart is None:
2024-12-30T09:35:15.466377555Z                 raise AssertionError(
2024-12-30T09:35:15.466410501Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.466413291Z                 )
2024-12-30T09:35:15.466415140Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.466417160Z             # are found or any other error occurs
2024-12-30T09:35:15.466419070Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.466421241Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.466452402Z >           torch._C._cuda_init()
2024-12-30T09:35:15.466457282Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.466459845Z 
2024-12-30T09:35:15.466488147Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.466629517Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-1024-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.466635357Z 
2024-12-30T09:35:15.466670514Z b = 6, h = 8, n = 1024, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.466674374Z 
2024-12-30T09:35:15.466852293Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.466855810Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.466857991Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.466859947Z         torch.manual_seed(2024)
2024-12-30T09:35:15.466861922Z         device = torch.device("cuda")
2024-12-30T09:35:15.466863921Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.466879383Z 
2024-12-30T09:35:15.466884173Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.466914777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.466917951Z 
2024-12-30T09:35:15.467104180Z     def _lazy_init():
2024-12-30T09:35:15.467107416Z         global _initialized, _queued_calls
2024-12-30T09:35:15.467109407Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.467111631Z             return
2024-12-30T09:35:15.467120988Z         with _initialization_lock:
2024-12-30T09:35:15.467125379Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.467129114Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.467132107Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.467151849Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.467156805Z             # find there is nothing left to do.
2024-12-30T09:35:15.467159999Z             if is_initialized():
2024-12-30T09:35:15.467201925Z                 return
2024-12-30T09:35:15.467208032Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.467210236Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.467212300Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.467214411Z             if _is_in_bad_fork():
2024-12-30T09:35:15.467216374Z                 raise RuntimeError(
2024-12-30T09:35:15.467218296Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.467224183Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.467226366Z                 )
2024-12-30T09:35:15.467228246Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.467242189Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.467244610Z             if _cudart is None:
2024-12-30T09:35:15.467266934Z                 raise AssertionError(
2024-12-30T09:35:15.467269297Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.467271380Z                 )
2024-12-30T09:35:15.467305730Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.467308498Z             # are found or any other error occurs
2024-12-30T09:35:15.467310433Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.467346404Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.467349280Z >           torch._C._cuda_init()
2024-12-30T09:35:15.467352414Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.467356112Z 
2024-12-30T09:35:15.467367215Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.467532214Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.467555890Z 
2024-12-30T09:35:15.467582686Z b = 6, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.467588386Z 
2024-12-30T09:35:15.467758515Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.467762252Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.467764446Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.467766517Z         torch.manual_seed(2024)
2024-12-30T09:35:15.467768454Z         device = torch.device("cuda")
2024-12-30T09:35:15.467771290Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.467773442Z 
2024-12-30T09:35:15.467802698Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.467823929Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.467827171Z 
2024-12-30T09:35:15.468013523Z     def _lazy_init():
2024-12-30T09:35:15.468016566Z         global _initialized, _queued_calls
2024-12-30T09:35:15.468018585Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.468020821Z             return
2024-12-30T09:35:15.468022750Z         with _initialization_lock:
2024-12-30T09:35:15.468041534Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.468051324Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.468053453Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.468055439Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.468058373Z             # find there is nothing left to do.
2024-12-30T09:35:15.468060920Z             if is_initialized():
2024-12-30T09:35:15.468063144Z                 return
2024-12-30T09:35:15.468086395Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.468089660Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.468091773Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.468126928Z             if _is_in_bad_fork():
2024-12-30T09:35:15.468134271Z                 raise RuntimeError(
2024-12-30T09:35:15.468136443Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.468145253Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.468147443Z                 )
2024-12-30T09:35:15.468149453Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.468152407Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.468169028Z             if _cudart is None:
2024-12-30T09:35:15.468174159Z                 raise AssertionError(
2024-12-30T09:35:15.468177300Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.468181303Z                 )
2024-12-30T09:35:15.468184672Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.468188449Z             # are found or any other error occurs
2024-12-30T09:35:15.468190407Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.468204671Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.468212012Z >           torch._C._cuda_init()
2024-12-30T09:35:15.468235314Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.468239300Z 
2024-12-30T09:35:15.468262194Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.468408186Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-4096-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.468412051Z 
2024-12-30T09:35:15.468445219Z b = 6, h = 8, n = 4096, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.468449280Z 
2024-12-30T09:35:15.468643415Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.468647405Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.468649523Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.468651505Z         torch.manual_seed(2024)
2024-12-30T09:35:15.468653628Z         device = torch.device("cuda")
2024-12-30T09:35:15.468655672Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.468657750Z 
2024-12-30T09:35:15.468687124Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.468691613Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.468693985Z 
2024-12-30T09:35:15.468891374Z     def _lazy_init():
2024-12-30T09:35:15.468900987Z         global _initialized, _queued_calls
2024-12-30T09:35:15.468903213Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.468905464Z             return
2024-12-30T09:35:15.468907438Z         with _initialization_lock:
2024-12-30T09:35:15.468909432Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.468936025Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.468941963Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.468944098Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.468946423Z             # find there is nothing left to do.
2024-12-30T09:35:15.468948379Z             if is_initialized():
2024-12-30T09:35:15.468963111Z                 return
2024-12-30T09:35:15.468966521Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.468968708Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.468998349Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.469001825Z             if _is_in_bad_fork():
2024-12-30T09:35:15.469003818Z                 raise RuntimeError(
2024-12-30T09:35:15.469005768Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.469024154Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.469027359Z                 )
2024-12-30T09:35:15.469029464Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.469031628Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.469046342Z             if _cudart is None:
2024-12-30T09:35:15.469049451Z                 raise AssertionError(
2024-12-30T09:35:15.469051335Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.469053412Z                 )
2024-12-30T09:35:15.469082753Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.469086039Z             # are found or any other error occurs
2024-12-30T09:35:15.469088029Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.469090064Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.469092076Z >           torch._C._cuda_init()
2024-12-30T09:35:15.469142882Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.469151761Z 
2024-12-30T09:35:15.469153701Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.469281214Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-8192-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.469284930Z 
2024-12-30T09:35:15.469323476Z b = 6, h = 8, n = 8192, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.469326180Z 
2024-12-30T09:35:15.469520741Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.469525085Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.469527147Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.469529112Z         torch.manual_seed(2024)
2024-12-30T09:35:15.469531101Z         device = torch.device("cuda")
2024-12-30T09:35:15.469533069Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.469547156Z 
2024-12-30T09:35:15.469558520Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.469588708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.469594407Z 
2024-12-30T09:35:15.469763502Z     def _lazy_init():
2024-12-30T09:35:15.469767985Z         global _initialized, _queued_calls
2024-12-30T09:35:15.469770167Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.469772366Z             return
2024-12-30T09:35:15.469786074Z         with _initialization_lock:
2024-12-30T09:35:15.469789176Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.469791269Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.469817624Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.469821105Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.469823127Z             # find there is nothing left to do.
2024-12-30T09:35:15.469857466Z             if is_initialized():
2024-12-30T09:35:15.469860445Z                 return
2024-12-30T09:35:15.469862377Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.469864395Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.469866598Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.469872635Z             if _is_in_bad_fork():
2024-12-30T09:35:15.469909299Z                 raise RuntimeError(
2024-12-30T09:35:15.469911906Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.469914430Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.469916500Z                 )
2024-12-30T09:35:15.469918396Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.469920508Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.469923027Z             if _cudart is None:
2024-12-30T09:35:15.469946621Z                 raise AssertionError(
2024-12-30T09:35:15.469949077Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.469951122Z                 )
2024-12-30T09:35:15.469952976Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.469972952Z             # are found or any other error occurs
2024-12-30T09:35:15.469977039Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.469979345Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.469981990Z >           torch._C._cuda_init()
2024-12-30T09:35:15.470007400Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.470010945Z 
2024-12-30T09:35:15.470052184Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.470178872Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-32-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.470182140Z 
2024-12-30T09:35:15.470231850Z b = 6, h = 8, n = 2048, d = 32, e = 64, dtype = torch.float16
2024-12-30T09:35:15.470235426Z 
2024-12-30T09:35:15.470396307Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.470399602Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.470401693Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.470403641Z         torch.manual_seed(2024)
2024-12-30T09:35:15.470417423Z         device = torch.device("cuda")
2024-12-30T09:35:15.470424468Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.470426741Z 
2024-12-30T09:35:15.470440142Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.470481952Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.470485731Z 
2024-12-30T09:35:15.470658381Z     def _lazy_init():
2024-12-30T09:35:15.470662114Z         global _initialized, _queued_calls
2024-12-30T09:35:15.470664148Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.470666318Z             return
2024-12-30T09:35:15.470676196Z         with _initialization_lock:
2024-12-30T09:35:15.470679594Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.470707177Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.470710306Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.470712300Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.470714398Z             # find there is nothing left to do.
2024-12-30T09:35:15.470716979Z             if is_initialized():
2024-12-30T09:35:15.470730671Z                 return
2024-12-30T09:35:15.470735109Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.470740576Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.470743744Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.470776523Z             if _is_in_bad_fork():
2024-12-30T09:35:15.470780324Z                 raise RuntimeError(
2024-12-30T09:35:15.470782328Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.470785225Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.470787431Z                 )
2024-12-30T09:35:15.470789405Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.470809256Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.470812481Z             if _cudart is None:
2024-12-30T09:35:15.470819420Z                 raise AssertionError(
2024-12-30T09:35:15.470822231Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.470868164Z                 )
2024-12-30T09:35:15.470870789Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.470872832Z             # are found or any other error occurs
2024-12-30T09:35:15.470874907Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.470876893Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.470882130Z >           torch._C._cuda_init()
2024-12-30T09:35:15.470894621Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.470898526Z 
2024-12-30T09:35:15.470925387Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.471082356Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-64-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.471086104Z 
2024-12-30T09:35:15.471099462Z b = 6, h = 8, n = 2048, d = 64, e = 64, dtype = torch.float16
2024-12-30T09:35:15.471102619Z 
2024-12-30T09:35:15.471333713Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.471338085Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.471340308Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.471342338Z         torch.manual_seed(2024)
2024-12-30T09:35:15.471344317Z         device = torch.device("cuda")
2024-12-30T09:35:15.471354976Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.471366673Z 
2024-12-30T09:35:15.471422540Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.471431216Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.471433620Z 
2024-12-30T09:35:15.471687257Z     def _lazy_init():
2024-12-30T09:35:15.471690889Z         global _initialized, _queued_calls
2024-12-30T09:35:15.471693026Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.471695204Z             return
2024-12-30T09:35:15.471697128Z         with _initialization_lock:
2024-12-30T09:35:15.471699031Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.471701108Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.471703640Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.471705811Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.471736700Z             # find there is nothing left to do.
2024-12-30T09:35:15.471742099Z             if is_initialized():
2024-12-30T09:35:15.471745563Z                 return
2024-12-30T09:35:15.471756167Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.471759282Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.471762282Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.471801992Z             if _is_in_bad_fork():
2024-12-30T09:35:15.471806532Z                 raise RuntimeError(
2024-12-30T09:35:15.471808565Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.471810911Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.471812998Z                 )
2024-12-30T09:35:15.471815810Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.471817914Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.471820476Z             if _cudart is None:
2024-12-30T09:35:15.471846879Z                 raise AssertionError(
2024-12-30T09:35:15.471850282Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.471852567Z                 )
2024-12-30T09:35:15.471873801Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.471877720Z             # are found or any other error occurs
2024-12-30T09:35:15.471892916Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.471897405Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.471900431Z >           torch._C._cuda_init()
2024-12-30T09:35:15.471934120Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.471937604Z 
2024-12-30T09:35:15.471939497Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.472127311Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-12-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.472131532Z 
2024-12-30T09:35:15.472134016Z b = 6, h = 12, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.472136234Z 
2024-12-30T09:35:15.472352934Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.472365870Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.472369981Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.472373240Z         torch.manual_seed(2024)
2024-12-30T09:35:15.472375970Z         device = torch.device("cuda")
2024-12-30T09:35:15.472386318Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.472389426Z 
2024-12-30T09:35:15.472392748Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.472426621Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.472435045Z 
2024-12-30T09:35:15.472623502Z     def _lazy_init():
2024-12-30T09:35:15.472628676Z         global _initialized, _queued_calls
2024-12-30T09:35:15.472630874Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.472633090Z             return
2024-12-30T09:35:15.472635008Z         with _initialization_lock:
2024-12-30T09:35:15.472637055Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.472652671Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.472656793Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.472658901Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.472660971Z             # find there is nothing left to do.
2024-12-30T09:35:15.472699874Z             if is_initialized():
2024-12-30T09:35:15.472703996Z                 return
2024-12-30T09:35:15.472706007Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.472708364Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.472711256Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.472714558Z             if _is_in_bad_fork():
2024-12-30T09:35:15.472717512Z                 raise RuntimeError(
2024-12-30T09:35:15.472721337Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.472724490Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.472727688Z                 )
2024-12-30T09:35:15.472739520Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.472742793Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.472761686Z             if _cudart is None:
2024-12-30T09:35:15.472765140Z                 raise AssertionError(
2024-12-30T09:35:15.472785969Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.472789254Z                 )
2024-12-30T09:35:15.472791319Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.472799831Z             # are found or any other error occurs
2024-12-30T09:35:15.472802756Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.472804934Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.472838221Z >           torch._C._cuda_init()
2024-12-30T09:35:15.472846527Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.472849875Z 
2024-12-30T09:35:15.472851947Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.473031705Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-16-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.473036137Z 
2024-12-30T09:35:15.473062527Z b = 6, h = 16, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.473066723Z 
2024-12-30T09:35:15.473241657Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.473244968Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.473247248Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.473249285Z         torch.manual_seed(2024)
2024-12-30T09:35:15.473251220Z         device = torch.device("cuda")
2024-12-30T09:35:15.473284690Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.473287315Z 
2024-12-30T09:35:15.473289127Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.473331515Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.473337596Z 
2024-12-30T09:35:15.473521497Z     def _lazy_init():
2024-12-30T09:35:15.473524762Z         global _initialized, _queued_calls
2024-12-30T09:35:15.473526688Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.473528870Z             return
2024-12-30T09:35:15.473530848Z         with _initialization_lock:
2024-12-30T09:35:15.473532755Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.473534885Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.473576396Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.473579596Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.473586307Z             # find there is nothing left to do.
2024-12-30T09:35:15.473588332Z             if is_initialized():
2024-12-30T09:35:15.473591224Z                 return
2024-12-30T09:35:15.473593251Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.473595324Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.473610865Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.473615893Z             if _is_in_bad_fork():
2024-12-30T09:35:15.473617900Z                 raise RuntimeError(
2024-12-30T09:35:15.473810074Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.473813633Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.473815892Z                 )
2024-12-30T09:35:15.473817800Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.473819809Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.473821854Z             if _cudart is None:
2024-12-30T09:35:15.473823882Z                 raise AssertionError(
2024-12-30T09:35:15.473825826Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.473827902Z                 )
2024-12-30T09:35:15.473829790Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.473831951Z             # are found or any other error occurs
2024-12-30T09:35:15.473833926Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.473835949Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.473837980Z >           torch._C._cuda_init()
2024-12-30T09:35:15.473840103Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.473842351Z 
2024-12-30T09:35:15.473844204Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.473939796Z [31m[1m_________________________________________________________________________ test_lightning2[dtype1-6-20-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.473943565Z 
2024-12-30T09:35:15.473983153Z b = 6, h = 20, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.473986683Z 
2024-12-30T09:35:15.474146360Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.474153732Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.474155997Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.474158032Z         torch.manual_seed(2024)
2024-12-30T09:35:15.474179672Z         device = torch.device("cuda")
2024-12-30T09:35:15.474183268Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.474185426Z 
2024-12-30T09:35:15.474220626Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.474223536Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.474225827Z 
2024-12-30T09:35:15.474421053Z     def _lazy_init():
2024-12-30T09:35:15.474424279Z         global _initialized, _queued_calls
2024-12-30T09:35:15.474426247Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.474428568Z             return
2024-12-30T09:35:15.474451667Z         with _initialization_lock:
2024-12-30T09:35:15.474455020Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.474457184Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.474459275Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.474465023Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.474468056Z             # find there is nothing left to do.
2024-12-30T09:35:15.474498985Z             if is_initialized():
2024-12-30T09:35:15.474502053Z                 return
2024-12-30T09:35:15.474504045Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.474523252Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.474528292Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.474530356Z             if _is_in_bad_fork():
2024-12-30T09:35:15.474566855Z                 raise RuntimeError(
2024-12-30T09:35:15.474570428Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.474572585Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.474610157Z                 )
2024-12-30T09:35:15.474612662Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.474614655Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.474616782Z             if _cudart is None:
2024-12-30T09:35:15.474623070Z                 raise AssertionError(
2024-12-30T09:35:15.474625721Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.474628346Z                 )
2024-12-30T09:35:15.474630315Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.474658234Z             # are found or any other error occurs
2024-12-30T09:35:15.474661937Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.474664032Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.474683401Z >           torch._C._cuda_init()
2024-12-30T09:35:15.474686604Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.474688812Z 
2024-12-30T09:35:15.474716005Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.474868999Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-1-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.474872240Z 
2024-12-30T09:35:15.474914433Z b = 1, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.474918757Z 
2024-12-30T09:35:15.475102604Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.475105879Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.475108045Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.475110059Z         torch.manual_seed(2024)
2024-12-30T09:35:15.475111951Z         device = torch.device("cuda")
2024-12-30T09:35:15.475121911Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.475140375Z 
2024-12-30T09:35:15.475143438Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.475183969Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.475187457Z 
2024-12-30T09:35:15.475353759Z     def _lazy_init():
2024-12-30T09:35:15.475364628Z         global _initialized, _queued_calls
2024-12-30T09:35:15.475366790Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.475380618Z             return
2024-12-30T09:35:15.475383911Z         with _initialization_lock:
2024-12-30T09:35:15.475385961Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.475400878Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.475404128Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.475426739Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.475429850Z             # find there is nothing left to do.
2024-12-30T09:35:15.475431938Z             if is_initialized():
2024-12-30T09:35:15.475433960Z                 return
2024-12-30T09:35:15.475464546Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.475471413Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.475473598Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.475475612Z             if _is_in_bad_fork():
2024-12-30T09:35:15.475477572Z                 raise RuntimeError(
2024-12-30T09:35:15.475493009Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.475496142Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.475518361Z                 )
2024-12-30T09:35:15.475521163Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.475523188Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.475525278Z             if _cudart is None:
2024-12-30T09:35:15.475589896Z                 raise AssertionError(
2024-12-30T09:35:15.475592880Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.475594965Z                 )
2024-12-30T09:35:15.475596935Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.475599008Z             # are found or any other error occurs
2024-12-30T09:35:15.475618179Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.475622297Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.475624375Z >           torch._C._cuda_init()
2024-12-30T09:35:15.475657565Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.475661006Z 
2024-12-30T09:35:15.475686468Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.475846601Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-2-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.475853586Z 
2024-12-30T09:35:15.475892151Z b = 2, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.475896230Z 
2024-12-30T09:35:15.476100859Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.476105331Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.476107421Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.476109375Z         torch.manual_seed(2024)
2024-12-30T09:35:15.476111350Z         device = torch.device("cuda")
2024-12-30T09:35:15.476113346Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.476129959Z 
2024-12-30T09:35:15.476133391Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.476164817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.476172700Z 
2024-12-30T09:35:15.476419051Z     def _lazy_init():
2024-12-30T09:35:15.476422581Z         global _initialized, _queued_calls
2024-12-30T09:35:15.476424582Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.476426953Z             return
2024-12-30T09:35:15.476428877Z         with _initialization_lock:
2024-12-30T09:35:15.476430876Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.476432865Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.476434899Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.476436872Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.476438843Z             # find there is nothing left to do.
2024-12-30T09:35:15.476440775Z             if is_initialized():
2024-12-30T09:35:15.476442793Z                 return
2024-12-30T09:35:15.476468886Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.476473208Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.476475262Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.476477266Z             if _is_in_bad_fork():
2024-12-30T09:35:15.476479217Z                 raise RuntimeError(
2024-12-30T09:35:15.476481261Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.476483356Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.476495398Z                 )
2024-12-30T09:35:15.476502638Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.476505501Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.476508044Z             if _cudart is None:
2024-12-30T09:35:15.476531683Z                 raise AssertionError(
2024-12-30T09:35:15.476535081Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.476537351Z                 )
2024-12-30T09:35:15.476558766Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.476562807Z             # are found or any other error occurs
2024-12-30T09:35:15.476593003Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.476598341Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.476601459Z >           torch._C._cuda_init()
2024-12-30T09:35:15.476607776Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.476610216Z 
2024-12-30T09:35:15.476654863Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.476795452Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-3-8-2048-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.476798744Z 
2024-12-30T09:35:15.476829310Z b = 3, h = 8, n = 2048, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.476832018Z 
2024-12-30T09:35:15.477027598Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.477032708Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.477035130Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.477037201Z         torch.manual_seed(2024)
2024-12-30T09:35:15.477039192Z         device = torch.device("cuda")
2024-12-30T09:35:15.477041236Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.477043864Z 
2024-12-30T09:35:15.477057575Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.477085386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.477088745Z 
2024-12-30T09:35:15.477261146Z     def _lazy_init():
2024-12-30T09:35:15.477264082Z         global _initialized, _queued_calls
2024-12-30T09:35:15.477266081Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.477276253Z             return
2024-12-30T09:35:15.477278197Z         with _initialization_lock:
2024-12-30T09:35:15.477352361Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.477354956Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.477369189Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.477371664Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.477373717Z             # find there is nothing left to do.
2024-12-30T09:35:15.477375670Z             if is_initialized():
2024-12-30T09:35:15.477377614Z                 return
2024-12-30T09:35:15.477379567Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.477381624Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.477383612Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.477385612Z             if _is_in_bad_fork():
2024-12-30T09:35:15.477388424Z                 raise RuntimeError(
2024-12-30T09:35:15.477390464Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.477392624Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.477411132Z                 )
2024-12-30T09:35:15.477415049Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.477417316Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.477438059Z             if _cudart is None:
2024-12-30T09:35:15.477441714Z                 raise AssertionError(
2024-12-30T09:35:15.477443656Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.477462585Z                 )
2024-12-30T09:35:15.477464878Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.477478882Z             # are found or any other error occurs
2024-12-30T09:35:15.477481975Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.477484148Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.477571248Z >           torch._C._cuda_init()
2024-12-30T09:35:15.477574412Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.477577016Z 
2024-12-30T09:35:15.477579446Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.477791676Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-913-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.477817124Z 
2024-12-30T09:35:15.477821020Z b = 6, h = 8, n = 913, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.477824714Z 
2024-12-30T09:35:15.477984840Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.477990562Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.477993748Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.477996011Z         torch.manual_seed(2024)
2024-12-30T09:35:15.477998078Z         device = torch.device("cuda")
2024-12-30T09:35:15.478019629Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.478025450Z 
2024-12-30T09:35:15.478027383Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.478044337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.478049423Z 
2024-12-30T09:35:15.478232648Z     def _lazy_init():
2024-12-30T09:35:15.478236140Z         global _initialized, _queued_calls
2024-12-30T09:35:15.478238427Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.478240991Z             return
2024-12-30T09:35:15.478274314Z         with _initialization_lock:
2024-12-30T09:35:15.478283973Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.478286845Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.478288903Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.478292497Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.478294651Z             # find there is nothing left to do.
2024-12-30T09:35:15.478296740Z             if is_initialized():
2024-12-30T09:35:15.478312041Z                 return
2024-12-30T09:35:15.478316520Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.478318708Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.478325497Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.478327692Z             if _is_in_bad_fork():
2024-12-30T09:35:15.478349226Z                 raise RuntimeError(
2024-12-30T09:35:15.478376870Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.478379780Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.478381999Z                 )
2024-12-30T09:35:15.478386530Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.478388898Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.478392189Z             if _cudart is None:
2024-12-30T09:35:15.478394454Z                 raise AssertionError(
2024-12-30T09:35:15.478431976Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.478438540Z                 )
2024-12-30T09:35:15.478440577Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.478442742Z             # are found or any other error occurs
2024-12-30T09:35:15.478454450Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.478456989Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.478460179Z >           torch._C._cuda_init()
2024-12-30T09:35:15.478478195Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.478483023Z 
2024-12-30T09:35:15.478538808Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.478692997Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-513-128-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.478699597Z 
2024-12-30T09:35:15.478776306Z b = 6, h = 8, n = 513, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.478780542Z 
2024-12-30T09:35:15.478895710Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.478899161Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.478901340Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.478903413Z         torch.manual_seed(2024)
2024-12-30T09:35:15.478906143Z         device = torch.device("cuda")
2024-12-30T09:35:15.478908426Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.478928338Z 
2024-12-30T09:35:15.478942836Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.478983564Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.478994010Z 
2024-12-30T09:35:15.479159583Z     def _lazy_init():
2024-12-30T09:35:15.479165498Z         global _initialized, _queued_calls
2024-12-30T09:35:15.479167666Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.479170118Z             return
2024-12-30T09:35:15.479172067Z         with _initialization_lock:
2024-12-30T09:35:15.479174067Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.479200399Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.479208477Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.479211721Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.479214942Z             # find there is nothing left to do.
2024-12-30T09:35:15.479218267Z             if is_initialized():
2024-12-30T09:35:15.479243401Z                 return
2024-12-30T09:35:15.479250426Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.479252775Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.479254983Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.479257316Z             if _is_in_bad_fork():
2024-12-30T09:35:15.479259456Z                 raise RuntimeError(
2024-12-30T09:35:15.479263095Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.479265268Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.479267366Z                 )
2024-12-30T09:35:15.479279620Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.479284834Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.479287193Z             if _cudart is None:
2024-12-30T09:35:15.479314127Z                 raise AssertionError(
2024-12-30T09:35:15.479316721Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.479319083Z                 )
2024-12-30T09:35:15.479321245Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.479336081Z             # are found or any other error occurs
2024-12-30T09:35:15.479341677Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.479370832Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.479378704Z >           torch._C._cuda_init()
2024-12-30T09:35:15.479386686Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.479391235Z 
2024-12-30T09:35:15.479424228Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.479670531Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-1213-128-64] __________________________________________________________________________[0m
2024-12-30T09:35:15.479677197Z 
2024-12-30T09:35:15.479679296Z b = 6, h = 8, n = 1213, d = 128, e = 64, dtype = torch.float16
2024-12-30T09:35:15.479681728Z 
2024-12-30T09:35:15.479823773Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.479829285Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.479831653Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.479833688Z         torch.manual_seed(2024)
2024-12-30T09:35:15.479837474Z         device = torch.device("cuda")
2024-12-30T09:35:15.479839499Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.479842655Z 
2024-12-30T09:35:15.479857101Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.479879840Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.479885428Z 
2024-12-30T09:35:15.480081531Z     def _lazy_init():
2024-12-30T09:35:15.480086076Z         global _initialized, _queued_calls
2024-12-30T09:35:15.480088616Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.480091081Z             return
2024-12-30T09:35:15.480093094Z         with _initialization_lock:
2024-12-30T09:35:15.480095018Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.480115319Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.480121331Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.480124374Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.480126671Z             # find there is nothing left to do.
2024-12-30T09:35:15.480128737Z             if is_initialized():
2024-12-30T09:35:15.480130796Z                 return
2024-12-30T09:35:15.480132808Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.480151924Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.480161334Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.480186983Z             if _is_in_bad_fork():
2024-12-30T09:35:15.480192739Z                 raise RuntimeError(
2024-12-30T09:35:15.480194662Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.480196768Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.480198831Z                 )
2024-12-30T09:35:15.480211591Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.480217692Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.480221677Z             if _cudart is None:
2024-12-30T09:35:15.480224307Z                 raise AssertionError(
2024-12-30T09:35:15.480227660Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.480252847Z                 )
2024-12-30T09:35:15.480257722Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.480260320Z             # are found or any other error occurs
2024-12-30T09:35:15.480263411Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.480331956Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.480337260Z >           torch._C._cuda_init()
2024-12-30T09:35:15.480339512Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.480342483Z 
2024-12-30T09:35:15.480344410Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.480494513Z [31m[1m__________________________________________________________________________ test_lightning2[dtype1-6-8-2048-16-64] ___________________________________________________________________________[0m
2024-12-30T09:35:15.480500367Z 
2024-12-30T09:35:15.480532941Z b = 6, h = 8, n = 2048, d = 16, e = 64, dtype = torch.float16
2024-12-30T09:35:15.480539142Z 
2024-12-30T09:35:15.480739742Z     @pytest.mark.parametrize("b, h, n, d, e", get_params())
2024-12-30T09:35:15.480760800Z     @pytest.mark.parametrize("dtype", [torch.bfloat16, torch.float16])
2024-12-30T09:35:15.480764664Z     def test_lightning2(b, h, n, d, e, dtype):
2024-12-30T09:35:15.480768099Z         torch.manual_seed(2024)
2024-12-30T09:35:15.480771561Z         device = torch.device("cuda")
2024-12-30T09:35:15.480820222Z >       q = (torch.randn((b, h, n, d), dtype=dtype, device=device) / 10).requires_grad_()
2024-12-30T09:35:15.480830678Z 
2024-12-30T09:35:15.480832616Z [1m[31mtests/ops/test_lightning2_no_decay.py[0m:37: 
2024-12-30T09:35:15.480834796Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.480837156Z 
2024-12-30T09:35:15.480965554Z     def _lazy_init():
2024-12-30T09:35:15.480971108Z         global _initialized, _queued_calls
2024-12-30T09:35:15.480973371Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.480975875Z             return
2024-12-30T09:35:15.480977873Z         with _initialization_lock:
2024-12-30T09:35:15.481007686Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.481013451Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.481015647Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.481017828Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.481019779Z             # find there is nothing left to do.
2024-12-30T09:35:15.481043177Z             if is_initialized():
2024-12-30T09:35:15.481046266Z                 return
2024-12-30T09:35:15.481048935Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.481052522Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.481070998Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.481077060Z             if _is_in_bad_fork():
2024-12-30T09:35:15.481079095Z                 raise RuntimeError(
2024-12-30T09:35:15.481106930Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.481112125Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.481114710Z                 )
2024-12-30T09:35:15.481116664Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.481122780Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.481126419Z             if _cudart is None:
2024-12-30T09:35:15.481154927Z                 raise AssertionError(
2024-12-30T09:35:15.481159025Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.481164508Z                 )
2024-12-30T09:35:15.481167499Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.481170684Z             # are found or any other error occurs
2024-12-30T09:35:15.481180523Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.481183008Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.481185020Z >           torch._C._cuda_init()
2024-12-30T09:35:15.481213251Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.481220155Z 
2024-12-30T09:35:15.481222245Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.481389089Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-1-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.481395028Z 
2024-12-30T09:35:15.481419647Z b = 1, n = 127, d = 768, dtype = torch.float32
2024-12-30T09:35:15.481425238Z 
2024-12-30T09:35:15.481616243Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.481622361Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.481624532Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.481626541Z         torch.manual_seed(2024)
2024-12-30T09:35:15.481628463Z         atol = 5e-2
2024-12-30T09:35:15.481633078Z         rtol = 1e-2
2024-12-30T09:35:15.481635341Z         device = torch.device("cuda")
2024-12-30T09:35:15.481637705Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.481658160Z 
2024-12-30T09:35:15.481663633Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.481717369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.481726803Z 
2024-12-30T09:35:15.481869481Z     def _lazy_init():
2024-12-30T09:35:15.481874924Z         global _initialized, _queued_calls
2024-12-30T09:35:15.481885258Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.481887945Z             return
2024-12-30T09:35:15.481891641Z         with _initialization_lock:
2024-12-30T09:35:15.481893858Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.481903596Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.481905911Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.481909447Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.481922707Z             # find there is nothing left to do.
2024-12-30T09:35:15.481970986Z             if is_initialized():
2024-12-30T09:35:15.481977465Z                 return
2024-12-30T09:35:15.481979625Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.481981971Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.481984135Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.481986191Z             if _is_in_bad_fork():
2024-12-30T09:35:15.481988104Z                 raise RuntimeError(
2024-12-30T09:35:15.481992590Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.481995145Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.482027242Z                 )
2024-12-30T09:35:15.482032291Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.482034980Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.482037248Z             if _cudart is None:
2024-12-30T09:35:15.482039277Z                 raise AssertionError(
2024-12-30T09:35:15.482055476Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.482061182Z                 )
2024-12-30T09:35:15.482075952Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.482081238Z             # are found or any other error occurs
2024-12-30T09:35:15.482083300Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.482085641Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.482087746Z >           torch._C._cuda_init()
2024-12-30T09:35:15.482129936Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.482136090Z 
2024-12-30T09:35:15.482138350Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.482282838Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.482288237Z 
2024-12-30T09:35:15.482307931Z b = 1, n = 127, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.482311060Z 
2024-12-30T09:35:15.482500847Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.482506754Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.482513104Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.482515152Z         torch.manual_seed(2024)
2024-12-30T09:35:15.482517115Z         atol = 5e-2
2024-12-30T09:35:15.482520032Z         rtol = 1e-2
2024-12-30T09:35:15.482522206Z         device = torch.device("cuda")
2024-12-30T09:35:15.482524132Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.482526273Z 
2024-12-30T09:35:15.482565264Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.482572363Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.482595893Z 
2024-12-30T09:35:15.482764311Z     def _lazy_init():
2024-12-30T09:35:15.482769642Z         global _initialized, _queued_calls
2024-12-30T09:35:15.482772134Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.482774360Z             return
2024-12-30T09:35:15.482798861Z         with _initialization_lock:
2024-12-30T09:35:15.482813761Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.482816906Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.482819054Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.482822887Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.482825334Z             # find there is nothing left to do.
2024-12-30T09:35:15.482851339Z             if is_initialized():
2024-12-30T09:35:15.482859499Z                 return
2024-12-30T09:35:15.482861494Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.482863624Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.482866284Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.482869131Z             if _is_in_bad_fork():
2024-12-30T09:35:15.482871393Z                 raise RuntimeError(
2024-12-30T09:35:15.482895059Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.482900864Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.482903000Z                 )
2024-12-30T09:35:15.482905190Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.482907315Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.482916967Z             if _cudart is None:
2024-12-30T09:35:15.482925699Z                 raise AssertionError(
2024-12-30T09:35:15.482939704Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.482941975Z                 )
2024-12-30T09:35:15.482943917Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.482959405Z             # are found or any other error occurs
2024-12-30T09:35:15.482963189Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.482965290Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.482997516Z >           torch._C._cuda_init()
2024-12-30T09:35:15.483007874Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.483010618Z 
2024-12-30T09:35:15.483031356Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.483164567Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-1-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.483169869Z 
2024-12-30T09:35:15.483194811Z b = 1, n = 128, d = 768, dtype = torch.float32
2024-12-30T09:35:15.483197948Z 
2024-12-30T09:35:15.483380161Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.483385597Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.483388152Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.483390261Z         torch.manual_seed(2024)
2024-12-30T09:35:15.483403472Z         atol = 5e-2
2024-12-30T09:35:15.483409324Z         rtol = 1e-2
2024-12-30T09:35:15.483411421Z         device = torch.device("cuda")
2024-12-30T09:35:15.483413589Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.483432732Z 
2024-12-30T09:35:15.483438184Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.483495073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.483501090Z 
2024-12-30T09:35:15.483666998Z     def _lazy_init():
2024-12-30T09:35:15.483679508Z         global _initialized, _queued_calls
2024-12-30T09:35:15.483682673Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.483687086Z             return
2024-12-30T09:35:15.483696824Z         with _initialization_lock:
2024-12-30T09:35:15.483700256Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.483703978Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.483709324Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.483712569Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.483716603Z             # find there is nothing left to do.
2024-12-30T09:35:15.483719480Z             if is_initialized():
2024-12-30T09:35:15.483722567Z                 return
2024-12-30T09:35:15.483740570Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.483746257Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.483748478Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.483750557Z             if _is_in_bad_fork():
2024-12-30T09:35:15.483775633Z                 raise RuntimeError(
2024-12-30T09:35:15.483781485Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.483783939Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.483786002Z                 )
2024-12-30T09:35:15.483787887Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.483801453Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.483806810Z             if _cudart is None:
2024-12-30T09:35:15.483809451Z                 raise AssertionError(
2024-12-30T09:35:15.483827517Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.483833564Z                 )
2024-12-30T09:35:15.483835541Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.483851946Z             # are found or any other error occurs
2024-12-30T09:35:15.483857253Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.483859906Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.483864259Z >           torch._C._cuda_init()
2024-12-30T09:35:15.483888828Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.483895236Z 
2024-12-30T09:35:15.483897737Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.484055658Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.484067830Z 
2024-12-30T09:35:15.484086137Z b = 1, n = 128, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.484101462Z 
2024-12-30T09:35:15.484251661Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.484256886Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.484259535Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.484262095Z         torch.manual_seed(2024)
2024-12-30T09:35:15.484286010Z         atol = 5e-2
2024-12-30T09:35:15.484292577Z         rtol = 1e-2
2024-12-30T09:35:15.484294746Z         device = torch.device("cuda")
2024-12-30T09:35:15.484296931Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.484299515Z 
2024-12-30T09:35:15.484326187Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.484331072Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.484333816Z 
2024-12-30T09:35:15.484538174Z     def _lazy_init():
2024-12-30T09:35:15.484543713Z         global _initialized, _queued_calls
2024-12-30T09:35:15.484547246Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.484559791Z             return
2024-12-30T09:35:15.484562935Z         with _initialization_lock:
2024-12-30T09:35:15.484566311Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.484571531Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.484573570Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.484576636Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.484579075Z             # find there is nothing left to do.
2024-12-30T09:35:15.484626193Z             if is_initialized():
2024-12-30T09:35:15.484635404Z                 return
2024-12-30T09:35:15.484637770Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.484640190Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.484642349Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.484644451Z             if _is_in_bad_fork():
2024-12-30T09:35:15.484673018Z                 raise RuntimeError(
2024-12-30T09:35:15.484682647Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.484685214Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.484687344Z                 )
2024-12-30T09:35:15.484689241Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.484691628Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.484695577Z             if _cudart is None:
2024-12-30T09:35:15.484697792Z                 raise AssertionError(
2024-12-30T09:35:15.484722709Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.484729006Z                 )
2024-12-30T09:35:15.484731044Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.484758694Z             # are found or any other error occurs
2024-12-30T09:35:15.484767256Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.484770800Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.484778694Z >           torch._C._cuda_init()
2024-12-30T09:35:15.484782196Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.484809859Z 
2024-12-30T09:35:15.484815306Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.484984440Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-1-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.484990192Z 
2024-12-30T09:35:15.485007154Z b = 1, n = 256, d = 768, dtype = torch.float32
2024-12-30T09:35:15.485012545Z 
2024-12-30T09:35:15.485192667Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.485198183Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.485200739Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.485202777Z         torch.manual_seed(2024)
2024-12-30T09:35:15.485204680Z         atol = 5e-2
2024-12-30T09:35:15.485206598Z         rtol = 1e-2
2024-12-30T09:35:15.485220120Z         device = torch.device("cuda")
2024-12-30T09:35:15.485225784Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.485228332Z 
2024-12-30T09:35:15.485250715Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.485256252Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.485263471Z 
2024-12-30T09:35:15.485465632Z     def _lazy_init():
2024-12-30T09:35:15.485471286Z         global _initialized, _queued_calls
2024-12-30T09:35:15.485473840Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.485476282Z             return
2024-12-30T09:35:15.485478240Z         with _initialization_lock:
2024-12-30T09:35:15.485480254Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.485508674Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.485514687Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.485517053Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.485519210Z             # find there is nothing left to do.
2024-12-30T09:35:15.485521164Z             if is_initialized():
2024-12-30T09:35:15.485538812Z                 return
2024-12-30T09:35:15.485546106Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.485558654Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.485562441Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.485567612Z             if _is_in_bad_fork():
2024-12-30T09:35:15.485570521Z                 raise RuntimeError(
2024-12-30T09:35:15.485575353Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.485578870Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.485582241Z                 )
2024-12-30T09:35:15.485632003Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.485637461Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.485639541Z             if _cudart is None:
2024-12-30T09:35:15.485641520Z                 raise AssertionError(
2024-12-30T09:35:15.485643416Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.485645501Z                 )
2024-12-30T09:35:15.485648855Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.485652080Z             # are found or any other error occurs
2024-12-30T09:35:15.485654561Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.485692285Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.485705408Z >           torch._C._cuda_init()
2024-12-30T09:35:15.485707717Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.485710226Z 
2024-12-30T09:35:15.485726154Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.485884787Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.485889801Z 
2024-12-30T09:35:15.485913228Z b = 1, n = 256, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.485916736Z 
2024-12-30T09:35:15.486107490Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.486110907Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.486113216Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.486115406Z         torch.manual_seed(2024)
2024-12-30T09:35:15.486117350Z         atol = 5e-2
2024-12-30T09:35:15.486119273Z         rtol = 1e-2
2024-12-30T09:35:15.486126248Z         device = torch.device("cuda")
2024-12-30T09:35:15.486129616Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.486131830Z 
2024-12-30T09:35:15.486158200Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.486180571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.486183273Z 
2024-12-30T09:35:15.486394193Z     def _lazy_init():
2024-12-30T09:35:15.486401158Z         global _initialized, _queued_calls
2024-12-30T09:35:15.486403173Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.486405360Z             return
2024-12-30T09:35:15.486407258Z         with _initialization_lock:
2024-12-30T09:35:15.486409157Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.486411231Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.486413217Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.486424475Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.486427978Z             # find there is nothing left to do.
2024-12-30T09:35:15.486448659Z             if is_initialized():
2024-12-30T09:35:15.486453000Z                 return
2024-12-30T09:35:15.486463539Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.486480463Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.486487276Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.486489340Z             if _is_in_bad_fork():
2024-12-30T09:35:15.486491507Z                 raise RuntimeError(
2024-12-30T09:35:15.486504819Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.486510774Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.486512837Z                 )
2024-12-30T09:35:15.486514830Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.486546111Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.486559115Z             if _cudart is None:
2024-12-30T09:35:15.486561152Z                 raise AssertionError(
2024-12-30T09:35:15.486563088Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.486565140Z                 )
2024-12-30T09:35:15.486567133Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.486569832Z             # are found or any other error occurs
2024-12-30T09:35:15.486573473Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.486595506Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.486599427Z >           torch._C._cuda_init()
2024-12-30T09:35:15.486625527Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.486629245Z 
2024-12-30T09:35:15.486631216Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.486857546Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-1-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.486864509Z 
2024-12-30T09:35:15.486866618Z b = 1, n = 257, d = 768, dtype = torch.float32
2024-12-30T09:35:15.486868823Z 
2024-12-30T09:35:15.487061233Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.487066384Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.487068654Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.487070617Z         torch.manual_seed(2024)
2024-12-30T09:35:15.487076973Z         atol = 5e-2
2024-12-30T09:35:15.487079017Z         rtol = 1e-2
2024-12-30T09:35:15.487080891Z         device = torch.device("cuda")
2024-12-30T09:35:15.487082903Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.487085597Z 
2024-12-30T09:35:15.487087608Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.487141084Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.487148218Z 
2024-12-30T09:35:15.487314139Z     def _lazy_init():
2024-12-30T09:35:15.487317346Z         global _initialized, _queued_calls
2024-12-30T09:35:15.487319351Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.487321706Z             return
2024-12-30T09:35:15.487344431Z         with _initialization_lock:
2024-12-30T09:35:15.487348111Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.487350229Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.487355508Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.487376511Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.487404260Z             # find there is nothing left to do.
2024-12-30T09:35:15.487409231Z             if is_initialized():
2024-12-30T09:35:15.487412252Z                 return
2024-12-30T09:35:15.487415052Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.487422372Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.487425697Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.487428813Z             if _is_in_bad_fork():
2024-12-30T09:35:15.487435758Z                 raise RuntimeError(
2024-12-30T09:35:15.487439196Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.487450672Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.487472752Z                 )
2024-12-30T09:35:15.487476012Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.487479212Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.487482253Z             if _cudart is None:
2024-12-30T09:35:15.487518685Z                 raise AssertionError(
2024-12-30T09:35:15.487521996Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.487529677Z                 )
2024-12-30T09:35:15.487531636Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.487555992Z             # are found or any other error occurs
2024-12-30T09:35:15.487560041Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.487562636Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.487564889Z >           torch._C._cuda_init()
2024-12-30T09:35:15.487582568Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.487586691Z 
2024-12-30T09:35:15.487592224Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.487778515Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.487782090Z 
2024-12-30T09:35:15.487811394Z b = 1, n = 257, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.487814874Z 
2024-12-30T09:35:15.487998442Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.488007494Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.488010206Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.488012233Z         torch.manual_seed(2024)
2024-12-30T09:35:15.488014226Z         atol = 5e-2
2024-12-30T09:35:15.488016658Z         rtol = 1e-2
2024-12-30T09:35:15.488020531Z         device = torch.device("cuda")
2024-12-30T09:35:15.488023084Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.488069757Z 
2024-12-30T09:35:15.488075825Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.488089839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.488095068Z 
2024-12-30T09:35:15.488281129Z     def _lazy_init():
2024-12-30T09:35:15.488286630Z         global _initialized, _queued_calls
2024-12-30T09:35:15.488288586Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.488291017Z             return
2024-12-30T09:35:15.488292901Z         with _initialization_lock:
2024-12-30T09:35:15.488320982Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.488329460Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.488331747Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.488333815Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.488337154Z             # find there is nothing left to do.
2024-12-30T09:35:15.488339270Z             if is_initialized():
2024-12-30T09:35:15.488341296Z                 return
2024-12-30T09:35:15.488344891Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.488388659Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.488392300Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.488394352Z             if _is_in_bad_fork():
2024-12-30T09:35:15.488396278Z                 raise RuntimeError(
2024-12-30T09:35:15.488415121Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.488420137Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.488422332Z                 )
2024-12-30T09:35:15.488425130Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.488427401Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.488430047Z             if _cudart is None:
2024-12-30T09:35:15.488460240Z                 raise AssertionError(
2024-12-30T09:35:15.488463463Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.488465646Z                 )
2024-12-30T09:35:15.488467534Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.488501957Z             # are found or any other error occurs
2024-12-30T09:35:15.488504558Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.488506554Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.488508905Z >           torch._C._cuda_init()
2024-12-30T09:35:15.488538873Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.488543169Z 
2024-12-30T09:35:15.488545105Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.488713116Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.488717149Z 
2024-12-30T09:35:15.488743027Z b = 1, n = 1024, d = 768, dtype = torch.float32
2024-12-30T09:35:15.488747446Z 
2024-12-30T09:35:15.488918953Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.488922757Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.488924835Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.488926764Z         torch.manual_seed(2024)
2024-12-30T09:35:15.488928736Z         atol = 5e-2
2024-12-30T09:35:15.488934412Z         rtol = 1e-2
2024-12-30T09:35:15.488954352Z         device = torch.device("cuda")
2024-12-30T09:35:15.488957328Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.488959390Z 
2024-12-30T09:35:15.489030744Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.489040047Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.489043081Z 
2024-12-30T09:35:15.489184254Z     def _lazy_init():
2024-12-30T09:35:15.489189742Z         global _initialized, _queued_calls
2024-12-30T09:35:15.489191728Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.489216788Z             return
2024-12-30T09:35:15.489220041Z         with _initialization_lock:
2024-12-30T09:35:15.489222038Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.489225717Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.489228368Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.489230615Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.489261368Z             # find there is nothing left to do.
2024-12-30T09:35:15.489264593Z             if is_initialized():
2024-12-30T09:35:15.489266504Z                 return
2024-12-30T09:35:15.489268397Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.489270533Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.489290478Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.489293762Z             if _is_in_bad_fork():
2024-12-30T09:35:15.489295783Z                 raise RuntimeError(
2024-12-30T09:35:15.489325338Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.489328430Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.489335336Z                 )
2024-12-30T09:35:15.489337327Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.489339371Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.489346396Z             if _cudart is None:
2024-12-30T09:35:15.489348449Z                 raise AssertionError(
2024-12-30T09:35:15.489381813Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.489385706Z                 )
2024-12-30T09:35:15.489387630Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.489405382Z             # are found or any other error occurs
2024-12-30T09:35:15.489408764Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.489410825Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.489427350Z >           torch._C._cuda_init()
2024-12-30T09:35:15.489435907Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.489445868Z 
2024-12-30T09:35:15.489447894Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.489631581Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.489635903Z 
2024-12-30T09:35:15.489657224Z b = 1, n = 1024, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.489660479Z 
2024-12-30T09:35:15.489836140Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.489839775Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.489842052Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.489844025Z         torch.manual_seed(2024)
2024-12-30T09:35:15.489846027Z         atol = 5e-2
2024-12-30T09:35:15.489870564Z         rtol = 1e-2
2024-12-30T09:35:15.489875185Z         device = torch.device("cuda")
2024-12-30T09:35:15.489878584Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.489881966Z 
2024-12-30T09:35:15.489907642Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.489910981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.489913439Z 
2024-12-30T09:35:15.490100039Z     def _lazy_init():
2024-12-30T09:35:15.490103547Z         global _initialized, _queued_calls
2024-12-30T09:35:15.490105456Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.490107563Z             return
2024-12-30T09:35:15.490109510Z         with _initialization_lock:
2024-12-30T09:35:15.490123281Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.490126998Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.490129096Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.490141302Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.490144385Z             # find there is nothing left to do.
2024-12-30T09:35:15.490161389Z             if is_initialized():
2024-12-30T09:35:15.490164364Z                 return
2024-12-30T09:35:15.490166429Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.490190257Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.490192951Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.490194962Z             if _is_in_bad_fork():
2024-12-30T09:35:15.490200264Z                 raise RuntimeError(
2024-12-30T09:35:15.490202937Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.490205211Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.490236480Z                 )
2024-12-30T09:35:15.490240767Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.490242994Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.490245194Z             if _cudart is None:
2024-12-30T09:35:15.490257351Z                 raise AssertionError(
2024-12-30T09:35:15.490260906Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.490263200Z                 )
2024-12-30T09:35:15.490297481Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.490300402Z             # are found or any other error occurs
2024-12-30T09:35:15.490302447Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.490327303Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.490329949Z >           torch._C._cuda_init()
2024-12-30T09:35:15.490332111Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.490338042Z 
2024-12-30T09:35:15.490365671Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.490555050Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.490559507Z 
2024-12-30T09:35:15.490561429Z b = 1, n = 1025, d = 768, dtype = torch.float32
2024-12-30T09:35:15.490563547Z 
2024-12-30T09:35:15.490727629Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.490739174Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.490741849Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.490743916Z         torch.manual_seed(2024)
2024-12-30T09:35:15.490745914Z         atol = 5e-2
2024-12-30T09:35:15.490747896Z         rtol = 1e-2
2024-12-30T09:35:15.490749889Z         device = torch.device("cuda")
2024-12-30T09:35:15.490752988Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.490755297Z 
2024-12-30T09:35:15.490789448Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.490792800Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.490795246Z 
2024-12-30T09:35:15.491013431Z     def _lazy_init():
2024-12-30T09:35:15.491017789Z         global _initialized, _queued_calls
2024-12-30T09:35:15.491019846Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.491025992Z             return
2024-12-30T09:35:15.491029816Z         with _initialization_lock:
2024-12-30T09:35:15.491031802Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.491033774Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.491035841Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.491038714Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.491040751Z             # find there is nothing left to do.
2024-12-30T09:35:15.491042726Z             if is_initialized():
2024-12-30T09:35:15.491075503Z                 return
2024-12-30T09:35:15.491078305Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.491080427Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.491090139Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.491092904Z             if _is_in_bad_fork():
2024-12-30T09:35:15.491094947Z                 raise RuntimeError(
2024-12-30T09:35:15.491111061Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.491114761Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.491123720Z                 )
2024-12-30T09:35:15.491127142Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.491147455Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.491150540Z             if _cudart is None:
2024-12-30T09:35:15.491152526Z                 raise AssertionError(
2024-12-30T09:35:15.491157314Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.491159490Z                 )
2024-12-30T09:35:15.491166341Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.491168449Z             # are found or any other error occurs
2024-12-30T09:35:15.491197377Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.491200287Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.491202564Z >           torch._C._cuda_init()
2024-12-30T09:35:15.491222574Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.491226948Z 
2024-12-30T09:35:15.491243723Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.491395527Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-1-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.491399530Z 
2024-12-30T09:35:15.491426456Z b = 1, n = 1025, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.491429885Z 
2024-12-30T09:35:15.491607180Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.491610934Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.491613025Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.491615012Z         torch.manual_seed(2024)
2024-12-30T09:35:15.491636923Z         atol = 5e-2
2024-12-30T09:35:15.491640346Z         rtol = 1e-2
2024-12-30T09:35:15.491642284Z         device = torch.device("cuda")
2024-12-30T09:35:15.491648859Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.491660527Z 
2024-12-30T09:35:15.491696084Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.491699475Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.491701925Z 
2024-12-30T09:35:15.491905071Z     def _lazy_init():
2024-12-30T09:35:15.491908332Z         global _initialized, _queued_calls
2024-12-30T09:35:15.491910312Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.491912672Z             return
2024-12-30T09:35:15.491914535Z         with _initialization_lock:
2024-12-30T09:35:15.491916526Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.491943117Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.491954511Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.491956845Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.491958904Z             # find there is nothing left to do.
2024-12-30T09:35:15.491965576Z             if is_initialized():
2024-12-30T09:35:15.491967737Z                 return
2024-12-30T09:35:15.491969761Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.491972116Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.491994467Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.491997317Z             if _is_in_bad_fork():
2024-12-30T09:35:15.491999327Z                 raise RuntimeError(
2024-12-30T09:35:15.492001387Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.492022734Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.492026154Z                 )
2024-12-30T09:35:15.492028046Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.492070254Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.492081248Z             if _cudart is None:
2024-12-30T09:35:15.492083558Z                 raise AssertionError(
2024-12-30T09:35:15.492085630Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.492088040Z                 )
2024-12-30T09:35:15.492091311Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.492100309Z             # are found or any other error occurs
2024-12-30T09:35:15.492102559Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.492105394Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.492107550Z >           torch._C._cuda_init()
2024-12-30T09:35:15.492140338Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.492145393Z 
2024-12-30T09:35:15.492170403Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.492315198Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-2-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.492318631Z 
2024-12-30T09:35:15.492320923Z b = 2, n = 127, d = 768, dtype = torch.float32
2024-12-30T09:35:15.492322897Z 
2024-12-30T09:35:15.492533241Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.492536977Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.492539511Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.492541422Z         torch.manual_seed(2024)
2024-12-30T09:35:15.492543346Z         atol = 5e-2
2024-12-30T09:35:15.492545204Z         rtol = 1e-2
2024-12-30T09:35:15.492547459Z         device = torch.device("cuda")
2024-12-30T09:35:15.492560610Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.492584426Z 
2024-12-30T09:35:15.492588263Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.492640393Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.492643833Z 
2024-12-30T09:35:15.492801717Z     def _lazy_init():
2024-12-30T09:35:15.492804922Z         global _initialized, _queued_calls
2024-12-30T09:35:15.492806896Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.492809046Z             return
2024-12-30T09:35:15.492811028Z         with _initialization_lock:
2024-12-30T09:35:15.492813587Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.492815634Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.492818395Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.492887007Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.492895954Z             # find there is nothing left to do.
2024-12-30T09:35:15.492898026Z             if is_initialized():
2024-12-30T09:35:15.492900048Z                 return
2024-12-30T09:35:15.492902077Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.492904180Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.492906283Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.492908343Z             if _is_in_bad_fork():
2024-12-30T09:35:15.492918739Z                 raise RuntimeError(
2024-12-30T09:35:15.492920818Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.492923008Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.492925133Z                 )
2024-12-30T09:35:15.492927117Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.492941565Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.492945160Z             if _cudart is None:
2024-12-30T09:35:15.492958199Z                 raise AssertionError(
2024-12-30T09:35:15.492961246Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.492963517Z                 )
2024-12-30T09:35:15.492992186Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.492995779Z             # are found or any other error occurs
2024-12-30T09:35:15.492997766Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.492999801Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.493014579Z >           torch._C._cuda_init()
2024-12-30T09:35:15.493017086Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.493087682Z 
2024-12-30T09:35:15.493102606Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.493215919Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.493220294Z 
2024-12-30T09:35:15.493247735Z b = 2, n = 127, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.493250418Z 
2024-12-30T09:35:15.493466150Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.493489196Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.493491953Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.493494050Z         torch.manual_seed(2024)
2024-12-30T09:35:15.493496304Z         atol = 5e-2
2024-12-30T09:35:15.493498364Z         rtol = 1e-2
2024-12-30T09:35:15.493500313Z         device = torch.device("cuda")
2024-12-30T09:35:15.493502423Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.493504691Z 
2024-12-30T09:35:15.493515478Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.493527321Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.493530048Z 
2024-12-30T09:35:15.493751565Z     def _lazy_init():
2024-12-30T09:35:15.493758806Z         global _initialized, _queued_calls
2024-12-30T09:35:15.493761341Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.493764032Z             return
2024-12-30T09:35:15.493766235Z         with _initialization_lock:
2024-12-30T09:35:15.493768380Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.493770632Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.493790567Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.493794805Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.493796878Z             # find there is nothing left to do.
2024-12-30T09:35:15.493799002Z             if is_initialized():
2024-12-30T09:35:15.493801061Z                 return
2024-12-30T09:35:15.493820543Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.493828901Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.493831202Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.493833411Z             if _is_in_bad_fork():
2024-12-30T09:35:15.493873503Z                 raise RuntimeError(
2024-12-30T09:35:15.493877156Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.493879559Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.493881803Z                 )
2024-12-30T09:35:15.493883759Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.493904938Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.493931199Z             if _cudart is None:
2024-12-30T09:35:15.493933907Z                 raise AssertionError(
2024-12-30T09:35:15.493935987Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.493938165Z                 )
2024-12-30T09:35:15.493940219Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.493942582Z             # are found or any other error occurs
2024-12-30T09:35:15.493946689Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.493948811Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.493951274Z >           torch._C._cuda_init()
2024-12-30T09:35:15.493955082Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.493983431Z 
2024-12-30T09:35:15.493986908Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.494166228Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-2-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.494173661Z 
2024-12-30T09:35:15.494189315Z b = 2, n = 128, d = 768, dtype = torch.float32
2024-12-30T09:35:15.494191839Z 
2024-12-30T09:35:15.494410558Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.494428126Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.494430855Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.494432928Z         torch.manual_seed(2024)
2024-12-30T09:35:15.494435012Z         atol = 5e-2
2024-12-30T09:35:15.494437120Z         rtol = 1e-2
2024-12-30T09:35:15.494439113Z         device = torch.device("cuda")
2024-12-30T09:35:15.494441234Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.494443569Z 
2024-12-30T09:35:15.494453419Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.494462242Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.494465834Z 
2024-12-30T09:35:15.494682894Z     def _lazy_init():
2024-12-30T09:35:15.494689731Z         global _initialized, _queued_calls
2024-12-30T09:35:15.494691922Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.494700336Z             return
2024-12-30T09:35:15.494702299Z         with _initialization_lock:
2024-12-30T09:35:15.494704217Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.494706463Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.494710931Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.494713307Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.494715905Z             # find there is nothing left to do.
2024-12-30T09:35:15.494723019Z             if is_initialized():
2024-12-30T09:35:15.494728804Z                 return
2024-12-30T09:35:15.494732466Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.494735019Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.494749978Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.494754861Z             if _is_in_bad_fork():
2024-12-30T09:35:15.494756889Z                 raise RuntimeError(
2024-12-30T09:35:15.494769667Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.494774283Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.494806325Z                 )
2024-12-30T09:35:15.494810826Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.494812924Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.494815000Z             if _cudart is None:
2024-12-30T09:35:15.494820029Z                 raise AssertionError(
2024-12-30T09:35:15.494832354Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.494841177Z                 )
2024-12-30T09:35:15.494843547Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.494861440Z             # are found or any other error occurs
2024-12-30T09:35:15.494866482Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.494870004Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.494881021Z >           torch._C._cuda_init()
2024-12-30T09:35:15.494899026Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.494903545Z 
2024-12-30T09:35:15.494919388Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.495073841Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.495078552Z 
2024-12-30T09:35:15.495090749Z b = 2, n = 128, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.495092972Z 
2024-12-30T09:35:15.495284179Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.495288791Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.495291125Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.495293112Z         torch.manual_seed(2024)
2024-12-30T09:35:15.495295077Z         atol = 5e-2
2024-12-30T09:35:15.495297252Z         rtol = 1e-2
2024-12-30T09:35:15.495316560Z         device = torch.device("cuda")
2024-12-30T09:35:15.495322983Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.495325992Z 
2024-12-30T09:35:15.495351377Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.495373865Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.495376952Z 
2024-12-30T09:35:15.495576569Z     def _lazy_init():
2024-12-30T09:35:15.495581635Z         global _initialized, _queued_calls
2024-12-30T09:35:15.495583649Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.495585891Z             return
2024-12-30T09:35:15.495587783Z         with _initialization_lock:
2024-12-30T09:35:15.495589749Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.495621421Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.495625952Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.495628050Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.495630200Z             # find there is nothing left to do.
2024-12-30T09:35:15.495632227Z             if is_initialized():
2024-12-30T09:35:15.495664031Z                 return
2024-12-30T09:35:15.495669523Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.495672010Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.495676361Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.495678502Z             if _is_in_bad_fork():
2024-12-30T09:35:15.495685264Z                 raise RuntimeError(
2024-12-30T09:35:15.495688691Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.495720970Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.495727254Z                 )
2024-12-30T09:35:15.495729336Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.495731576Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.495733792Z             if _cudart is None:
2024-12-30T09:35:15.495747780Z                 raise AssertionError(
2024-12-30T09:35:15.495750099Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.495768719Z                 )
2024-12-30T09:35:15.495773795Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.495790867Z             # are found or any other error occurs
2024-12-30T09:35:15.495792915Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.495800614Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.495806937Z >           torch._C._cuda_init()
2024-12-30T09:35:15.495809292Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.495831794Z 
2024-12-30T09:35:15.495834286Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.496003332Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-2-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.496006789Z 
2024-12-30T09:35:15.496044672Z b = 2, n = 256, d = 768, dtype = torch.float32
2024-12-30T09:35:15.496050828Z 
2024-12-30T09:35:15.496207246Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.496210876Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.496213520Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.496215683Z         torch.manual_seed(2024)
2024-12-30T09:35:15.496217715Z         atol = 5e-2
2024-12-30T09:35:15.496228174Z         rtol = 1e-2
2024-12-30T09:35:15.496232278Z         device = torch.device("cuda")
2024-12-30T09:35:15.496234369Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.496307088Z 
2024-12-30T09:35:15.496315167Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.496325060Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.496332066Z 
2024-12-30T09:35:15.496505259Z     def _lazy_init():
2024-12-30T09:35:15.496509785Z         global _initialized, _queued_calls
2024-12-30T09:35:15.496511863Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.496514127Z             return
2024-12-30T09:35:15.496516034Z         with _initialization_lock:
2024-12-30T09:35:15.496517961Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.496520042Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.496536780Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.496541873Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.496543912Z             # find there is nothing left to do.
2024-12-30T09:35:15.496545888Z             if is_initialized():
2024-12-30T09:35:15.496556170Z                 return
2024-12-30T09:35:15.496578164Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.496582754Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.496584875Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.496599410Z             if _is_in_bad_fork():
2024-12-30T09:35:15.496603152Z                 raise RuntimeError(
2024-12-30T09:35:15.496627794Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.496631489Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.496633613Z                 )
2024-12-30T09:35:15.496635512Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.496676358Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.496679678Z             if _cudart is None:
2024-12-30T09:35:15.496681720Z                 raise AssertionError(
2024-12-30T09:35:15.496683799Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.496685973Z                 )
2024-12-30T09:35:15.496687964Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.496691012Z             # are found or any other error occurs
2024-12-30T09:35:15.496693198Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.496715444Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.496719364Z >           torch._C._cuda_init()
2024-12-30T09:35:15.496740999Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.496744239Z 
2024-12-30T09:35:15.496746299Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.496916560Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.496922512Z 
2024-12-30T09:35:15.496932047Z b = 2, n = 256, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.496934188Z 
2024-12-30T09:35:15.497116070Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.497119594Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.497121763Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.497123800Z         torch.manual_seed(2024)
2024-12-30T09:35:15.497125717Z         atol = 5e-2
2024-12-30T09:35:15.497127635Z         rtol = 1e-2
2024-12-30T09:35:15.497147078Z         device = torch.device("cuda")
2024-12-30T09:35:15.497149846Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.497152084Z 
2024-12-30T09:35:15.497183832Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.497188749Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.497191242Z 
2024-12-30T09:35:15.497424074Z     def _lazy_init():
2024-12-30T09:35:15.497427531Z         global _initialized, _queued_calls
2024-12-30T09:35:15.497429509Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.497431763Z             return
2024-12-30T09:35:15.497433752Z         with _initialization_lock:
2024-12-30T09:35:15.497435709Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.497446510Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.497450197Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.497452251Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.497470393Z             # find there is nothing left to do.
2024-12-30T09:35:15.497482626Z             if is_initialized():
2024-12-30T09:35:15.497484652Z                 return
2024-12-30T09:35:15.497486533Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.497488565Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.497490660Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.497523864Z             if _is_in_bad_fork():
2024-12-30T09:35:15.497527918Z                 raise RuntimeError(
2024-12-30T09:35:15.497529898Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.497532131Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.497534324Z                 )
2024-12-30T09:35:15.497537434Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.497539647Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.497541847Z             if _cudart is None:
2024-12-30T09:35:15.497569967Z                 raise AssertionError(
2024-12-30T09:35:15.497574102Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.497576279Z                 )
2024-12-30T09:35:15.497588168Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.497591058Z             # are found or any other error occurs
2024-12-30T09:35:15.497604848Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.497607251Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.497630445Z >           torch._C._cuda_init()
2024-12-30T09:35:15.497633977Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.497664668Z 
2024-12-30T09:35:15.497667895Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.497830675Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-2-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.497834010Z 
2024-12-30T09:35:15.497854004Z b = 2, n = 257, d = 768, dtype = torch.float32
2024-12-30T09:35:15.497856449Z 
2024-12-30T09:35:15.498039005Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.498041918Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.498044127Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.498058066Z         torch.manual_seed(2024)
2024-12-30T09:35:15.498060224Z         atol = 5e-2
2024-12-30T09:35:15.498068947Z         rtol = 1e-2
2024-12-30T09:35:15.498075397Z         device = torch.device("cuda")
2024-12-30T09:35:15.498077665Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.498079955Z 
2024-12-30T09:35:15.498167104Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.498171788Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.498174157Z 
2024-12-30T09:35:15.498325270Z     def _lazy_init():
2024-12-30T09:35:15.498328963Z         global _initialized, _queued_calls
2024-12-30T09:35:15.498330987Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.498333121Z             return
2024-12-30T09:35:15.498335040Z         with _initialization_lock:
2024-12-30T09:35:15.498336997Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.498339134Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.498341190Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.498373611Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.498379816Z             # find there is nothing left to do.
2024-12-30T09:35:15.498382721Z             if is_initialized():
2024-12-30T09:35:15.498385611Z                 return
2024-12-30T09:35:15.498401339Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.498405088Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.498422860Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.498428231Z             if _is_in_bad_fork():
2024-12-30T09:35:15.498431134Z                 raise RuntimeError(
2024-12-30T09:35:15.498441954Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.498445396Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.498447469Z                 )
2024-12-30T09:35:15.498449447Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.498484219Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.498486916Z             if _cudart is None:
2024-12-30T09:35:15.498488919Z                 raise AssertionError(
2024-12-30T09:35:15.498495140Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.498498103Z                 )
2024-12-30T09:35:15.498500692Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.498503441Z             # are found or any other error occurs
2024-12-30T09:35:15.498543383Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.498547023Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.498557153Z >           torch._C._cuda_init()
2024-12-30T09:35:15.498559379Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.498593424Z 
2024-12-30T09:35:15.498595801Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.498752433Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.498756297Z 
2024-12-30T09:35:15.498775662Z b = 2, n = 257, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.498778281Z 
2024-12-30T09:35:15.498945847Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.498948808Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.498951001Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.498974170Z         torch.manual_seed(2024)
2024-12-30T09:35:15.498976643Z         atol = 5e-2
2024-12-30T09:35:15.498978530Z         rtol = 1e-2
2024-12-30T09:35:15.498980410Z         device = torch.device("cuda")
2024-12-30T09:35:15.498996178Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.499001278Z 
2024-12-30T09:35:15.499021472Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.499029607Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.499033312Z 
2024-12-30T09:35:15.499253771Z     def _lazy_init():
2024-12-30T09:35:15.499262029Z         global _initialized, _queued_calls
2024-12-30T09:35:15.499264100Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.499266414Z             return
2024-12-30T09:35:15.499268322Z         with _initialization_lock:
2024-12-30T09:35:15.499275110Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.499277195Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.499279359Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.499281383Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.499283433Z             # find there is nothing left to do.
2024-12-30T09:35:15.499287035Z             if is_initialized():
2024-12-30T09:35:15.499289149Z                 return
2024-12-30T09:35:15.499291095Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.499294661Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.499309170Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.499312812Z             if _is_in_bad_fork():
2024-12-30T09:35:15.499323018Z                 raise RuntimeError(
2024-12-30T09:35:15.499337029Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.499339425Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.499341480Z                 )
2024-12-30T09:35:15.499343367Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.499374754Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.499378567Z             if _cudart is None:
2024-12-30T09:35:15.499380605Z                 raise AssertionError(
2024-12-30T09:35:15.499383187Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.499385349Z                 )
2024-12-30T09:35:15.499424154Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.499427916Z             # are found or any other error occurs
2024-12-30T09:35:15.499429886Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.499432005Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.499434620Z >           torch._C._cuda_init()
2024-12-30T09:35:15.499458073Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.499462757Z 
2024-12-30T09:35:15.499464608Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.499629122Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.499636436Z 
2024-12-30T09:35:15.499655625Z b = 2, n = 1024, d = 768, dtype = torch.float32
2024-12-30T09:35:15.499658071Z 
2024-12-30T09:35:15.499845826Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.499850075Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.499852329Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.499854260Z         torch.manual_seed(2024)
2024-12-30T09:35:15.499856283Z         atol = 5e-2
2024-12-30T09:35:15.499858182Z         rtol = 1e-2
2024-12-30T09:35:15.499874311Z         device = torch.device("cuda")
2024-12-30T09:35:15.499877980Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.499880177Z 
2024-12-30T09:35:15.499917046Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.499919709Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.499922000Z 
2024-12-30T09:35:15.500113507Z     def _lazy_init():
2024-12-30T09:35:15.500116573Z         global _initialized, _queued_calls
2024-12-30T09:35:15.500118558Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.500120748Z             return
2024-12-30T09:35:15.500122596Z         with _initialization_lock:
2024-12-30T09:35:15.500134939Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.500138403Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.500141176Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.500150981Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.500155234Z             # find there is nothing left to do.
2024-12-30T09:35:15.500172189Z             if is_initialized():
2024-12-30T09:35:15.500183003Z                 return
2024-12-30T09:35:15.500186216Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.500202312Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.500204825Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.500206795Z             if _is_in_bad_fork():
2024-12-30T09:35:15.500219611Z                 raise RuntimeError(
2024-12-30T09:35:15.500228928Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.500236096Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.500260751Z                 )
2024-12-30T09:35:15.500264887Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.500267201Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.500269317Z             if _cudart is None:
2024-12-30T09:35:15.500280286Z                 raise AssertionError(
2024-12-30T09:35:15.500283452Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.500293123Z                 )
2024-12-30T09:35:15.500296699Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.500314506Z             # are found or any other error occurs
2024-12-30T09:35:15.500339627Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.500343611Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.500347053Z >           torch._C._cuda_init()
2024-12-30T09:35:15.500381409Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.500385140Z 
2024-12-30T09:35:15.500387003Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.500555266Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.500560298Z 
2024-12-30T09:35:15.500634595Z b = 2, n = 1024, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.500638192Z 
2024-12-30T09:35:15.500753180Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.500756862Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.500759125Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.500761262Z         torch.manual_seed(2024)
2024-12-30T09:35:15.500763344Z         atol = 5e-2
2024-12-30T09:35:15.500797700Z         rtol = 1e-2
2024-12-30T09:35:15.500808367Z         device = torch.device("cuda")
2024-12-30T09:35:15.500810845Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.500813208Z 
2024-12-30T09:35:15.500826309Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.500830299Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.500836758Z 
2024-12-30T09:35:15.501036375Z     def _lazy_init():
2024-12-30T09:35:15.501039447Z         global _initialized, _queued_calls
2024-12-30T09:35:15.501041524Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.501043739Z             return
2024-12-30T09:35:15.501045983Z         with _initialization_lock:
2024-12-30T09:35:15.501047965Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.501049960Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.501071241Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.501075071Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.501077123Z             # find there is nothing left to do.
2024-12-30T09:35:15.501079069Z             if is_initialized():
2024-12-30T09:35:15.501081067Z                 return
2024-12-30T09:35:15.501104033Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.501107266Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.501109280Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.501111237Z             if _is_in_bad_fork():
2024-12-30T09:35:15.501123812Z                 raise RuntimeError(
2024-12-30T09:35:15.501133262Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.501135513Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.501144080Z                 )
2024-12-30T09:35:15.501157249Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.501159595Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.501175940Z             if _cudart is None:
2024-12-30T09:35:15.501178293Z                 raise AssertionError(
2024-12-30T09:35:15.501180187Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.501200166Z                 )
2024-12-30T09:35:15.501202658Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.501204649Z             # are found or any other error occurs
2024-12-30T09:35:15.501212189Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.501215895Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.501238452Z >           torch._C._cuda_init()
2024-12-30T09:35:15.501249073Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.501254642Z 
2024-12-30T09:35:15.501279554Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.501429433Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.501432656Z 
2024-12-30T09:35:15.501461705Z b = 2, n = 1025, d = 768, dtype = torch.float32
2024-12-30T09:35:15.501464222Z 
2024-12-30T09:35:15.501652951Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.501656150Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.501658271Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.501660189Z         torch.manual_seed(2024)
2024-12-30T09:35:15.501662061Z         atol = 5e-2
2024-12-30T09:35:15.501664009Z         rtol = 1e-2
2024-12-30T09:35:15.501666172Z         device = torch.device("cuda")
2024-12-30T09:35:15.501700049Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.501702736Z 
2024-12-30T09:35:15.501705144Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.501732199Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.501736675Z 
2024-12-30T09:35:15.501910485Z     def _lazy_init():
2024-12-30T09:35:15.501913757Z         global _initialized, _queued_calls
2024-12-30T09:35:15.501915743Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.501917852Z             return
2024-12-30T09:35:15.501919754Z         with _initialization_lock:
2024-12-30T09:35:15.501929702Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.501938139Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.501940483Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.501943292Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.501973114Z             # find there is nothing left to do.
2024-12-30T09:35:15.501976181Z             if is_initialized():
2024-12-30T09:35:15.501978120Z                 return
2024-12-30T09:35:15.502003065Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.502015451Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.502017898Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.502020702Z             if _is_in_bad_fork():
2024-12-30T09:35:15.502022675Z                 raise RuntimeError(
2024-12-30T09:35:15.502024630Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.502045011Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.502060945Z                 )
2024-12-30T09:35:15.502063583Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.502067453Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.502069752Z             if _cudart is None:
2024-12-30T09:35:15.502079581Z                 raise AssertionError(
2024-12-30T09:35:15.502087374Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.502110702Z                 )
2024-12-30T09:35:15.502114128Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.502116403Z             # are found or any other error occurs
2024-12-30T09:35:15.502118568Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.502122165Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.502125677Z >           torch._C._cuda_init()
2024-12-30T09:35:15.502169061Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.502173916Z 
2024-12-30T09:35:15.502175773Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.502318987Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-2-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.502322103Z 
2024-12-30T09:35:15.502343673Z b = 2, n = 1025, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.502346178Z 
2024-12-30T09:35:15.502521834Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.502524578Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.502526710Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.502528690Z         torch.manual_seed(2024)
2024-12-30T09:35:15.502542592Z         atol = 5e-2
2024-12-30T09:35:15.502547234Z         rtol = 1e-2
2024-12-30T09:35:15.502567600Z         device = torch.device("cuda")
2024-12-30T09:35:15.502569811Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.502572174Z 
2024-12-30T09:35:15.502672367Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.502676552Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.502679043Z 
2024-12-30T09:35:15.502814723Z     def _lazy_init():
2024-12-30T09:35:15.502819495Z         global _initialized, _queued_calls
2024-12-30T09:35:15.502822944Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.502826344Z             return
2024-12-30T09:35:15.502847938Z         with _initialization_lock:
2024-12-30T09:35:15.502857045Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.502859331Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.502861426Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.502864313Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.502866542Z             # find there is nothing left to do.
2024-12-30T09:35:15.502885262Z             if is_initialized():
2024-12-30T09:35:15.502888891Z                 return
2024-12-30T09:35:15.502890834Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.502903417Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.502908042Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.502910113Z             if _is_in_bad_fork():
2024-12-30T09:35:15.502924576Z                 raise RuntimeError(
2024-12-30T09:35:15.502928383Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.502932584Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.502935259Z                 )
2024-12-30T09:35:15.502937334Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.502965809Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.502969060Z             if _cudart is None:
2024-12-30T09:35:15.502970974Z                 raise AssertionError(
2024-12-30T09:35:15.502995135Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.502997566Z                 )
2024-12-30T09:35:15.503002941Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.503005590Z             # are found or any other error occurs
2024-12-30T09:35:15.503007707Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.503021271Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.503034936Z >           torch._C._cuda_init()
2024-12-30T09:35:15.503057357Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.503063795Z 
2024-12-30T09:35:15.503067704Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.503221810Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-4-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.503227180Z 
2024-12-30T09:35:15.503256771Z b = 4, n = 127, d = 768, dtype = torch.float32
2024-12-30T09:35:15.503268029Z 
2024-12-30T09:35:15.503445008Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.503449083Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.503451482Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.503453437Z         torch.manual_seed(2024)
2024-12-30T09:35:15.503455460Z         atol = 5e-2
2024-12-30T09:35:15.503457495Z         rtol = 1e-2
2024-12-30T09:35:15.503464131Z         device = torch.device("cuda")
2024-12-30T09:35:15.503466468Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.503468682Z 
2024-12-30T09:35:15.503488743Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.503525280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.503531138Z 
2024-12-30T09:35:15.503727862Z     def _lazy_init():
2024-12-30T09:35:15.503733064Z         global _initialized, _queued_calls
2024-12-30T09:35:15.503735055Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.503737371Z             return
2024-12-30T09:35:15.503739296Z         with _initialization_lock:
2024-12-30T09:35:15.503741343Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.503772232Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.503779397Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.503781566Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.503783623Z             # find there is nothing left to do.
2024-12-30T09:35:15.503785684Z             if is_initialized():
2024-12-30T09:35:15.503787608Z                 return
2024-12-30T09:35:15.503789513Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.503795382Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.503799237Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.503801324Z             if _is_in_bad_fork():
2024-12-30T09:35:15.503824756Z                 raise RuntimeError(
2024-12-30T09:35:15.503827969Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.503830510Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.503840235Z                 )
2024-12-30T09:35:15.503842393Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.503861601Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.503864117Z             if _cudart is None:
2024-12-30T09:35:15.503869316Z                 raise AssertionError(
2024-12-30T09:35:15.503871404Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.503885936Z                 )
2024-12-30T09:35:15.503888296Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.503902027Z             # are found or any other error occurs
2024-12-30T09:35:15.503906112Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.503956819Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.503959587Z >           torch._C._cuda_init()
2024-12-30T09:35:15.503961791Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.503964143Z 
2024-12-30T09:35:15.503974571Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.504121329Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.504124775Z 
2024-12-30T09:35:15.504159572Z b = 4, n = 127, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.504165713Z 
2024-12-30T09:35:15.504320915Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.504324382Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.504326786Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.504328699Z         torch.manual_seed(2024)
2024-12-30T09:35:15.504348968Z         atol = 5e-2
2024-12-30T09:35:15.504352335Z         rtol = 1e-2
2024-12-30T09:35:15.504354634Z         device = torch.device("cuda")
2024-12-30T09:35:15.504356587Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.504368670Z 
2024-12-30T09:35:15.504400516Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.504403327Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.504405610Z 
2024-12-30T09:35:15.504607262Z     def _lazy_init():
2024-12-30T09:35:15.504610918Z         global _initialized, _queued_calls
2024-12-30T09:35:15.504612846Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.504614979Z             return
2024-12-30T09:35:15.504616879Z         with _initialization_lock:
2024-12-30T09:35:15.504626039Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.504629642Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.504651430Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.504654581Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.504656647Z             # find there is nothing left to do.
2024-12-30T09:35:15.504658573Z             if is_initialized():
2024-12-30T09:35:15.504681217Z                 return
2024-12-30T09:35:15.504683655Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.504685638Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.504687709Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.504705490Z             if _is_in_bad_fork():
2024-12-30T09:35:15.504707864Z                 raise RuntimeError(
2024-12-30T09:35:15.504709769Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.504731487Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.504734675Z                 )
2024-12-30T09:35:15.504736586Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.504754565Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.504756880Z             if _cudart is None:
2024-12-30T09:35:15.504758802Z                 raise AssertionError(
2024-12-30T09:35:15.504782663Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.504785146Z                 )
2024-12-30T09:35:15.504787090Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.504793963Z             # are found or any other error occurs
2024-12-30T09:35:15.504796038Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.504837597Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.504840416Z >           torch._C._cuda_init()
2024-12-30T09:35:15.504842438Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.504844803Z 
2024-12-30T09:35:15.504879071Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.505041037Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-4-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.505050023Z 
2024-12-30T09:35:15.505051986Z b = 4, n = 128, d = 768, dtype = torch.float32
2024-12-30T09:35:15.505053994Z 
2024-12-30T09:35:15.505227522Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.505231369Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.505233588Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.505235621Z         torch.manual_seed(2024)
2024-12-30T09:35:15.505237579Z         atol = 5e-2
2024-12-30T09:35:15.505239483Z         rtol = 1e-2
2024-12-30T09:35:15.505241396Z         device = torch.device("cuda")
2024-12-30T09:35:15.505243425Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.505284755Z 
2024-12-30T09:35:15.505288411Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.505290746Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.505293102Z 
2024-12-30T09:35:15.505504207Z     def _lazy_init():
2024-12-30T09:35:15.505507472Z         global _initialized, _queued_calls
2024-12-30T09:35:15.505514237Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.505516780Z             return
2024-12-30T09:35:15.505518681Z         with _initialization_lock:
2024-12-30T09:35:15.505520586Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.505522639Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.505524717Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.505539160Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.505543930Z             # find there is nothing left to do.
2024-12-30T09:35:15.505546204Z             if is_initialized():
2024-12-30T09:35:15.505557358Z                 return
2024-12-30T09:35:15.505560336Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.505562507Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.505576545Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.505580274Z             if _is_in_bad_fork():
2024-12-30T09:35:15.505611894Z                 raise RuntimeError(
2024-12-30T09:35:15.505614637Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.505616857Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.505619064Z                 )
2024-12-30T09:35:15.505623985Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.505627312Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.505639443Z             if _cudart is None:
2024-12-30T09:35:15.505641926Z                 raise AssertionError(
2024-12-30T09:35:15.505648462Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.505671671Z                 )
2024-12-30T09:35:15.505674103Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.505676232Z             # are found or any other error occurs
2024-12-30T09:35:15.505678189Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.505727162Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.505730782Z >           torch._C._cuda_init()
2024-12-30T09:35:15.505732836Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.505735296Z 
2024-12-30T09:35:15.505757702Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.505915596Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.505919281Z 
2024-12-30T09:35:15.505921203Z b = 4, n = 128, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.505923241Z 
2024-12-30T09:35:15.506103642Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.506107142Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.506109491Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.506111479Z         torch.manual_seed(2024)
2024-12-30T09:35:15.506113536Z         atol = 5e-2
2024-12-30T09:35:15.506115494Z         rtol = 1e-2
2024-12-30T09:35:15.506117446Z         device = torch.device("cuda")
2024-12-30T09:35:15.506132960Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.506136302Z 
2024-12-30T09:35:15.506161898Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.506165217Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.506168032Z 
2024-12-30T09:35:15.506386130Z     def _lazy_init():
2024-12-30T09:35:15.506389333Z         global _initialized, _queued_calls
2024-12-30T09:35:15.506391321Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.506393647Z             return
2024-12-30T09:35:15.506395626Z         with _initialization_lock:
2024-12-30T09:35:15.506397559Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.506423274Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.506427704Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.506429774Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.506431822Z             # find there is nothing left to do.
2024-12-30T09:35:15.506433806Z             if is_initialized():
2024-12-30T09:35:15.506447498Z                 return
2024-12-30T09:35:15.506450695Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.506476790Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.506481270Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.506487544Z             if _is_in_bad_fork():
2024-12-30T09:35:15.506489532Z                 raise RuntimeError(
2024-12-30T09:35:15.506491459Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.506527287Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.506530883Z                 )
2024-12-30T09:35:15.506532861Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.506535005Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.506537060Z             if _cudart is None:
2024-12-30T09:35:15.506538984Z                 raise AssertionError(
2024-12-30T09:35:15.506540861Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.506544103Z                 )
2024-12-30T09:35:15.506546062Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.506567177Z             # are found or any other error occurs
2024-12-30T09:35:15.506571490Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.506592891Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.506596225Z >           torch._C._cuda_init()
2024-12-30T09:35:15.506626249Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.506629250Z 
2024-12-30T09:35:15.506631123Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.506812092Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-4-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.506818799Z 
2024-12-30T09:35:15.506829194Z b = 4, n = 256, d = 768, dtype = torch.float32
2024-12-30T09:35:15.506832233Z 
2024-12-30T09:35:15.507014121Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.507016738Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.507018876Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.507020804Z         torch.manual_seed(2024)
2024-12-30T09:35:15.507022704Z         atol = 5e-2
2024-12-30T09:35:15.507024672Z         rtol = 1e-2
2024-12-30T09:35:15.507026704Z         device = torch.device("cuda")
2024-12-30T09:35:15.507028669Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.507044491Z 
2024-12-30T09:35:15.507048398Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.507092748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.507096254Z 
2024-12-30T09:35:15.507264909Z     def _lazy_init():
2024-12-30T09:35:15.507268198Z         global _initialized, _queued_calls
2024-12-30T09:35:15.507270226Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.507272440Z             return
2024-12-30T09:35:15.507274341Z         with _initialization_lock:
2024-12-30T09:35:15.507288201Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.507291465Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.507293550Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.507295545Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.507342654Z             # find there is nothing left to do.
2024-12-30T09:35:15.507345255Z             if is_initialized():
2024-12-30T09:35:15.507347145Z                 return
2024-12-30T09:35:15.507349029Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.507351172Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.507381198Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.507384703Z             if _is_in_bad_fork():
2024-12-30T09:35:15.507386677Z                 raise RuntimeError(
2024-12-30T09:35:15.507388705Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.507413140Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.507415790Z                 )
2024-12-30T09:35:15.507417763Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.507419796Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.507426202Z             if _cudart is None:
2024-12-30T09:35:15.507428309Z                 raise AssertionError(
2024-12-30T09:35:15.507462216Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.507465597Z                 )
2024-12-30T09:35:15.507467499Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.507469605Z             # are found or any other error occurs
2024-12-30T09:35:15.507491109Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.507496386Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.507499434Z >           torch._C._cuda_init()
2024-12-30T09:35:15.507534550Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.507538978Z 
2024-12-30T09:35:15.507541850Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.507712330Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.507717269Z 
2024-12-30T09:35:15.507745242Z b = 4, n = 256, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.507748730Z 
2024-12-30T09:35:15.507996777Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.508001966Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.508005405Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.508008593Z         torch.manual_seed(2024)
2024-12-30T09:35:15.508011657Z         atol = 5e-2
2024-12-30T09:35:15.508014732Z         rtol = 1e-2
2024-12-30T09:35:15.508042238Z         device = torch.device("cuda")
2024-12-30T09:35:15.508046359Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.508048640Z 
2024-12-30T09:35:15.508050524Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.508108965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.508112254Z 
2024-12-30T09:35:15.508379837Z     def _lazy_init():
2024-12-30T09:35:15.508385235Z         global _initialized, _queued_calls
2024-12-30T09:35:15.508388535Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.508391904Z             return
2024-12-30T09:35:15.508394869Z         with _initialization_lock:
2024-12-30T09:35:15.508398990Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.508402287Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.508405477Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.508446984Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.508460534Z             # find there is nothing left to do.
2024-12-30T09:35:15.508462670Z             if is_initialized():
2024-12-30T09:35:15.508464689Z                 return
2024-12-30T09:35:15.508466626Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.508468771Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.508471908Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.508474029Z             if _is_in_bad_fork():
2024-12-30T09:35:15.508476051Z                 raise RuntimeError(
2024-12-30T09:35:15.508497531Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.508501340Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.508506889Z                 )
2024-12-30T09:35:15.508508997Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.508560267Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.508564616Z             if _cudart is None:
2024-12-30T09:35:15.508566595Z                 raise AssertionError(
2024-12-30T09:35:15.508568649Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.508570809Z                 )
2024-12-30T09:35:15.508572717Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.508579948Z             # are found or any other error occurs
2024-12-30T09:35:15.508582188Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.508587632Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.508609142Z >           torch._C._cuda_init()
2024-12-30T09:35:15.508628162Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.508632096Z 
2024-12-30T09:35:15.508681299Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.508847220Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-4-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.508852204Z 
2024-12-30T09:35:15.508854078Z b = 4, n = 257, d = 768, dtype = torch.float32
2024-12-30T09:35:15.508856114Z 
2024-12-30T09:35:15.509128567Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.509132470Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.509141592Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.509143597Z         torch.manual_seed(2024)
2024-12-30T09:35:15.509145570Z         atol = 5e-2
2024-12-30T09:35:15.509147444Z         rtol = 1e-2
2024-12-30T09:35:15.509149350Z         device = torch.device("cuda")
2024-12-30T09:35:15.509189750Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.509193516Z 
2024-12-30T09:35:15.509195523Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.509221416Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.509225110Z 
2024-12-30T09:35:15.509427475Z     def _lazy_init():
2024-12-30T09:35:15.509430745Z         global _initialized, _queued_calls
2024-12-30T09:35:15.509432795Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.509434970Z             return
2024-12-30T09:35:15.509436875Z         with _initialization_lock:
2024-12-30T09:35:15.509474638Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.509479298Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.509481457Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.509483553Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.509485789Z             # find there is nothing left to do.
2024-12-30T09:35:15.509487869Z             if is_initialized():
2024-12-30T09:35:15.509489830Z                 return
2024-12-30T09:35:15.509491785Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.509528475Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.509531590Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.509533658Z             if _is_in_bad_fork():
2024-12-30T09:35:15.509535628Z                 raise RuntimeError(
2024-12-30T09:35:15.509537608Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.509579420Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.509586064Z                 )
2024-12-30T09:35:15.509588138Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.509590334Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.509593380Z             if _cudart is None:
2024-12-30T09:35:15.509599355Z                 raise AssertionError(
2024-12-30T09:35:15.509601372Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.509603487Z                 )
2024-12-30T09:35:15.509614895Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.509618289Z             # are found or any other error occurs
2024-12-30T09:35:15.509620308Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.509635018Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.509637498Z >           torch._C._cuda_init()
2024-12-30T09:35:15.509684209Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.509687424Z 
2024-12-30T09:35:15.509689292Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.509895793Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.509900384Z 
2024-12-30T09:35:15.509902420Z b = 4, n = 257, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.509904537Z 
2024-12-30T09:35:15.510110776Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.510114143Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.510116363Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.510118412Z         torch.manual_seed(2024)
2024-12-30T09:35:15.510120434Z         atol = 5e-2
2024-12-30T09:35:15.510122332Z         rtol = 1e-2
2024-12-30T09:35:15.510136930Z         device = torch.device("cuda")
2024-12-30T09:35:15.510140284Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.510142640Z 
2024-12-30T09:35:15.510194274Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.510197584Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.510199996Z 
2024-12-30T09:35:15.510385542Z     def _lazy_init():
2024-12-30T09:35:15.510389137Z         global _initialized, _queued_calls
2024-12-30T09:35:15.510391216Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.510393441Z             return
2024-12-30T09:35:15.510399358Z         with _initialization_lock:
2024-12-30T09:35:15.510404367Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.510406576Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.510408589Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.510410584Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.510443589Z             # find there is nothing left to do.
2024-12-30T09:35:15.510447343Z             if is_initialized():
2024-12-30T09:35:15.510449450Z                 return
2024-12-30T09:35:15.510451406Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.510471081Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.510474402Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.510476435Z             if _is_in_bad_fork():
2024-12-30T09:35:15.510478411Z                 raise RuntimeError(
2024-12-30T09:35:15.510495159Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.510498265Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.510500338Z                 )
2024-12-30T09:35:15.510511564Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.510514829Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.510540398Z             if _cudart is None:
2024-12-30T09:35:15.510543370Z                 raise AssertionError(
2024-12-30T09:35:15.510545496Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.510561325Z                 )
2024-12-30T09:35:15.510564096Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.510583656Z             # are found or any other error occurs
2024-12-30T09:35:15.510586752Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.510602252Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.510614346Z >           torch._C._cuda_init()
2024-12-30T09:35:15.510645035Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.510648452Z 
2024-12-30T09:35:15.510650386Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.510807979Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.510818915Z 
2024-12-30T09:35:15.510842499Z b = 4, n = 1024, d = 768, dtype = torch.float32
2024-12-30T09:35:15.510845821Z 
2024-12-30T09:35:15.511018319Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.511021926Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.511024159Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.511026230Z         torch.manual_seed(2024)
2024-12-30T09:35:15.511028268Z         atol = 5e-2
2024-12-30T09:35:15.511042708Z         rtol = 1e-2
2024-12-30T09:35:15.511045931Z         device = torch.device("cuda")
2024-12-30T09:35:15.511048013Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.511051433Z 
2024-12-30T09:35:15.511099414Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.511103051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.511105441Z 
2024-12-30T09:35:15.511289015Z     def _lazy_init():
2024-12-30T09:35:15.511291874Z         global _initialized, _queued_calls
2024-12-30T09:35:15.511293935Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.511296098Z             return
2024-12-30T09:35:15.511297975Z         with _initialization_lock:
2024-12-30T09:35:15.511333944Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.511349431Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.511351679Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.511353888Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.511356679Z             # find there is nothing left to do.
2024-12-30T09:35:15.511366593Z             if is_initialized():
2024-12-30T09:35:15.511369162Z                 return
2024-12-30T09:35:15.511412792Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.511415895Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.511418127Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.511420154Z             if _is_in_bad_fork():
2024-12-30T09:35:15.511457469Z                 raise RuntimeError(
2024-12-30T09:35:15.511464543Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.511466860Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.511483571Z                 )
2024-12-30T09:35:15.511489513Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.511491639Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.511514215Z             if _cudart is None:
2024-12-30T09:35:15.511517725Z                 raise AssertionError(
2024-12-30T09:35:15.511519673Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.511567821Z                 )
2024-12-30T09:35:15.511571576Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.511573773Z             # are found or any other error occurs
2024-12-30T09:35:15.511585474Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.511594887Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.511597640Z >           torch._C._cuda_init()
2024-12-30T09:35:15.511673874Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.511677775Z 
2024-12-30T09:35:15.511679628Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.511914256Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.511918860Z 
2024-12-30T09:35:15.511949880Z b = 4, n = 1024, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.511953388Z 
2024-12-30T09:35:15.512226488Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.512230786Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.512232973Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.512234903Z         torch.manual_seed(2024)
2024-12-30T09:35:15.512236983Z         atol = 5e-2
2024-12-30T09:35:15.512239364Z         rtol = 1e-2
2024-12-30T09:35:15.512241368Z         device = torch.device("cuda")
2024-12-30T09:35:15.512261889Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.512265225Z 
2024-12-30T09:35:15.512341822Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.512345503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.512354692Z 
2024-12-30T09:35:15.512630584Z     def _lazy_init():
2024-12-30T09:35:15.512634685Z         global _initialized, _queued_calls
2024-12-30T09:35:15.512636741Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.512638932Z             return
2024-12-30T09:35:15.512653925Z         with _initialization_lock:
2024-12-30T09:35:15.512657656Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.512659822Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.512661856Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.512664607Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.512666656Z             # find there is nothing left to do.
2024-12-30T09:35:15.512718680Z             if is_initialized():
2024-12-30T09:35:15.512721854Z                 return
2024-12-30T09:35:15.512723936Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.512725999Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.512728054Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.512730031Z             if _is_in_bad_fork():
2024-12-30T09:35:15.512742437Z                 raise RuntimeError(
2024-12-30T09:35:15.512745702Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.512747956Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.512760205Z                 )
2024-12-30T09:35:15.512765864Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.512770763Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.512806989Z             if _cudart is None:
2024-12-30T09:35:15.512809551Z                 raise AssertionError(
2024-12-30T09:35:15.512811483Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.512813590Z                 )
2024-12-30T09:35:15.512816902Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.512841090Z             # are found or any other error occurs
2024-12-30T09:35:15.512843911Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.512845978Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.512874212Z >           torch._C._cuda_init()
2024-12-30T09:35:15.512881218Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.512883944Z 
2024-12-30T09:35:15.512924514Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.513097246Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.513102294Z 
2024-12-30T09:35:15.513104217Z b = 4, n = 1025, d = 768, dtype = torch.float32
2024-12-30T09:35:15.513106264Z 
2024-12-30T09:35:15.513317981Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.513321485Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.513323752Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.513325670Z         torch.manual_seed(2024)
2024-12-30T09:35:15.513327543Z         atol = 5e-2
2024-12-30T09:35:15.513329489Z         rtol = 1e-2
2024-12-30T09:35:15.513331385Z         device = torch.device("cuda")
2024-12-30T09:35:15.513346310Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.513349539Z 
2024-12-30T09:35:15.513371424Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.513414351Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.513417897Z 
2024-12-30T09:35:15.513603186Z     def _lazy_init():
2024-12-30T09:35:15.513607398Z         global _initialized, _queued_calls
2024-12-30T09:35:15.513609383Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.513611638Z             return
2024-12-30T09:35:15.513613549Z         with _initialization_lock:
2024-12-30T09:35:15.513618302Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.513621595Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.513654004Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.513657822Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.513659915Z             # find there is nothing left to do.
2024-12-30T09:35:15.513661931Z             if is_initialized():
2024-12-30T09:35:15.513663849Z                 return
2024-12-30T09:35:15.513695292Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.513699028Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.513701079Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.513703071Z             if _is_in_bad_fork():
2024-12-30T09:35:15.513705085Z                 raise RuntimeError(
2024-12-30T09:35:15.513725677Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.513729128Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.513731192Z                 )
2024-12-30T09:35:15.513733097Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.513754219Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.513756931Z             if _cudart is None:
2024-12-30T09:35:15.513758844Z                 raise AssertionError(
2024-12-30T09:35:15.513760910Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.513763033Z                 )
2024-12-30T09:35:15.513788792Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.513792099Z             # are found or any other error occurs
2024-12-30T09:35:15.513794157Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.513796303Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.513816709Z >           torch._C._cuda_init()
2024-12-30T09:35:15.513819280Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.513822234Z 
2024-12-30T09:35:15.513873978Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.514074881Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-4-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.514080965Z 
2024-12-30T09:35:15.514083877Z b = 4, n = 1025, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.514087272Z 
2024-12-30T09:35:15.514211656Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.514215310Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.514217510Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.514229182Z         torch.manual_seed(2024)
2024-12-30T09:35:15.514237491Z         atol = 5e-2
2024-12-30T09:35:15.514274660Z         rtol = 1e-2
2024-12-30T09:35:15.514278336Z         device = torch.device("cuda")
2024-12-30T09:35:15.514280369Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.514282483Z 
2024-12-30T09:35:15.514330582Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.514333247Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.514335704Z 
2024-12-30T09:35:15.514572327Z     def _lazy_init():
2024-12-30T09:35:15.514577963Z         global _initialized, _queued_calls
2024-12-30T09:35:15.514579977Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.514582197Z             return
2024-12-30T09:35:15.514584221Z         with _initialization_lock:
2024-12-30T09:35:15.514622149Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.514624988Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.514626975Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.514629098Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.514631225Z             # find there is nothing left to do.
2024-12-30T09:35:15.514633222Z             if is_initialized():
2024-12-30T09:35:15.514661346Z                 return
2024-12-30T09:35:15.514665008Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.514667165Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.514669208Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.514750693Z             if _is_in_bad_fork():
2024-12-30T09:35:15.514753610Z                 raise RuntimeError(
2024-12-30T09:35:15.514755590Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.514757727Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.514759803Z                 )
2024-12-30T09:35:15.514761685Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.514763779Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.514765838Z             if _cudart is None:
2024-12-30T09:35:15.514767718Z                 raise AssertionError(
2024-12-30T09:35:15.514769693Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.514776371Z                 )
2024-12-30T09:35:15.514779191Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.514781259Z             # are found or any other error occurs
2024-12-30T09:35:15.514783206Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.514785214Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.514813867Z >           torch._C._cuda_init()
2024-12-30T09:35:15.514819899Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.514823227Z 
2024-12-30T09:35:15.514839702Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.515067931Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-8-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.515072298Z 
2024-12-30T09:35:15.515074301Z b = 8, n = 127, d = 768, dtype = torch.float32
2024-12-30T09:35:15.515076274Z 
2024-12-30T09:35:15.515305907Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.515310941Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.515313256Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.515315248Z         torch.manual_seed(2024)
2024-12-30T09:35:15.515317197Z         atol = 5e-2
2024-12-30T09:35:15.515319090Z         rtol = 1e-2
2024-12-30T09:35:15.515322576Z         device = torch.device("cuda")
2024-12-30T09:35:15.515324742Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.515326815Z 
2024-12-30T09:35:15.515380030Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.515391259Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.515393996Z 
2024-12-30T09:35:15.515589336Z     def _lazy_init():
2024-12-30T09:35:15.515594017Z         global _initialized, _queued_calls
2024-12-30T09:35:15.515595986Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.515598216Z             return
2024-12-30T09:35:15.515600212Z         with _initialization_lock:
2024-12-30T09:35:15.515602771Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.515609514Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.515624840Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.515628006Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.515645477Z             # find there is nothing left to do.
2024-12-30T09:35:15.515654051Z             if is_initialized():
2024-12-30T09:35:15.515673069Z                 return
2024-12-30T09:35:15.515675917Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.515678871Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.515694063Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.515697194Z             if _is_in_bad_fork():
2024-12-30T09:35:15.515708134Z                 raise RuntimeError(
2024-12-30T09:35:15.515717864Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.515720135Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.515722211Z                 )
2024-12-30T09:35:15.515728062Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.515743500Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.515746012Z             if _cudart is None:
2024-12-30T09:35:15.515747946Z                 raise AssertionError(
2024-12-30T09:35:15.515784616Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.515788383Z                 )
2024-12-30T09:35:15.515790474Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.515792623Z             # are found or any other error occurs
2024-12-30T09:35:15.515794584Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.515807892Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.515811158Z >           torch._C._cuda_init()
2024-12-30T09:35:15.515845125Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.515848511Z 
2024-12-30T09:35:15.515850422Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.516018493Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.516022196Z 
2024-12-30T09:35:15.516046862Z b = 8, n = 127, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.516050641Z 
2024-12-30T09:35:15.516221557Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.516224949Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.516227019Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.516229001Z         torch.manual_seed(2024)
2024-12-30T09:35:15.516234106Z         atol = 5e-2
2024-12-30T09:35:15.516236177Z         rtol = 1e-2
2024-12-30T09:35:15.516238215Z         device = torch.device("cuda")
2024-12-30T09:35:15.516249986Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.516264406Z 
2024-12-30T09:35:15.516285913Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.516316415Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.516319744Z 
2024-12-30T09:35:15.516524523Z     def _lazy_init():
2024-12-30T09:35:15.516527981Z         global _initialized, _queued_calls
2024-12-30T09:35:15.516530054Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.516532432Z             return
2024-12-30T09:35:15.516534342Z         with _initialization_lock:
2024-12-30T09:35:15.516563745Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.516567059Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.516569241Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.516571324Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.516584607Z             # find there is nothing left to do.
2024-12-30T09:35:15.516589859Z             if is_initialized():
2024-12-30T09:35:15.516607850Z                 return
2024-12-30T09:35:15.516610675Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.516612694Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.516614761Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.516633422Z             if _is_in_bad_fork():
2024-12-30T09:35:15.516635759Z                 raise RuntimeError(
2024-12-30T09:35:15.516643216Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.516645522Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.516656251Z                 )
2024-12-30T09:35:15.516687673Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.516690556Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.516692734Z             if _cudart is None:
2024-12-30T09:35:15.516694754Z                 raise AssertionError(
2024-12-30T09:35:15.516705428Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.516709330Z                 )
2024-12-30T09:35:15.516712628Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.516725163Z             # are found or any other error occurs
2024-12-30T09:35:15.516733226Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.516736234Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.516738900Z >           torch._C._cuda_init()
2024-12-30T09:35:15.516782149Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.516785061Z 
2024-12-30T09:35:15.516786960Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.516953587Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-8-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.516957466Z 
2024-12-30T09:35:15.516978517Z b = 8, n = 128, d = 768, dtype = torch.float32
2024-12-30T09:35:15.516981071Z 
2024-12-30T09:35:15.517197770Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.517203291Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.517206611Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.517209550Z         torch.manual_seed(2024)
2024-12-30T09:35:15.517212460Z         atol = 5e-2
2024-12-30T09:35:15.517215934Z         rtol = 1e-2
2024-12-30T09:35:15.517217939Z         device = torch.device("cuda")
2024-12-30T09:35:15.517220626Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.517222718Z 
2024-12-30T09:35:15.517224625Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.517259525Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.517264395Z 
2024-12-30T09:35:15.517473803Z     def _lazy_init():
2024-12-30T09:35:15.517477500Z         global _initialized, _queued_calls
2024-12-30T09:35:15.517479499Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.517481789Z             return
2024-12-30T09:35:15.517483803Z         with _initialization_lock:
2024-12-30T09:35:15.517504990Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.517508659Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.517510776Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.517512849Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.517560846Z             # find there is nothing left to do.
2024-12-30T09:35:15.517564677Z             if is_initialized():
2024-12-30T09:35:15.517566690Z                 return
2024-12-30T09:35:15.517568842Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.517570956Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.517573718Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.517603543Z             if _is_in_bad_fork():
2024-12-30T09:35:15.517606110Z                 raise RuntimeError(
2024-12-30T09:35:15.517608200Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.517611130Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.517629186Z                 )
2024-12-30T09:35:15.517631855Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.517634066Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.517662363Z             if _cudart is None:
2024-12-30T09:35:15.517668436Z                 raise AssertionError(
2024-12-30T09:35:15.517670583Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.517672802Z                 )
2024-12-30T09:35:15.517675326Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.517691949Z             # are found or any other error occurs
2024-12-30T09:35:15.517694924Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.517697051Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.517730149Z >           torch._C._cuda_init()
2024-12-30T09:35:15.517733815Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.517739731Z 
2024-12-30T09:35:15.517764386Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.517942525Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.517949254Z 
2024-12-30T09:35:15.517952373Z b = 8, n = 128, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.517956695Z 
2024-12-30T09:35:15.518151598Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.518156428Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.518159904Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.518163110Z         torch.manual_seed(2024)
2024-12-30T09:35:15.518166904Z         atol = 5e-2
2024-12-30T09:35:15.518169344Z         rtol = 1e-2
2024-12-30T09:35:15.518171641Z         device = torch.device("cuda")
2024-12-30T09:35:15.518189907Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.518192435Z 
2024-12-30T09:35:15.518242690Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.518248560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.518252296Z 
2024-12-30T09:35:15.518456564Z     def _lazy_init():
2024-12-30T09:35:15.518463669Z         global _initialized, _queued_calls
2024-12-30T09:35:15.518465784Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.518467987Z             return
2024-12-30T09:35:15.518469890Z         with _initialization_lock:
2024-12-30T09:35:15.518471998Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.518474099Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.518476085Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.518479156Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.518481180Z             # find there is nothing left to do.
2024-12-30T09:35:15.518498758Z             if is_initialized():
2024-12-30T09:35:15.518502245Z                 return
2024-12-30T09:35:15.518504202Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.518521404Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.518529116Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.518531216Z             if _is_in_bad_fork():
2024-12-30T09:35:15.518579274Z                 raise RuntimeError(
2024-12-30T09:35:15.518583337Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.518585651Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.518587688Z                 )
2024-12-30T09:35:15.518589568Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.518605706Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.518608254Z             if _cudart is None:
2024-12-30T09:35:15.518610240Z                 raise AssertionError(
2024-12-30T09:35:15.518619281Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.518621641Z                 )
2024-12-30T09:35:15.518624206Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.518632748Z             # are found or any other error occurs
2024-12-30T09:35:15.518636903Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.518639687Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.518668499Z >           torch._C._cuda_init()
2024-12-30T09:35:15.518671693Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.518687233Z 
2024-12-30T09:35:15.518693234Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.518884595Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-8-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.518888360Z 
2024-12-30T09:35:15.518890344Z b = 8, n = 256, d = 768, dtype = torch.float32
2024-12-30T09:35:15.518892377Z 
2024-12-30T09:35:15.519084850Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.519090801Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.519093049Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.519095012Z         torch.manual_seed(2024)
2024-12-30T09:35:15.519096948Z         atol = 5e-2
2024-12-30T09:35:15.519099004Z         rtol = 1e-2
2024-12-30T09:35:15.519101541Z         device = torch.device("cuda")
2024-12-30T09:35:15.519107524Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.519110029Z 
2024-12-30T09:35:15.519125707Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.519144992Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.519161049Z 
2024-12-30T09:35:15.519342123Z     def _lazy_init():
2024-12-30T09:35:15.519345842Z         global _initialized, _queued_calls
2024-12-30T09:35:15.519347911Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.519349997Z             return
2024-12-30T09:35:15.519351879Z         with _initialization_lock:
2024-12-30T09:35:15.519353851Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.519356420Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.519368298Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.519380566Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.519384346Z             # find there is nothing left to do.
2024-12-30T09:35:15.519416463Z             if is_initialized():
2024-12-30T09:35:15.519419367Z                 return
2024-12-30T09:35:15.519421286Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.519423439Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.519434172Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.519437543Z             if _is_in_bad_fork():
2024-12-30T09:35:15.519439543Z                 raise RuntimeError(
2024-12-30T09:35:15.519464564Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.519467698Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.519469769Z                 )
2024-12-30T09:35:15.519471732Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.519486954Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.519490170Z             if _cudart is None:
2024-12-30T09:35:15.519492090Z                 raise AssertionError(
2024-12-30T09:35:15.519503293Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.519505800Z                 )
2024-12-30T09:35:15.519530426Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.519535975Z             # are found or any other error occurs
2024-12-30T09:35:15.519561061Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.519564267Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.519566409Z >           torch._C._cuda_init()
2024-12-30T09:35:15.519573694Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.519591006Z 
2024-12-30T09:35:15.519596345Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.519766722Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.519769987Z 
2024-12-30T09:35:15.519796091Z b = 8, n = 256, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.519798666Z 
2024-12-30T09:35:15.519975864Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.519980518Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.519982835Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.519984821Z         torch.manual_seed(2024)
2024-12-30T09:35:15.519986853Z         atol = 5e-2
2024-12-30T09:35:15.519989618Z         rtol = 1e-2
2024-12-30T09:35:15.519991714Z         device = torch.device("cuda")
2024-12-30T09:35:15.519993736Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.520016184Z 
2024-12-30T09:35:15.520019032Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.520071432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.520074708Z 
2024-12-30T09:35:15.520258128Z     def _lazy_init():
2024-12-30T09:35:15.520262624Z         global _initialized, _queued_calls
2024-12-30T09:35:15.520265736Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.520269005Z             return
2024-12-30T09:35:15.520272252Z         with _initialization_lock:
2024-12-30T09:35:15.520309008Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.520312175Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.520314144Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.520319997Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.520322102Z             # find there is nothing left to do.
2024-12-30T09:35:15.520324172Z             if is_initialized():
2024-12-30T09:35:15.520326090Z                 return
2024-12-30T09:35:15.520329340Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.520331490Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.520333542Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.520370751Z             if _is_in_bad_fork():
2024-12-30T09:35:15.520373667Z                 raise RuntimeError(
2024-12-30T09:35:15.520375616Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.520387133Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.520391959Z                 )
2024-12-30T09:35:15.520399383Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.520402604Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.520426558Z             if _cudart is None:
2024-12-30T09:35:15.520429086Z                 raise AssertionError(
2024-12-30T09:35:15.520431131Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.520433231Z                 )
2024-12-30T09:35:15.520447152Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.520450797Z             # are found or any other error occurs
2024-12-30T09:35:15.520457594Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.520460732Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.520501557Z >           torch._C._cuda_init()
2024-12-30T09:35:15.520504305Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.520506648Z 
2024-12-30T09:35:15.520511909Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.520687033Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype0-8-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.520691229Z 
2024-12-30T09:35:15.520735785Z b = 8, n = 257, d = 768, dtype = torch.float32
2024-12-30T09:35:15.520738933Z 
2024-12-30T09:35:15.520904279Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.520910162Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.520912464Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.520914364Z         torch.manual_seed(2024)
2024-12-30T09:35:15.520916237Z         atol = 5e-2
2024-12-30T09:35:15.520918105Z         rtol = 1e-2
2024-12-30T09:35:15.520920031Z         device = torch.device("cuda")
2024-12-30T09:35:15.520922077Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.520924547Z 
2024-12-30T09:35:15.520926334Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.520969458Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.520973300Z 
2024-12-30T09:35:15.521148055Z     def _lazy_init():
2024-12-30T09:35:15.521152124Z         global _initialized, _queued_calls
2024-12-30T09:35:15.521154197Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.521156491Z             return
2024-12-30T09:35:15.521158372Z         with _initialization_lock:
2024-12-30T09:35:15.521161593Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.521163673Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.521166317Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.521181754Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.521185920Z             # find there is nothing left to do.
2024-12-30T09:35:15.521201239Z             if is_initialized():
2024-12-30T09:35:15.521204180Z                 return
2024-12-30T09:35:15.521225335Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.521228089Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.521232016Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.521235198Z             if _is_in_bad_fork():
2024-12-30T09:35:15.521238196Z                 raise RuntimeError(
2024-12-30T09:35:15.521267901Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.521272794Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.521274989Z                 )
2024-12-30T09:35:15.521276917Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.521304865Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.521315624Z             if _cudart is None:
2024-12-30T09:35:15.521317757Z                 raise AssertionError(
2024-12-30T09:35:15.521319747Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.521322326Z                 )
2024-12-30T09:35:15.521324760Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.521327091Z             # are found or any other error occurs
2024-12-30T09:35:15.521356470Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.521368990Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.521371221Z >           torch._C._cuda_init()
2024-12-30T09:35:15.521391416Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.521395210Z 
2024-12-30T09:35:15.521411970Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.521569035Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.521572894Z 
2024-12-30T09:35:15.521602727Z b = 8, n = 257, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.521606231Z 
2024-12-30T09:35:15.521768185Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.521771318Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.521773491Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.521775422Z         torch.manual_seed(2024)
2024-12-30T09:35:15.521798771Z         atol = 5e-2
2024-12-30T09:35:15.521801241Z         rtol = 1e-2
2024-12-30T09:35:15.521803125Z         device = torch.device("cuda")
2024-12-30T09:35:15.521805230Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.521823915Z 
2024-12-30T09:35:15.521826467Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.521878979Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.521882632Z 
2024-12-30T09:35:15.522049594Z     def _lazy_init():
2024-12-30T09:35:15.522053406Z         global _initialized, _queued_calls
2024-12-30T09:35:15.522055435Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.522062246Z             return
2024-12-30T09:35:15.522064193Z         with _initialization_lock:
2024-12-30T09:35:15.522101303Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.522110990Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.522114432Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.522117771Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.522120779Z             # find there is nothing left to do.
2024-12-30T09:35:15.522123594Z             if is_initialized():
2024-12-30T09:35:15.522128089Z                 return
2024-12-30T09:35:15.522131064Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.522134187Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.522137214Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.522140198Z             if _is_in_bad_fork():
2024-12-30T09:35:15.522144368Z                 raise RuntimeError(
2024-12-30T09:35:15.522147924Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.522152107Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.522175653Z                 )
2024-12-30T09:35:15.522178627Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.522180756Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.522209232Z             if _cudart is None:
2024-12-30T09:35:15.522214559Z                 raise AssertionError(
2024-12-30T09:35:15.522217766Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.522221121Z                 )
2024-12-30T09:35:15.522234907Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.522237835Z             # are found or any other error occurs
2024-12-30T09:35:15.522239767Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.522241824Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.522280490Z >           torch._C._cuda_init()
2024-12-30T09:35:15.522283850Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.522286362Z 
2024-12-30T09:35:15.522297518Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.522457145Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.522461246Z 
2024-12-30T09:35:15.522484316Z b = 8, n = 1024, d = 768, dtype = torch.float32
2024-12-30T09:35:15.522488323Z 
2024-12-30T09:35:15.522674821Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.522678664Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.522680838Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.522682849Z         torch.manual_seed(2024)
2024-12-30T09:35:15.522684737Z         atol = 5e-2
2024-12-30T09:35:15.522686624Z         rtol = 1e-2
2024-12-30T09:35:15.522700436Z         device = torch.device("cuda")
2024-12-30T09:35:15.522703776Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.522706027Z 
2024-12-30T09:35:15.522745496Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.522748459Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.522750788Z 
2024-12-30T09:35:15.522963875Z     def _lazy_init():
2024-12-30T09:35:15.522969433Z         global _initialized, _queued_calls
2024-12-30T09:35:15.522971488Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.522973686Z             return
2024-12-30T09:35:15.522975562Z         with _initialization_lock:
2024-12-30T09:35:15.522977449Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.522979495Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.522981538Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.522983926Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.522997366Z             # find there is nothing left to do.
2024-12-30T09:35:15.523001691Z             if is_initialized():
2024-12-30T09:35:15.523003655Z                 return
2024-12-30T09:35:15.523018275Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.523021177Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.523023179Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.523032760Z             if _is_in_bad_fork():
2024-12-30T09:35:15.523043814Z                 raise RuntimeError(
2024-12-30T09:35:15.523056464Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.523059905Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.523061984Z                 )
2024-12-30T09:35:15.523074234Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.523077391Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.523082274Z             if _cudart is None:
2024-12-30T09:35:15.523084339Z                 raise AssertionError(
2024-12-30T09:35:15.523118080Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.523121218Z                 )
2024-12-30T09:35:15.523123201Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.523125194Z             # are found or any other error occurs
2024-12-30T09:35:15.523161643Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.523164328Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.523166518Z >           torch._C._cuda_init()
2024-12-30T09:35:15.523168519Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.523171288Z 
2024-12-30T09:35:15.523210073Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.523342120Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.523345509Z 
2024-12-30T09:35:15.523380738Z b = 8, n = 1024, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.523384859Z 
2024-12-30T09:35:15.523558475Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.523563103Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.523565319Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.523567250Z         torch.manual_seed(2024)
2024-12-30T09:35:15.523569175Z         atol = 5e-2
2024-12-30T09:35:15.523577642Z         rtol = 1e-2
2024-12-30T09:35:15.523579751Z         device = torch.device("cuda")
2024-12-30T09:35:15.523581711Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.523623749Z 
2024-12-30T09:35:15.523626069Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.523646787Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.523650695Z 
2024-12-30T09:35:15.523837980Z     def _lazy_init():
2024-12-30T09:35:15.523841217Z         global _initialized, _queued_calls
2024-12-30T09:35:15.523843237Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.523845433Z             return
2024-12-30T09:35:15.523847342Z         with _initialization_lock:
2024-12-30T09:35:15.523868848Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.523873342Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.523875380Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.523877486Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.523880179Z             # find there is nothing left to do.
2024-12-30T09:35:15.523882337Z             if is_initialized():
2024-12-30T09:35:15.523917833Z                 return
2024-12-30T09:35:15.523920949Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.523923047Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.523925145Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.523927130Z             if _is_in_bad_fork():
2024-12-30T09:35:15.523968796Z                 raise RuntimeError(
2024-12-30T09:35:15.523972072Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.523974563Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.523977601Z                 )
2024-12-30T09:35:15.523981544Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.523983684Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.523986295Z             if _cudart is None:
2024-12-30T09:35:15.523988320Z                 raise AssertionError(
2024-12-30T09:35:15.523990819Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.524024755Z                 )
2024-12-30T09:35:15.524027716Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.524029888Z             # are found or any other error occurs
2024-12-30T09:35:15.524031836Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.524057878Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.524061888Z >           torch._C._cuda_init()
2024-12-30T09:35:15.524067137Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.524069813Z 
2024-12-30T09:35:15.524095816Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.524266882Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.524270035Z 
2024-12-30T09:35:15.524271908Z b = 8, n = 1025, d = 768, dtype = torch.float32
2024-12-30T09:35:15.524273949Z 
2024-12-30T09:35:15.524483147Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.524486431Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.524488632Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.524490658Z         torch.manual_seed(2024)
2024-12-30T09:35:15.524492611Z         atol = 5e-2
2024-12-30T09:35:15.524494589Z         rtol = 1e-2
2024-12-30T09:35:15.524496499Z         device = torch.device("cuda")
2024-12-30T09:35:15.524498467Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.524500538Z 
2024-12-30T09:35:15.524557019Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.524570183Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.524573143Z 
2024-12-30T09:35:15.524768270Z     def _lazy_init():
2024-12-30T09:35:15.524771990Z         global _initialized, _queued_calls
2024-12-30T09:35:15.524773968Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.524776088Z             return
2024-12-30T09:35:15.524777958Z         with _initialization_lock:
2024-12-30T09:35:15.524779958Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.524782043Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.524784039Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.524786058Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.524811891Z             # find there is nothing left to do.
2024-12-30T09:35:15.524822725Z             if is_initialized():
2024-12-30T09:35:15.524824887Z                 return
2024-12-30T09:35:15.524827062Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.524829249Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.524831297Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.524848198Z             if _is_in_bad_fork():
2024-12-30T09:35:15.524850686Z                 raise RuntimeError(
2024-12-30T09:35:15.524852670Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.524875160Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.524877644Z                 )
2024-12-30T09:35:15.524879605Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.524910783Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.524914294Z             if _cudart is None:
2024-12-30T09:35:15.524916315Z                 raise AssertionError(
2024-12-30T09:35:15.524918309Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.524920420Z                 )
2024-12-30T09:35:15.524940372Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.524943733Z             # are found or any other error occurs
2024-12-30T09:35:15.524945931Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.524948141Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.524978341Z >           torch._C._cuda_init()
2024-12-30T09:35:15.524981443Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.524984028Z 
2024-12-30T09:35:15.525026381Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.525163062Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype0-8-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.525166772Z 
2024-12-30T09:35:15.525168734Z b = 8, n = 1025, d = 1024, dtype = torch.float32
2024-12-30T09:35:15.525170793Z 
2024-12-30T09:35:15.525352498Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.525356222Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.525371588Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.525378302Z         torch.manual_seed(2024)
2024-12-30T09:35:15.525380422Z         atol = 5e-2
2024-12-30T09:35:15.525382476Z         rtol = 1e-2
2024-12-30T09:35:15.525385093Z         device = torch.device("cuda")
2024-12-30T09:35:15.525387207Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.525389274Z 
2024-12-30T09:35:15.525408104Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.525433643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.525437146Z 
2024-12-30T09:35:15.525623803Z     def _lazy_init():
2024-12-30T09:35:15.525627420Z         global _initialized, _queued_calls
2024-12-30T09:35:15.525629575Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.525631823Z             return
2024-12-30T09:35:15.525664446Z         with _initialization_lock:
2024-12-30T09:35:15.525668644Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.525670841Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.525672844Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.525674917Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.525703257Z             # find there is nothing left to do.
2024-12-30T09:35:15.525706149Z             if is_initialized():
2024-12-30T09:35:15.525708066Z                 return
2024-12-30T09:35:15.525710073Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.525788126Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.525790884Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.525792915Z             if _is_in_bad_fork():
2024-12-30T09:35:15.525794911Z                 raise RuntimeError(
2024-12-30T09:35:15.525796909Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.525798976Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.525801052Z                 )
2024-12-30T09:35:15.525802989Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.525804955Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.525806981Z             if _cudart is None:
2024-12-30T09:35:15.525808965Z                 raise AssertionError(
2024-12-30T09:35:15.525826850Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.525831567Z                 )
2024-12-30T09:35:15.525833566Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.525835810Z             # are found or any other error occurs
2024-12-30T09:35:15.525837798Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.525859475Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.525862753Z >           torch._C._cuda_init()
2024-12-30T09:35:15.525865194Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.525893970Z 
2024-12-30T09:35:15.525897043Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.526058419Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-1-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.526061793Z 
2024-12-30T09:35:15.526089930Z b = 1, n = 127, d = 768, dtype = torch.float16
2024-12-30T09:35:15.526093219Z 
2024-12-30T09:35:15.526274562Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.526278330Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.526280428Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.526282419Z         torch.manual_seed(2024)
2024-12-30T09:35:15.526284444Z         atol = 5e-2
2024-12-30T09:35:15.526286782Z         rtol = 1e-2
2024-12-30T09:35:15.526288797Z         device = torch.device("cuda")
2024-12-30T09:35:15.526299931Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.526303139Z 
2024-12-30T09:35:15.526351543Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.526354764Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.526363849Z 
2024-12-30T09:35:15.526559948Z     def _lazy_init():
2024-12-30T09:35:15.526563830Z         global _initialized, _queued_calls
2024-12-30T09:35:15.526565857Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.526568001Z             return
2024-12-30T09:35:15.526569914Z         with _initialization_lock:
2024-12-30T09:35:15.526576480Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.526578627Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.526585961Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.526588189Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.526590214Z             # find there is nothing left to do.
2024-12-30T09:35:15.526592245Z             if is_initialized():
2024-12-30T09:35:15.526655074Z                 return
2024-12-30T09:35:15.526659022Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.526662050Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.526664879Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.526668110Z             if _is_in_bad_fork():
2024-12-30T09:35:15.526671327Z                 raise RuntimeError(
2024-12-30T09:35:15.526674569Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.526678622Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.526682150Z                 )
2024-12-30T09:35:15.526685127Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.526688956Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.526693443Z             if _cudart is None:
2024-12-30T09:35:15.526732083Z                 raise AssertionError(
2024-12-30T09:35:15.526736353Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.526738524Z                 )
2024-12-30T09:35:15.526740442Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.526742481Z             # are found or any other error occurs
2024-12-30T09:35:15.526744438Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.526756980Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.526760208Z >           torch._C._cuda_init()
2024-12-30T09:35:15.526788793Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.526792198Z 
2024-12-30T09:35:15.526794069Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.526956716Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.526964807Z 
2024-12-30T09:35:15.526985851Z b = 1, n = 127, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.526989302Z 
2024-12-30T09:35:15.527159127Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.527162328Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.527164447Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.527166422Z         torch.manual_seed(2024)
2024-12-30T09:35:15.527168290Z         atol = 5e-2
2024-12-30T09:35:15.527177196Z         rtol = 1e-2
2024-12-30T09:35:15.527184178Z         device = torch.device("cuda")
2024-12-30T09:35:15.527186344Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.527189183Z 
2024-12-30T09:35:15.527234665Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.527238058Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.527240522Z 
2024-12-30T09:35:15.527445414Z     def _lazy_init():
2024-12-30T09:35:15.527449236Z         global _initialized, _queued_calls
2024-12-30T09:35:15.527451212Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.527453417Z             return
2024-12-30T09:35:15.527455412Z         with _initialization_lock:
2024-12-30T09:35:15.527457399Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.527459401Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.527461408Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.527482141Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.527485568Z             # find there is nothing left to do.
2024-12-30T09:35:15.527487550Z             if is_initialized():
2024-12-30T09:35:15.527489629Z                 return
2024-12-30T09:35:15.527510343Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.527513449Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.527515483Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.527533532Z             if _is_in_bad_fork():
2024-12-30T09:35:15.527536571Z                 raise RuntimeError(
2024-12-30T09:35:15.527538618Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.527558769Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.527561598Z                 )
2024-12-30T09:35:15.527584166Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.527587005Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.527589137Z             if _cudart is None:
2024-12-30T09:35:15.527617699Z                 raise AssertionError(
2024-12-30T09:35:15.527620195Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.527622330Z                 )
2024-12-30T09:35:15.527624191Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.527627047Z             # are found or any other error occurs
2024-12-30T09:35:15.527629120Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.527639789Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.527668501Z >           torch._C._cuda_init()
2024-12-30T09:35:15.527671499Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.527673998Z 
2024-12-30T09:35:15.527712011Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.527843464Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-1-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.527846654Z 
2024-12-30T09:35:15.527872489Z b = 1, n = 128, d = 768, dtype = torch.float16
2024-12-30T09:35:15.527874897Z 
2024-12-30T09:35:15.528054959Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.528057771Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.528059951Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.528061891Z         torch.manual_seed(2024)
2024-12-30T09:35:15.528063777Z         atol = 5e-2
2024-12-30T09:35:15.528065979Z         rtol = 1e-2
2024-12-30T09:35:15.528067906Z         device = torch.device("cuda")
2024-12-30T09:35:15.528096519Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.528100545Z 
2024-12-30T09:35:15.528102416Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.528139629Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.528147436Z 
2024-12-30T09:35:15.528331311Z     def _lazy_init():
2024-12-30T09:35:15.528334182Z         global _initialized, _queued_calls
2024-12-30T09:35:15.528336165Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.528338605Z             return
2024-12-30T09:35:15.528340622Z         with _initialization_lock:
2024-12-30T09:35:15.528342613Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.528345509Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.528347552Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.528371098Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.528375366Z             # find there is nothing left to do.
2024-12-30T09:35:15.528396204Z             if is_initialized():
2024-12-30T09:35:15.528399428Z                 return
2024-12-30T09:35:15.528401444Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.528403568Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.528405593Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.528407959Z             if _is_in_bad_fork():
2024-12-30T09:35:15.528444618Z                 raise RuntimeError(
2024-12-30T09:35:15.528447988Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.528450124Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.528452163Z                 )
2024-12-30T09:35:15.528454420Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.528463355Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.528466211Z             if _cudart is None:
2024-12-30T09:35:15.528468303Z                 raise AssertionError(
2024-12-30T09:35:15.528485269Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.528488734Z                 )
2024-12-30T09:35:15.528490672Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.528506810Z             # are found or any other error occurs
2024-12-30T09:35:15.528509905Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.528512231Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.528531331Z >           torch._C._cuda_init()
2024-12-30T09:35:15.528539866Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.528553623Z 
2024-12-30T09:35:15.528576282Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.528726659Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.528730764Z 
2024-12-30T09:35:15.528752953Z b = 1, n = 128, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.528756120Z 
2024-12-30T09:35:15.528916160Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.528919187Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.528921513Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.528923462Z         torch.manual_seed(2024)
2024-12-30T09:35:15.528948132Z         atol = 5e-2
2024-12-30T09:35:15.528950797Z         rtol = 1e-2
2024-12-30T09:35:15.528952760Z         device = torch.device("cuda")
2024-12-30T09:35:15.528954837Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.528983059Z 
2024-12-30T09:35:15.528986495Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.529019755Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.529023116Z 
2024-12-30T09:35:15.529185179Z     def _lazy_init():
2024-12-30T09:35:15.529188103Z         global _initialized, _queued_calls
2024-12-30T09:35:15.529190382Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.529192537Z             return
2024-12-30T09:35:15.529204370Z         with _initialization_lock:
2024-12-30T09:35:15.529207269Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.529210002Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.529227437Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.529230634Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.529232768Z             # find there is nothing left to do.
2024-12-30T09:35:15.529243354Z             if is_initialized():
2024-12-30T09:35:15.529246512Z                 return
2024-12-30T09:35:15.529260270Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.529267706Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.529279521Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.529283123Z             if _is_in_bad_fork():
2024-12-30T09:35:15.529310516Z                 raise RuntimeError(
2024-12-30T09:35:15.529314070Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.529316275Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.529318351Z                 )
2024-12-30T09:35:15.529330064Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.529333383Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.529339970Z             if _cudart is None:
2024-12-30T09:35:15.529354943Z                 raise AssertionError(
2024-12-30T09:35:15.529389658Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.529401595Z                 )
2024-12-30T09:35:15.529403990Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.529406188Z             # are found or any other error occurs
2024-12-30T09:35:15.529408222Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.529411248Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.529413436Z >           torch._C._cuda_init()
2024-12-30T09:35:15.529450570Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.529454580Z 
2024-12-30T09:35:15.529456706Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.529606813Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-1-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.529611556Z 
2024-12-30T09:35:15.529637461Z b = 1, n = 256, d = 768, dtype = torch.float16
2024-12-30T09:35:15.529640904Z 
2024-12-30T09:35:15.529811858Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.529815202Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.529817523Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.529819643Z         torch.manual_seed(2024)
2024-12-30T09:35:15.529821558Z         atol = 5e-2
2024-12-30T09:35:15.529845076Z         rtol = 1e-2
2024-12-30T09:35:15.529852201Z         device = torch.device("cuda")
2024-12-30T09:35:15.529854244Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.529856413Z 
2024-12-30T09:35:15.529886275Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.529889884Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.529892207Z 
2024-12-30T09:35:15.530077501Z     def _lazy_init():
2024-12-30T09:35:15.530080718Z         global _initialized, _queued_calls
2024-12-30T09:35:15.530082701Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.530084879Z             return
2024-12-30T09:35:15.530100187Z         with _initialization_lock:
2024-12-30T09:35:15.530103885Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.530106039Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.530108127Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.530134741Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.530137888Z             # find there is nothing left to do.
2024-12-30T09:35:15.530139855Z             if is_initialized():
2024-12-30T09:35:15.530146353Z                 return
2024-12-30T09:35:15.530149478Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.530151679Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.530178217Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.530181318Z             if _is_in_bad_fork():
2024-12-30T09:35:15.530183250Z                 raise RuntimeError(
2024-12-30T09:35:15.530185252Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.530190697Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.530192921Z                 )
2024-12-30T09:35:15.530224414Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.530227002Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.530229079Z             if _cudart is None:
2024-12-30T09:35:15.530230973Z                 raise AssertionError(
2024-12-30T09:35:15.530252265Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.530257094Z                 )
2024-12-30T09:35:15.530262631Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.530265215Z             # are found or any other error occurs
2024-12-30T09:35:15.530271838Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.530275319Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.530280621Z >           torch._C._cuda_init()
2024-12-30T09:35:15.530318139Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.530321062Z 
2024-12-30T09:35:15.530322878Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.530501285Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.530505159Z 
2024-12-30T09:35:15.530507034Z b = 1, n = 256, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.530509078Z 
2024-12-30T09:35:15.530703182Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.530708442Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.530710677Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.530712638Z         torch.manual_seed(2024)
2024-12-30T09:35:15.530714598Z         atol = 5e-2
2024-12-30T09:35:15.530716576Z         rtol = 1e-2
2024-12-30T09:35:15.530719207Z         device = torch.device("cuda")
2024-12-30T09:35:15.530721379Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.530724039Z 
2024-12-30T09:35:15.530761840Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.530765248Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.530767543Z 
2024-12-30T09:35:15.530979107Z     def _lazy_init():
2024-12-30T09:35:15.530981798Z         global _initialized, _queued_calls
2024-12-30T09:35:15.530983774Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.530985845Z             return
2024-12-30T09:35:15.530987819Z         with _initialization_lock:
2024-12-30T09:35:15.530989806Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.530991798Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.530997729Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.531017232Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.531020910Z             # find there is nothing left to do.
2024-12-30T09:35:15.531022964Z             if is_initialized():
2024-12-30T09:35:15.531032787Z                 return
2024-12-30T09:35:15.531036931Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.531040243Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.531057197Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.531060606Z             if _is_in_bad_fork():
2024-12-30T09:35:15.531062600Z                 raise RuntimeError(
2024-12-30T09:35:15.531081445Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.531085135Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.531087253Z                 )
2024-12-30T09:35:15.531102959Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.531106128Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.531108245Z             if _cudart is None:
2024-12-30T09:35:15.531123318Z                 raise AssertionError(
2024-12-30T09:35:15.531126299Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.531128952Z                 )
2024-12-30T09:35:15.531145827Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.531151618Z             # are found or any other error occurs
2024-12-30T09:35:15.531153632Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.531180507Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.531183739Z >           torch._C._cuda_init()
2024-12-30T09:35:15.531185797Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.531188208Z 
2024-12-30T09:35:15.531214940Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.531349429Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-1-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.531352843Z 
2024-12-30T09:35:15.531409724Z b = 1, n = 257, d = 768, dtype = torch.float16
2024-12-30T09:35:15.531417918Z 
2024-12-30T09:35:15.531578894Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.531583567Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.531585781Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.531587737Z         torch.manual_seed(2024)
2024-12-30T09:35:15.531589637Z         atol = 5e-2
2024-12-30T09:35:15.531591594Z         rtol = 1e-2
2024-12-30T09:35:15.531593468Z         device = torch.device("cuda")
2024-12-30T09:35:15.531600800Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.531603030Z 
2024-12-30T09:35:15.531630943Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.531652028Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.531655021Z 
2024-12-30T09:35:15.531848743Z     def _lazy_init():
2024-12-30T09:35:15.531852820Z         global _initialized, _queued_calls
2024-12-30T09:35:15.531854909Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.531857232Z             return
2024-12-30T09:35:15.531859115Z         with _initialization_lock:
2024-12-30T09:35:15.531861044Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.531880200Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.531883489Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.531885509Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.531919845Z             # find there is nothing left to do.
2024-12-30T09:35:15.531924567Z             if is_initialized():
2024-12-30T09:35:15.531926610Z                 return
2024-12-30T09:35:15.531939658Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.531942397Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.531944477Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.531946548Z             if _is_in_bad_fork():
2024-12-30T09:35:15.531959792Z                 raise RuntimeError(
2024-12-30T09:35:15.531968346Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.531970726Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.531988214Z                 )
2024-12-30T09:35:15.531992098Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.531998593Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.532000784Z             if _cudart is None:
2024-12-30T09:35:15.532006349Z                 raise AssertionError(
2024-12-30T09:35:15.532008454Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.532039066Z                 )
2024-12-30T09:35:15.532041768Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.532043923Z             # are found or any other error occurs
2024-12-30T09:35:15.532079692Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.532089820Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.532092129Z >           torch._C._cuda_init()
2024-12-30T09:35:15.532114491Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.532118593Z 
2024-12-30T09:35:15.532120633Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.532279088Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.532282972Z 
2024-12-30T09:35:15.532284901Z b = 1, n = 257, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.532287067Z 
2024-12-30T09:35:15.532484089Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.532488586Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.532490877Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.532492830Z         torch.manual_seed(2024)
2024-12-30T09:35:15.532494776Z         atol = 5e-2
2024-12-30T09:35:15.532496758Z         rtol = 1e-2
2024-12-30T09:35:15.532498693Z         device = torch.device("cuda")
2024-12-30T09:35:15.532500684Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.532502778Z 
2024-12-30T09:35:15.532540093Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.532543657Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.532555243Z 
2024-12-30T09:35:15.532756058Z     def _lazy_init():
2024-12-30T09:35:15.532761426Z         global _initialized, _queued_calls
2024-12-30T09:35:15.532770662Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.532774106Z             return
2024-12-30T09:35:15.532777627Z         with _initialization_lock:
2024-12-30T09:35:15.532809789Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.532814048Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.532816170Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.532818185Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.532820199Z             # find there is nothing left to do.
2024-12-30T09:35:15.532822537Z             if is_initialized():
2024-12-30T09:35:15.532824457Z                 return
2024-12-30T09:35:15.532826482Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.532828519Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.532831317Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.532833418Z             if _is_in_bad_fork():
2024-12-30T09:35:15.532835354Z                 raise RuntimeError(
2024-12-30T09:35:15.532894057Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.532896915Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.532898967Z                 )
2024-12-30T09:35:15.532900885Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.532902993Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.532905098Z             if _cudart is None:
2024-12-30T09:35:15.532907024Z                 raise AssertionError(
2024-12-30T09:35:15.532909040Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.532911139Z                 )
2024-12-30T09:35:15.532950703Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.532959418Z             # are found or any other error occurs
2024-12-30T09:35:15.532961547Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.532963695Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.532965858Z >           torch._C._cuda_init()
2024-12-30T09:35:15.532968954Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.532971389Z 
2024-12-30T09:35:15.532984744Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.533124463Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.533128882Z 
2024-12-30T09:35:15.533161709Z b = 1, n = 1024, d = 768, dtype = torch.float16
2024-12-30T09:35:15.533166166Z 
2024-12-30T09:35:15.533326214Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.533329914Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.533332133Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.533334173Z         torch.manual_seed(2024)
2024-12-30T09:35:15.533342218Z         atol = 5e-2
2024-12-30T09:35:15.533345501Z         rtol = 1e-2
2024-12-30T09:35:15.533347425Z         device = torch.device("cuda")
2024-12-30T09:35:15.533401274Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.533404373Z 
2024-12-30T09:35:15.533406284Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.533429386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.533434667Z 
2024-12-30T09:35:15.533631357Z     def _lazy_init():
2024-12-30T09:35:15.533634890Z         global _initialized, _queued_calls
2024-12-30T09:35:15.533636937Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.533639140Z             return
2024-12-30T09:35:15.533641052Z         with _initialization_lock:
2024-12-30T09:35:15.533643884Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.533646084Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.533648063Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.533650056Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.533689360Z             # find there is nothing left to do.
2024-12-30T09:35:15.533692098Z             if is_initialized():
2024-12-30T09:35:15.533694002Z                 return
2024-12-30T09:35:15.533696054Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.533698159Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.533702019Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.533708394Z             if _is_in_bad_fork():
2024-12-30T09:35:15.533720004Z                 raise RuntimeError(
2024-12-30T09:35:15.533745088Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.533749208Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.533751353Z                 )
2024-12-30T09:35:15.533753307Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.533785344Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.533788272Z             if _cudart is None:
2024-12-30T09:35:15.533790276Z                 raise AssertionError(
2024-12-30T09:35:15.533792278Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.533794859Z                 )
2024-12-30T09:35:15.533811149Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.533818172Z             # are found or any other error occurs
2024-12-30T09:35:15.533820319Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.533831579Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.533857862Z >           torch._C._cuda_init()
2024-12-30T09:35:15.533860505Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.533862961Z 
2024-12-30T09:35:15.533877913Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.534027236Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.534030529Z 
2024-12-30T09:35:15.534052816Z b = 1, n = 1024, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.534055369Z 
2024-12-30T09:35:15.534230419Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.534234272Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.534236369Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.534238358Z         torch.manual_seed(2024)
2024-12-30T09:35:15.534240295Z         atol = 5e-2
2024-12-30T09:35:15.534242274Z         rtol = 1e-2
2024-12-30T09:35:15.534255812Z         device = torch.device("cuda")
2024-12-30T09:35:15.534259894Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.534266609Z 
2024-12-30T09:35:15.534303575Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.534306692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.534309090Z 
2024-12-30T09:35:15.534503323Z     def _lazy_init():
2024-12-30T09:35:15.534507600Z         global _initialized, _queued_calls
2024-12-30T09:35:15.534509637Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.534511936Z             return
2024-12-30T09:35:15.534513909Z         with _initialization_lock:
2024-12-30T09:35:15.534543371Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.534560452Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.534563493Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.534565561Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.534567633Z             # find there is nothing left to do.
2024-12-30T09:35:15.534570270Z             if is_initialized():
2024-12-30T09:35:15.534583778Z                 return
2024-12-30T09:35:15.534587155Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.534604206Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.534607466Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.534609467Z             if _is_in_bad_fork():
2024-12-30T09:35:15.534636633Z                 raise RuntimeError(
2024-12-30T09:35:15.534639757Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.534641955Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.534644133Z                 )
2024-12-30T09:35:15.534662631Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.534665380Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.534667505Z             if _cudart is None:
2024-12-30T09:35:15.534680288Z                 raise AssertionError(
2024-12-30T09:35:15.534686749Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.534692737Z                 )
2024-12-30T09:35:15.534695799Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.534732169Z             # are found or any other error occurs
2024-12-30T09:35:15.534739655Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.534741741Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.534755716Z >           torch._C._cuda_init()
2024-12-30T09:35:15.534759208Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.534789935Z 
2024-12-30T09:35:15.534793190Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.534938071Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.534941331Z 
2024-12-30T09:35:15.534971844Z b = 1, n = 1025, d = 768, dtype = torch.float16
2024-12-30T09:35:15.534974854Z 
2024-12-30T09:35:15.535144209Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.535146979Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.535149095Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.535151140Z         torch.manual_seed(2024)
2024-12-30T09:35:15.535153070Z         atol = 5e-2
2024-12-30T09:35:15.535178147Z         rtol = 1e-2
2024-12-30T09:35:15.535181722Z         device = torch.device("cuda")
2024-12-30T09:35:15.535184716Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.535186997Z 
2024-12-30T09:35:15.535224202Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.535227421Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.535229764Z 
2024-12-30T09:35:15.535437142Z     def _lazy_init():
2024-12-30T09:35:15.535452321Z         global _initialized, _queued_calls
2024-12-30T09:35:15.535455115Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.535457491Z             return
2024-12-30T09:35:15.535459587Z         with _initialization_lock:
2024-12-30T09:35:15.535461615Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.535463718Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.535467093Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.535469194Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.535478736Z             # find there is nothing left to do.
2024-12-30T09:35:15.535481630Z             if is_initialized():
2024-12-30T09:35:15.535483866Z                 return
2024-12-30T09:35:15.535520159Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.535526140Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.535529041Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.535531917Z             if _is_in_bad_fork():
2024-12-30T09:35:15.535558905Z                 raise RuntimeError(
2024-12-30T09:35:15.535562236Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.535564346Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.535566433Z                 )
2024-12-30T09:35:15.535581549Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.535585852Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.535587990Z             if _cudart is None:
2024-12-30T09:35:15.535611458Z                 raise AssertionError(
2024-12-30T09:35:15.535614532Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.535616705Z                 )
2024-12-30T09:35:15.535618648Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.535639096Z             # are found or any other error occurs
2024-12-30T09:35:15.535641763Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.535643864Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.535645912Z >           torch._C._cuda_init()
2024-12-30T09:35:15.535676465Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.535680278Z 
2024-12-30T09:35:15.535682165Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.535851634Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-1-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.535856393Z 
2024-12-30T09:35:15.535892103Z b = 1, n = 1025, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.535895744Z 
2024-12-30T09:35:15.536081916Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.536085155Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.536096791Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.536098934Z         torch.manual_seed(2024)
2024-12-30T09:35:15.536100831Z         atol = 5e-2
2024-12-30T09:35:15.536102761Z         rtol = 1e-2
2024-12-30T09:35:15.536104628Z         device = torch.device("cuda")
2024-12-30T09:35:15.536107184Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.536109257Z 
2024-12-30T09:35:15.536170786Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.536177504Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.536179962Z 
2024-12-30T09:35:15.536328440Z     def _lazy_init():
2024-12-30T09:35:15.536332037Z         global _initialized, _queued_calls
2024-12-30T09:35:15.536335935Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.536338211Z             return
2024-12-30T09:35:15.536340183Z         with _initialization_lock:
2024-12-30T09:35:15.536342163Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.536405054Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.536408993Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.536411055Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.536413077Z             # find there is nothing left to do.
2024-12-30T09:35:15.536415140Z             if is_initialized():
2024-12-30T09:35:15.536417104Z                 return
2024-12-30T09:35:15.536419108Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.536421127Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.536423394Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.536450318Z             if _is_in_bad_fork():
2024-12-30T09:35:15.536453550Z                 raise RuntimeError(
2024-12-30T09:35:15.536455609Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.536457806Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.536460005Z                 )
2024-12-30T09:35:15.536461896Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.536464478Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.536487586Z             if _cudart is None:
2024-12-30T09:35:15.536490932Z                 raise AssertionError(
2024-12-30T09:35:15.536492937Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.536495061Z                 )
2024-12-30T09:35:15.536497697Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.536499691Z             # are found or any other error occurs
2024-12-30T09:35:15.536521686Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.536525075Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.536527214Z >           torch._C._cuda_init()
2024-12-30T09:35:15.536576282Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.536580531Z 
2024-12-30T09:35:15.536582439Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.536752225Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-2-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.536756968Z 
2024-12-30T09:35:15.536766053Z b = 2, n = 127, d = 768, dtype = torch.float16
2024-12-30T09:35:15.536769016Z 
2024-12-30T09:35:15.536938181Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.536941336Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.536943427Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.536945397Z         torch.manual_seed(2024)
2024-12-30T09:35:15.536947381Z         atol = 5e-2
2024-12-30T09:35:15.536985513Z         rtol = 1e-2
2024-12-30T09:35:15.536990525Z         device = torch.device("cuda")
2024-12-30T09:35:15.536992637Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.536994855Z 
2024-12-30T09:35:15.537012526Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.537015482Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.537017839Z 
2024-12-30T09:35:15.537215488Z     def _lazy_init():
2024-12-30T09:35:15.537219095Z         global _initialized, _queued_calls
2024-12-30T09:35:15.537221086Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.537223375Z             return
2024-12-30T09:35:15.537230195Z         with _initialization_lock:
2024-12-30T09:35:15.537262256Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.537266216Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.537268358Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.537270413Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.537272416Z             # find there is nothing left to do.
2024-12-30T09:35:15.537274445Z             if is_initialized():
2024-12-30T09:35:15.537276388Z                 return
2024-12-30T09:35:15.537292380Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.537295806Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.537297951Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.537300563Z             if _is_in_bad_fork():
2024-12-30T09:35:15.537302504Z                 raise RuntimeError(
2024-12-30T09:35:15.537310905Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.537346763Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.537356283Z                 )
2024-12-30T09:35:15.537371680Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.537376361Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.537379690Z             if _cudart is None:
2024-12-30T09:35:15.537382754Z                 raise AssertionError(
2024-12-30T09:35:15.537386229Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.537389528Z                 )
2024-12-30T09:35:15.537460042Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.537472028Z             # are found or any other error occurs
2024-12-30T09:35:15.537474498Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.537487824Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.537490096Z >           torch._C._cuda_init()
2024-12-30T09:35:15.537492337Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.537494835Z 
2024-12-30T09:35:15.537496723Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.537663370Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.537679072Z 
2024-12-30T09:35:15.537680983Z b = 2, n = 127, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.537682926Z 
2024-12-30T09:35:15.537870808Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.537875510Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.537877909Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.537880054Z         torch.manual_seed(2024)
2024-12-30T09:35:15.537882016Z         atol = 5e-2
2024-12-30T09:35:15.537883925Z         rtol = 1e-2
2024-12-30T09:35:15.537909004Z         device = torch.device("cuda")
2024-12-30T09:35:15.537912147Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.537914385Z 
2024-12-30T09:35:15.537917079Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.537958329Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.537962612Z 
2024-12-30T09:35:15.538149230Z     def _lazy_init():
2024-12-30T09:35:15.538152848Z         global _initialized, _queued_calls
2024-12-30T09:35:15.538154888Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.538157233Z             return
2024-12-30T09:35:15.538159225Z         with _initialization_lock:
2024-12-30T09:35:15.538161228Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.538163253Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.538165252Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.538167277Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.538193514Z             # find there is nothing left to do.
2024-12-30T09:35:15.538196876Z             if is_initialized():
2024-12-30T09:35:15.538198915Z                 return
2024-12-30T09:35:15.538201132Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.538203303Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.538228932Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.538232420Z             if _is_in_bad_fork():
2024-12-30T09:35:15.538234375Z                 raise RuntimeError(
2024-12-30T09:35:15.538244272Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.538247227Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.538249369Z                 )
2024-12-30T09:35:15.538264180Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.538267975Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.538270132Z             if _cudart is None:
2024-12-30T09:35:15.538293123Z                 raise AssertionError(
2024-12-30T09:35:15.538296365Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.538298569Z                 )
2024-12-30T09:35:15.538300516Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.538329345Z             # are found or any other error occurs
2024-12-30T09:35:15.538332536Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.538334711Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.538336809Z >           torch._C._cuda_init()
2024-12-30T09:35:15.538380505Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.538384146Z 
2024-12-30T09:35:15.538386105Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.538545461Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-2-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.538560381Z 
2024-12-30T09:35:15.538587178Z b = 2, n = 128, d = 768, dtype = torch.float16
2024-12-30T09:35:15.538590516Z 
2024-12-30T09:35:15.538753467Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.538757424Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.538759624Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.538761599Z         torch.manual_seed(2024)
2024-12-30T09:35:15.538764123Z         atol = 5e-2
2024-12-30T09:35:15.538766215Z         rtol = 1e-2
2024-12-30T09:35:15.538768671Z         device = torch.device("cuda")
2024-12-30T09:35:15.538803590Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.538807863Z 
2024-12-30T09:35:15.538809924Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.538856394Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.538865846Z 
2024-12-30T09:35:15.539022203Z     def _lazy_init():
2024-12-30T09:35:15.539025782Z         global _initialized, _queued_calls
2024-12-30T09:35:15.539027838Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.539030046Z             return
2024-12-30T09:35:15.539033452Z         with _initialization_lock:
2024-12-30T09:35:15.539035557Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.539037652Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.539049632Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.539052952Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.539068748Z             # find there is nothing left to do.
2024-12-30T09:35:15.539091393Z             if is_initialized():
2024-12-30T09:35:15.539094242Z                 return
2024-12-30T09:35:15.539096149Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.539098147Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.539113973Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.539116379Z             if _is_in_bad_fork():
2024-12-30T09:35:15.539130165Z                 raise RuntimeError(
2024-12-30T09:35:15.539132562Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.539139306Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.539141474Z                 )
2024-12-30T09:35:15.539166259Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.539169329Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.539171470Z             if _cudart is None:
2024-12-30T09:35:15.539182745Z                 raise AssertionError(
2024-12-30T09:35:15.539186827Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.539213712Z                 )
2024-12-30T09:35:15.539220186Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.539222430Z             # are found or any other error occurs
2024-12-30T09:35:15.539224419Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.539227372Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.539233940Z >           torch._C._cuda_init()
2024-12-30T09:35:15.539263280Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.539267008Z 
2024-12-30T09:35:15.539285419Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.539437784Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.539441446Z 
2024-12-30T09:35:15.539467713Z b = 2, n = 128, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.539471119Z 
2024-12-30T09:35:15.539644456Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.539649828Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.539653148Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.539656253Z         torch.manual_seed(2024)
2024-12-30T09:35:15.539659147Z         atol = 5e-2
2024-12-30T09:35:15.539675763Z         rtol = 1e-2
2024-12-30T09:35:15.539680023Z         device = torch.device("cuda")
2024-12-30T09:35:15.539683091Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.539686811Z 
2024-12-30T09:35:15.539736747Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.539740409Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.539742639Z 
2024-12-30T09:35:15.539921689Z     def _lazy_init():
2024-12-30T09:35:15.539925524Z         global _initialized, _queued_calls
2024-12-30T09:35:15.539927691Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.539930160Z             return
2024-12-30T09:35:15.539932118Z         with _initialization_lock:
2024-12-30T09:35:15.539934142Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.539952679Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.539956040Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.539958187Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.539960270Z             # find there is nothing left to do.
2024-12-30T09:35:15.539987154Z             if is_initialized():
2024-12-30T09:35:15.539990185Z                 return
2024-12-30T09:35:15.539996802Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.539998963Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.540002022Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.540004023Z             if _is_in_bad_fork():
2024-12-30T09:35:15.540006654Z                 raise RuntimeError(
2024-12-30T09:35:15.540044875Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.540048546Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.540050654Z                 )
2024-12-30T09:35:15.540052546Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.540073966Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.540077731Z             if _cudart is None:
2024-12-30T09:35:15.540079858Z                 raise AssertionError(
2024-12-30T09:35:15.540093842Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.540107620Z                 )
2024-12-30T09:35:15.540131487Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.540134801Z             # are found or any other error occurs
2024-12-30T09:35:15.540138891Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.540141139Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.540143255Z >           torch._C._cuda_init()
2024-12-30T09:35:15.540179599Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.540183633Z 
2024-12-30T09:35:15.540185490Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.540326718Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-2-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.540330386Z 
2024-12-30T09:35:15.540355535Z b = 2, n = 256, d = 768, dtype = torch.float16
2024-12-30T09:35:15.540364893Z 
2024-12-30T09:35:15.540538626Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.540542077Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.540544280Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.540546302Z         torch.manual_seed(2024)
2024-12-30T09:35:15.540558989Z         atol = 5e-2
2024-12-30T09:35:15.540561595Z         rtol = 1e-2
2024-12-30T09:35:15.540563591Z         device = torch.device("cuda")
2024-12-30T09:35:15.540594853Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.540597457Z 
2024-12-30T09:35:15.540599352Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.540623131Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.540628233Z 
2024-12-30T09:35:15.540818333Z     def _lazy_init():
2024-12-30T09:35:15.540821631Z         global _initialized, _queued_calls
2024-12-30T09:35:15.540823636Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.540825861Z             return
2024-12-30T09:35:15.540827774Z         with _initialization_lock:
2024-12-30T09:35:15.540829740Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.540832372Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.540856207Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.540859660Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.540861792Z             # find there is nothing left to do.
2024-12-30T09:35:15.540875616Z             if is_initialized():
2024-12-30T09:35:15.540883867Z                 return
2024-12-30T09:35:15.540893940Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.540896378Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.540898489Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.540900509Z             if _is_in_bad_fork():
2024-12-30T09:35:15.540925321Z                 raise RuntimeError(
2024-12-30T09:35:15.540928247Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.540930445Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.540946682Z                 )
2024-12-30T09:35:15.540950422Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.540964243Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.540966872Z             if _cudart is None:
2024-12-30T09:35:15.540968837Z                 raise AssertionError(
2024-12-30T09:35:15.540995956Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.541002591Z                 )
2024-12-30T09:35:15.541004527Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.541006580Z             # are found or any other error occurs
2024-12-30T09:35:15.541017247Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.541019522Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.541021551Z >           torch._C._cuda_init()
2024-12-30T09:35:15.541043933Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.541046670Z 
2024-12-30T09:35:15.541063294Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.541223690Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.541226704Z 
2024-12-30T09:35:15.541228565Z b = 2, n = 256, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.541230490Z 
2024-12-30T09:35:15.541426968Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.541432848Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.541435255Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.541437241Z         torch.manual_seed(2024)
2024-12-30T09:35:15.541439295Z         atol = 5e-2
2024-12-30T09:35:15.541441240Z         rtol = 1e-2
2024-12-30T09:35:15.541455258Z         device = torch.device("cuda")
2024-12-30T09:35:15.541458797Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.541462070Z 
2024-12-30T09:35:15.541466565Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.541494314Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.541498688Z 
2024-12-30T09:35:15.541703358Z     def _lazy_init():
2024-12-30T09:35:15.541707318Z         global _initialized, _queued_calls
2024-12-30T09:35:15.541709331Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.541711452Z             return
2024-12-30T09:35:15.541713449Z         with _initialization_lock:
2024-12-30T09:35:15.541715364Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.541721923Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.541724973Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.541727017Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.541729039Z             # find there is nothing left to do.
2024-12-30T09:35:15.541755411Z             if is_initialized():
2024-12-30T09:35:15.541757980Z                 return
2024-12-30T09:35:15.541759942Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.541770307Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.541773738Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.541775791Z             if _is_in_bad_fork():
2024-12-30T09:35:15.541851208Z                 raise RuntimeError(
2024-12-30T09:35:15.541854664Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.541856857Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.541858934Z                 )
2024-12-30T09:35:15.541860876Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.541862859Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.541864888Z             if _cudart is None:
2024-12-30T09:35:15.541866831Z                 raise AssertionError(
2024-12-30T09:35:15.541868775Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.541870844Z                 )
2024-12-30T09:35:15.541879164Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.541881333Z             # are found or any other error occurs
2024-12-30T09:35:15.541887928Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.541890976Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.541894290Z >           torch._C._cuda_init()
2024-12-30T09:35:15.541934744Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.541938517Z 
2024-12-30T09:35:15.541940355Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.542094245Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-2-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.542098491Z 
2024-12-30T09:35:15.542118434Z b = 2, n = 257, d = 768, dtype = torch.float16
2024-12-30T09:35:15.542121726Z 
2024-12-30T09:35:15.542289343Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.542293660Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.542295841Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.542297833Z         torch.manual_seed(2024)
2024-12-30T09:35:15.542299733Z         atol = 5e-2
2024-12-30T09:35:15.542301640Z         rtol = 1e-2
2024-12-30T09:35:15.542318156Z         device = torch.device("cuda")
2024-12-30T09:35:15.542320594Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.542323672Z 
2024-12-30T09:35:15.542373206Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.542376936Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.542379194Z 
2024-12-30T09:35:15.542572469Z     def _lazy_init():
2024-12-30T09:35:15.542576074Z         global _initialized, _queued_calls
2024-12-30T09:35:15.542578066Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.542580211Z             return
2024-12-30T09:35:15.542582181Z         with _initialization_lock:
2024-12-30T09:35:15.542584124Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.542602599Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.542605135Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.542607132Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.542618173Z             # find there is nothing left to do.
2024-12-30T09:35:15.542642545Z             if is_initialized():
2024-12-30T09:35:15.542645942Z                 return
2024-12-30T09:35:15.542647925Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.542650029Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.542679737Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.542682621Z             if _is_in_bad_fork():
2024-12-30T09:35:15.542684542Z                 raise RuntimeError(
2024-12-30T09:35:15.542686679Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.542698193Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.542708423Z                 )
2024-12-30T09:35:15.542711598Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.542713754Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.542733289Z             if _cudart is None:
2024-12-30T09:35:15.542740383Z                 raise AssertionError(
2024-12-30T09:35:15.542742414Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.542745690Z                 )
2024-12-30T09:35:15.542747700Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.542766924Z             # are found or any other error occurs
2024-12-30T09:35:15.542769989Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.542772009Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.542803942Z >           torch._C._cuda_init()
2024-12-30T09:35:15.542807482Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.542809896Z 
2024-12-30T09:35:15.542835550Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.542975800Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.542979635Z 
2024-12-30T09:35:15.543005665Z b = 2, n = 257, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.543008913Z 
2024-12-30T09:35:15.543185793Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.543189102Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.543191211Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.543193277Z         torch.manual_seed(2024)
2024-12-30T09:35:15.543210582Z         atol = 5e-2
2024-12-30T09:35:15.543213737Z         rtol = 1e-2
2024-12-30T09:35:15.543215670Z         device = torch.device("cuda")
2024-12-30T09:35:15.543217706Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.543219928Z 
2024-12-30T09:35:15.543254788Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.543258040Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.543260511Z 
2024-12-30T09:35:15.543470684Z     def _lazy_init():
2024-12-30T09:35:15.543474193Z         global _initialized, _queued_calls
2024-12-30T09:35:15.543476164Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.543478460Z             return
2024-12-30T09:35:15.543480422Z         with _initialization_lock:
2024-12-30T09:35:15.543482337Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.543484313Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.543487005Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.543500772Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.543519108Z             # find there is nothing left to do.
2024-12-30T09:35:15.543525697Z             if is_initialized():
2024-12-30T09:35:15.543527745Z                 return
2024-12-30T09:35:15.543530304Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.543558804Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.543562657Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.543564634Z             if _is_in_bad_fork():
2024-12-30T09:35:15.543667200Z                 raise RuntimeError(
2024-12-30T09:35:15.543669801Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.543672022Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.543674012Z                 )
2024-12-30T09:35:15.543675876Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.543677839Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.543679958Z             if _cudart is None:
2024-12-30T09:35:15.543681876Z                 raise AssertionError(
2024-12-30T09:35:15.543683831Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.543685875Z                 )
2024-12-30T09:35:15.543687816Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.543689903Z             # are found or any other error occurs
2024-12-30T09:35:15.543691829Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.543694448Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.543696570Z >           torch._C._cuda_init()
2024-12-30T09:35:15.543698537Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.543703906Z 
2024-12-30T09:35:15.543715798Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.543861956Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.543866210Z 
2024-12-30T09:35:15.543904465Z b = 2, n = 1024, d = 768, dtype = torch.float16
2024-12-30T09:35:15.543911847Z 
2024-12-30T09:35:15.544063614Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.544066944Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.544069086Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.544071534Z         torch.manual_seed(2024)
2024-12-30T09:35:15.544073443Z         atol = 5e-2
2024-12-30T09:35:15.544075411Z         rtol = 1e-2
2024-12-30T09:35:15.544077807Z         device = torch.device("cuda")
2024-12-30T09:35:15.544098539Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.544101623Z 
2024-12-30T09:35:15.544136821Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.544140927Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.544143314Z 
2024-12-30T09:35:15.544347361Z     def _lazy_init():
2024-12-30T09:35:15.544351265Z         global _initialized, _queued_calls
2024-12-30T09:35:15.544354412Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.544364633Z             return
2024-12-30T09:35:15.544394756Z         with _initialization_lock:
2024-12-30T09:35:15.544399123Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.544402728Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.544405890Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.544409130Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.544412759Z             # find there is nothing left to do.
2024-12-30T09:35:15.544424810Z             if is_initialized():
2024-12-30T09:35:15.544431619Z                 return
2024-12-30T09:35:15.544438391Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.544440649Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.544447957Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.544465382Z             if _is_in_bad_fork():
2024-12-30T09:35:15.544470227Z                 raise RuntimeError(
2024-12-30T09:35:15.544472980Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.544476674Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.544506130Z                 )
2024-12-30T09:35:15.544510304Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.544513506Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.544516413Z             if _cudart is None:
2024-12-30T09:35:15.544527028Z                 raise AssertionError(
2024-12-30T09:35:15.544532216Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.544534553Z                 )
2024-12-30T09:35:15.544536445Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.544568344Z             # are found or any other error occurs
2024-12-30T09:35:15.544574600Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.544577935Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.544581201Z >           torch._C._cuda_init()
2024-12-30T09:35:15.544610432Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.544613889Z 
2024-12-30T09:35:15.544615872Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.544769447Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.544772869Z 
2024-12-30T09:35:15.544804470Z b = 2, n = 1024, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.544807987Z 
2024-12-30T09:35:15.544991073Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.544996629Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.545000266Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.545003164Z         torch.manual_seed(2024)
2024-12-30T09:35:15.545006045Z         atol = 5e-2
2024-12-30T09:35:15.545009338Z         rtol = 1e-2
2024-12-30T09:35:15.545012399Z         device = torch.device("cuda")
2024-12-30T09:35:15.545023398Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.545025581Z 
2024-12-30T09:35:15.545054908Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.545058700Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.545061142Z 
2024-12-30T09:35:15.545254959Z     def _lazy_init():
2024-12-30T09:35:15.545257598Z         global _initialized, _queued_calls
2024-12-30T09:35:15.545259533Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.545261644Z             return
2024-12-30T09:35:15.545263705Z         with _initialization_lock:
2024-12-30T09:35:15.545265651Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.545267647Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.545269662Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.545307130Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.545311110Z             # find there is nothing left to do.
2024-12-30T09:35:15.545314656Z             if is_initialized():
2024-12-30T09:35:15.545317691Z                 return
2024-12-30T09:35:15.545320690Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.545323803Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.545328212Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.545331584Z             if _is_in_bad_fork():
2024-12-30T09:35:15.545335363Z                 raise RuntimeError(
2024-12-30T09:35:15.545352814Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.545355542Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.545363303Z                 )
2024-12-30T09:35:15.545366953Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.545400818Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.545404881Z             if _cudart is None:
2024-12-30T09:35:15.545406876Z                 raise AssertionError(
2024-12-30T09:35:15.545408823Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.545411516Z                 )
2024-12-30T09:35:15.545413566Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.545438377Z             # are found or any other error occurs
2024-12-30T09:35:15.545443670Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.545446613Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.545450548Z >           torch._C._cuda_init()
2024-12-30T09:35:15.545472932Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.545476917Z 
2024-12-30T09:35:15.545526567Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.545648633Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.545652626Z 
2024-12-30T09:35:15.545684524Z b = 2, n = 1025, d = 768, dtype = torch.float16
2024-12-30T09:35:15.545687811Z 
2024-12-30T09:35:15.545863455Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.545869055Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.545872379Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.545875356Z         torch.manual_seed(2024)
2024-12-30T09:35:15.545878347Z         atol = 5e-2
2024-12-30T09:35:15.545881456Z         rtol = 1e-2
2024-12-30T09:35:15.545884142Z         device = torch.device("cuda")
2024-12-30T09:35:15.545886213Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.545888338Z 
2024-12-30T09:35:15.545927915Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.545931251Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.545958173Z 
2024-12-30T09:35:15.546136892Z     def _lazy_init():
2024-12-30T09:35:15.546139608Z         global _initialized, _queued_calls
2024-12-30T09:35:15.546141707Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.546160894Z             return
2024-12-30T09:35:15.546164713Z         with _initialization_lock:
2024-12-30T09:35:15.546166714Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.546192322Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.546195381Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.546203021Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.546205066Z             # find there is nothing left to do.
2024-12-30T09:35:15.546239635Z             if is_initialized():
2024-12-30T09:35:15.546241993Z                 return
2024-12-30T09:35:15.546243961Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.546245970Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.546248418Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.546250493Z             if _is_in_bad_fork():
2024-12-30T09:35:15.546260356Z                 raise RuntimeError(
2024-12-30T09:35:15.546271990Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.546277022Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.546298784Z                 )
2024-12-30T09:35:15.546302938Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.546305147Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.546316919Z             if _cudart is None:
2024-12-30T09:35:15.546320455Z                 raise AssertionError(
2024-12-30T09:35:15.546322457Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.546324773Z                 )
2024-12-30T09:35:15.546327432Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.546344884Z             # are found or any other error occurs
2024-12-30T09:35:15.546347934Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.546350142Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.546355516Z >           torch._C._cuda_init()
2024-12-30T09:35:15.546395672Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.546399461Z 
2024-12-30T09:35:15.546418206Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.546572501Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-2-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.546576656Z 
2024-12-30T09:35:15.546607422Z b = 2, n = 1025, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.546610287Z 
2024-12-30T09:35:15.546790556Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.546800666Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.546802893Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.546804880Z         torch.manual_seed(2024)
2024-12-30T09:35:15.546806798Z         atol = 5e-2
2024-12-30T09:35:15.546808880Z         rtol = 1e-2
2024-12-30T09:35:15.546810913Z         device = torch.device("cuda")
2024-12-30T09:35:15.546813822Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.546815963Z 
2024-12-30T09:35:15.546856904Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.546860848Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.546863438Z 
2024-12-30T09:35:15.547031809Z     def _lazy_init():
2024-12-30T09:35:15.547035724Z         global _initialized, _queued_calls
2024-12-30T09:35:15.547037730Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.547039965Z             return
2024-12-30T09:35:15.547041841Z         with _initialization_lock:
2024-12-30T09:35:15.547055406Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.547059529Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.547083028Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.547085610Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.547087659Z             # find there is nothing left to do.
2024-12-30T09:35:15.547089704Z             if is_initialized():
2024-12-30T09:35:15.547105912Z                 return
2024-12-30T09:35:15.547108983Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.547111188Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.547134469Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.547138702Z             if _is_in_bad_fork():
2024-12-30T09:35:15.547140928Z                 raise RuntimeError(
2024-12-30T09:35:15.547142950Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.547145660Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.547160245Z                 )
2024-12-30T09:35:15.547163095Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.547187357Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.547190854Z             if _cudart is None:
2024-12-30T09:35:15.547192876Z                 raise AssertionError(
2024-12-30T09:35:15.547195366Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.547197868Z                 )
2024-12-30T09:35:15.547201885Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.547237727Z             # are found or any other error occurs
2024-12-30T09:35:15.547240732Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.547242786Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.547244923Z >           torch._C._cuda_init()
2024-12-30T09:35:15.547265581Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.547269141Z 
2024-12-30T09:35:15.547307125Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.547472938Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-4-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.547476672Z 
2024-12-30T09:35:15.547504208Z b = 4, n = 127, d = 768, dtype = torch.float16
2024-12-30T09:35:15.547507683Z 
2024-12-30T09:35:15.547696336Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.547699994Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.547702075Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.547704104Z         torch.manual_seed(2024)
2024-12-30T09:35:15.547709326Z         atol = 5e-2
2024-12-30T09:35:15.547713608Z         rtol = 1e-2
2024-12-30T09:35:15.547742824Z         device = torch.device("cuda")
2024-12-30T09:35:15.547746324Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.547748591Z 
2024-12-30T09:35:15.547763172Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.547789115Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.547792469Z 
2024-12-30T09:35:15.547988572Z     def _lazy_init():
2024-12-30T09:35:15.547991836Z         global _initialized, _queued_calls
2024-12-30T09:35:15.547993866Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.548000494Z             return
2024-12-30T09:35:15.548002552Z         with _initialization_lock:
2024-12-30T09:35:15.548004532Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.548006671Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.548008684Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.548011343Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.548013376Z             # find there is nothing left to do.
2024-12-30T09:35:15.548015409Z             if is_initialized():
2024-12-30T09:35:15.548035661Z                 return
2024-12-30T09:35:15.548039315Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.548041596Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.548060387Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.548063690Z             if _is_in_bad_fork():
2024-12-30T09:35:15.548079863Z                 raise RuntimeError(
2024-12-30T09:35:15.548082920Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.548106691Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.548109908Z                 )
2024-12-30T09:35:15.548128522Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.548131536Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.548133696Z             if _cudart is None:
2024-12-30T09:35:15.548156331Z                 raise AssertionError(
2024-12-30T09:35:15.548158645Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.548160728Z                 )
2024-12-30T09:35:15.548173943Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.548176287Z             # are found or any other error occurs
2024-12-30T09:35:15.548196302Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.548199148Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.548201263Z >           torch._C._cuda_init()
2024-12-30T09:35:15.548255728Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.548259423Z 
2024-12-30T09:35:15.548261389Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.548433827Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.548437889Z 
2024-12-30T09:35:15.548465328Z b = 4, n = 127, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.548468168Z 
2024-12-30T09:35:15.548663126Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.548666430Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.548668611Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.548670604Z         torch.manual_seed(2024)
2024-12-30T09:35:15.548672621Z         atol = 5e-2
2024-12-30T09:35:15.548700661Z         rtol = 1e-2
2024-12-30T09:35:15.548703105Z         device = torch.device("cuda")
2024-12-30T09:35:15.548705091Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.548707137Z 
2024-12-30T09:35:15.548759319Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.548767266Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.548769737Z 
2024-12-30T09:35:15.548982302Z     def _lazy_init():
2024-12-30T09:35:15.548989039Z         global _initialized, _queued_calls
2024-12-30T09:35:15.548991120Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.548995114Z             return
2024-12-30T09:35:15.548997205Z         with _initialization_lock:
2024-12-30T09:35:15.549023625Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.549028137Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.549030259Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.549032344Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.549034335Z             # find there is nothing left to do.
2024-12-30T09:35:15.549057384Z             if is_initialized():
2024-12-30T09:35:15.549060526Z                 return
2024-12-30T09:35:15.549062700Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.549084656Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.549088208Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.549090273Z             if _is_in_bad_fork():
2024-12-30T09:35:15.549097588Z                 raise RuntimeError(
2024-12-30T09:35:15.549111384Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.549114077Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.549116173Z                 )
2024-12-30T09:35:15.549118766Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.549121059Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.549141830Z             if _cudart is None:
2024-12-30T09:35:15.549146393Z                 raise AssertionError(
2024-12-30T09:35:15.549164355Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.549170824Z                 )
2024-12-30T09:35:15.549175036Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.549178082Z             # are found or any other error occurs
2024-12-30T09:35:15.549181239Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.549206139Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.549209231Z >           torch._C._cuda_init()
2024-12-30T09:35:15.549211476Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.549236074Z 
2024-12-30T09:35:15.549239356Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.549428489Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-4-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.549432172Z 
2024-12-30T09:35:15.549434064Z b = 4, n = 128, d = 768, dtype = torch.float16
2024-12-30T09:35:15.549436093Z 
2024-12-30T09:35:15.549661982Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.549668450Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.549670734Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.549672702Z         torch.manual_seed(2024)
2024-12-30T09:35:15.549674817Z         atol = 5e-2
2024-12-30T09:35:15.549676852Z         rtol = 1e-2
2024-12-30T09:35:15.549678750Z         device = torch.device("cuda")
2024-12-30T09:35:15.549682368Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.549684698Z 
2024-12-30T09:35:15.549702223Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.549727104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.549729919Z 
2024-12-30T09:35:15.549928833Z     def _lazy_init():
2024-12-30T09:35:15.549933129Z         global _initialized, _queued_calls
2024-12-30T09:35:15.549935166Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.549937367Z             return
2024-12-30T09:35:15.549939279Z         with _initialization_lock:
2024-12-30T09:35:15.549941332Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.549969798Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.549982323Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.549984820Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.549987087Z             # find there is nothing left to do.
2024-12-30T09:35:15.549989341Z             if is_initialized():
2024-12-30T09:35:15.549992440Z                 return
2024-12-30T09:35:15.549994757Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.549996910Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.549999070Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.550014898Z             if _is_in_bad_fork():
2024-12-30T09:35:15.550017909Z                 raise RuntimeError(
2024-12-30T09:35:15.550036865Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.550042102Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.550045622Z                 )
2024-12-30T09:35:15.550059972Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.550063146Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.550099769Z             if _cudart is None:
2024-12-30T09:35:15.550103951Z                 raise AssertionError(
2024-12-30T09:35:15.550106137Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.550108468Z                 )
2024-12-30T09:35:15.550111343Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.550113552Z             # are found or any other error occurs
2024-12-30T09:35:15.550116114Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.550129419Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.550133825Z >           torch._C._cuda_init()
2024-12-30T09:35:15.550184519Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.550188476Z 
2024-12-30T09:35:15.550190425Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.550361775Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.550366864Z 
2024-12-30T09:35:15.550368925Z b = 4, n = 128, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.550370881Z 
2024-12-30T09:35:15.550586791Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.550594431Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.550597534Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.550600346Z         torch.manual_seed(2024)
2024-12-30T09:35:15.550603020Z         atol = 5e-2
2024-12-30T09:35:15.550606019Z         rtol = 1e-2
2024-12-30T09:35:15.550609512Z         device = torch.device("cuda")
2024-12-30T09:35:15.550612722Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.550615511Z 
2024-12-30T09:35:15.550642203Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.550645844Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.550648330Z 
2024-12-30T09:35:15.550857056Z     def _lazy_init():
2024-12-30T09:35:15.550870011Z         global _initialized, _queued_calls
2024-12-30T09:35:15.550872254Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.550874412Z             return
2024-12-30T09:35:15.550876403Z         with _initialization_lock:
2024-12-30T09:35:15.550878354Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.550880523Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.550883363Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.550885478Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.550887517Z             # find there is nothing left to do.
2024-12-30T09:35:15.550903883Z             if is_initialized():
2024-12-30T09:35:15.550907923Z                 return
2024-12-30T09:35:15.550909842Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.550947595Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.550951091Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.550953222Z             if _is_in_bad_fork():
2024-12-30T09:35:15.550955174Z                 raise RuntimeError(
2024-12-30T09:35:15.550957088Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.550959256Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.550965470Z                 )
2024-12-30T09:35:15.550967543Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.550989155Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.550991588Z             if _cudart is None:
2024-12-30T09:35:15.550998468Z                 raise AssertionError(
2024-12-30T09:35:15.551000581Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.551039019Z                 )
2024-12-30T09:35:15.551042127Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.551044343Z             # are found or any other error occurs
2024-12-30T09:35:15.551046337Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.551048465Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.551065825Z >           torch._C._cuda_init()
2024-12-30T09:35:15.551068838Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.551076222Z 
2024-12-30T09:35:15.551105625Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.551248648Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-4-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.551251427Z 
2024-12-30T09:35:15.551284328Z b = 4, n = 256, d = 768, dtype = torch.float16
2024-12-30T09:35:15.551288725Z 
2024-12-30T09:35:15.551464914Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.551467874Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.551470125Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.551475053Z         torch.manual_seed(2024)
2024-12-30T09:35:15.551477040Z         atol = 5e-2
2024-12-30T09:35:15.551503182Z         rtol = 1e-2
2024-12-30T09:35:15.551505708Z         device = torch.device("cuda")
2024-12-30T09:35:15.551507729Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.551509852Z 
2024-12-30T09:35:15.551516869Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.551582759Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.551592102Z 
2024-12-30T09:35:15.551754020Z     def _lazy_init():
2024-12-30T09:35:15.551758204Z         global _initialized, _queued_calls
2024-12-30T09:35:15.551760242Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.551762470Z             return
2024-12-30T09:35:15.551764406Z         with _initialization_lock:
2024-12-30T09:35:15.551766403Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.551770557Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.551772772Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.551790661Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.551793812Z             # find there is nothing left to do.
2024-12-30T09:35:15.551795778Z             if is_initialized():
2024-12-30T09:35:15.551798126Z                 return
2024-12-30T09:35:15.551809201Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.551814002Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.551825671Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.551828604Z             if _is_in_bad_fork():
2024-12-30T09:35:15.551836415Z                 raise RuntimeError(
2024-12-30T09:35:15.551838423Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.551874702Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.551878171Z                 )
2024-12-30T09:35:15.551880202Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.551882353Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.551891435Z             if _cudart is None:
2024-12-30T09:35:15.551895038Z                 raise AssertionError(
2024-12-30T09:35:15.551899808Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.551913413Z                 )
2024-12-30T09:35:15.551917469Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.551919688Z             # are found or any other error occurs
2024-12-30T09:35:15.551927681Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.551931888Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.551940348Z >           torch._C._cuda_init()
2024-12-30T09:35:15.551961553Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.551966646Z 
2024-12-30T09:35:15.551995186Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.552137094Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.552141705Z 
2024-12-30T09:35:15.552165963Z b = 4, n = 256, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.552169067Z 
2024-12-30T09:35:15.552327549Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.552331246Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.552333452Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.552335593Z         torch.manual_seed(2024)
2024-12-30T09:35:15.552348802Z         atol = 5e-2
2024-12-30T09:35:15.552351785Z         rtol = 1e-2
2024-12-30T09:35:15.552353864Z         device = torch.device("cuda")
2024-12-30T09:35:15.552379890Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.552383461Z 
2024-12-30T09:35:15.552412882Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.552416195Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.552418742Z 
2024-12-30T09:35:15.552610601Z     def _lazy_init():
2024-12-30T09:35:15.552614462Z         global _initialized, _queued_calls
2024-12-30T09:35:15.552616582Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.552618977Z             return
2024-12-30T09:35:15.552636839Z         with _initialization_lock:
2024-12-30T09:35:15.552643119Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.552656226Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.552659484Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.552661543Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.552691348Z             # find there is nothing left to do.
2024-12-30T09:35:15.552694288Z             if is_initialized():
2024-12-30T09:35:15.552696275Z                 return
2024-12-30T09:35:15.552698272Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.552700397Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.552714345Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.552720423Z             if _is_in_bad_fork():
2024-12-30T09:35:15.552753118Z                 raise RuntimeError(
2024-12-30T09:35:15.552756314Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.552758644Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.552760755Z                 )
2024-12-30T09:35:15.552762683Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.552764700Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.552783743Z             if _cudart is None:
2024-12-30T09:35:15.552786128Z                 raise AssertionError(
2024-12-30T09:35:15.552788068Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.552790246Z                 )
2024-12-30T09:35:15.552796573Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.552798753Z             # are found or any other error occurs
2024-12-30T09:35:15.552826026Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.552828495Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.552830571Z >           torch._C._cuda_init()
2024-12-30T09:35:15.552860684Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.552864495Z 
2024-12-30T09:35:15.552866431Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.553014454Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-4-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.553027614Z 
2024-12-30T09:35:15.553049556Z b = 4, n = 257, d = 768, dtype = torch.float16
2024-12-30T09:35:15.553053149Z 
2024-12-30T09:35:15.553221668Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.553225582Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.553227748Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.553229771Z         torch.manual_seed(2024)
2024-12-30T09:35:15.553231705Z         atol = 5e-2
2024-12-30T09:35:15.553235099Z         rtol = 1e-2
2024-12-30T09:35:15.553262024Z         device = torch.device("cuda")
2024-12-30T09:35:15.553265521Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.553267801Z 
2024-12-30T09:35:15.553306404Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.553309796Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.553312280Z 
2024-12-30T09:35:15.553524341Z     def _lazy_init():
2024-12-30T09:35:15.553527642Z         global _initialized, _queued_calls
2024-12-30T09:35:15.553529836Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.553532091Z             return
2024-12-30T09:35:15.553533977Z         with _initialization_lock:
2024-12-30T09:35:15.553535882Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.553537877Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.553539966Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.553541965Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.553578148Z             # find there is nothing left to do.
2024-12-30T09:35:15.553581708Z             if is_initialized():
2024-12-30T09:35:15.553583694Z                 return
2024-12-30T09:35:15.553585689Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.553588512Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.553590668Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.553621112Z             if _is_in_bad_fork():
2024-12-30T09:35:15.553625650Z                 raise RuntimeError(
2024-12-30T09:35:15.553627852Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.553634620Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.553637298Z                 )
2024-12-30T09:35:15.553657761Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.553660488Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.553662601Z             if _cudart is None:
2024-12-30T09:35:15.553665867Z                 raise AssertionError(
2024-12-30T09:35:15.553668361Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.553670786Z                 )
2024-12-30T09:35:15.553694236Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.553696850Z             # are found or any other error occurs
2024-12-30T09:35:15.553699095Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.553701564Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.553729841Z >           torch._C._cuda_init()
2024-12-30T09:35:15.553732509Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.553735436Z 
2024-12-30T09:35:15.553768062Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.553906842Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.553910341Z 
2024-12-30T09:35:15.553941010Z b = 4, n = 257, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.553944380Z 
2024-12-30T09:35:15.554110069Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.554114261Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.554116612Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.554118733Z         torch.manual_seed(2024)
2024-12-30T09:35:15.554120674Z         atol = 5e-2
2024-12-30T09:35:15.554124779Z         rtol = 1e-2
2024-12-30T09:35:15.554127252Z         device = torch.device("cuda")
2024-12-30T09:35:15.554156349Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.554162381Z 
2024-12-30T09:35:15.554177738Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.554180962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.554186772Z 
2024-12-30T09:35:15.554386512Z     def _lazy_init():
2024-12-30T09:35:15.554390822Z         global _initialized, _queued_calls
2024-12-30T09:35:15.554392860Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.554395175Z             return
2024-12-30T09:35:15.554423666Z         with _initialization_lock:
2024-12-30T09:35:15.554427563Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.554430601Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.554432704Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.554434749Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.554436872Z             # find there is nothing left to do.
2024-12-30T09:35:15.554439432Z             if is_initialized():
2024-12-30T09:35:15.554441527Z                 return
2024-12-30T09:35:15.554443544Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.554455651Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.554459893Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.554485008Z             if _is_in_bad_fork():
2024-12-30T09:35:15.554487959Z                 raise RuntimeError(
2024-12-30T09:35:15.554489870Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.554510961Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.554514293Z                 )
2024-12-30T09:35:15.554516263Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.554518371Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.554531165Z             if _cudart is None:
2024-12-30T09:35:15.554533459Z                 raise AssertionError(
2024-12-30T09:35:15.554577945Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.554581523Z                 )
2024-12-30T09:35:15.554583413Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.554585549Z             # are found or any other error occurs
2024-12-30T09:35:15.554587593Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.554593600Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.554595811Z >           torch._C._cuda_init()
2024-12-30T09:35:15.554656508Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.554661377Z 
2024-12-30T09:35:15.554663361Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.554806796Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.554815585Z 
2024-12-30T09:35:15.554833245Z b = 4, n = 1024, d = 768, dtype = torch.float16
2024-12-30T09:35:15.554836594Z 
2024-12-30T09:35:15.555013945Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.555017341Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.555019499Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.555021483Z         torch.manual_seed(2024)
2024-12-30T09:35:15.555023505Z         atol = 5e-2
2024-12-30T09:35:15.555025396Z         rtol = 1e-2
2024-12-30T09:35:15.555027285Z         device = torch.device("cuda")
2024-12-30T09:35:15.555048822Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.555052110Z 
2024-12-30T09:35:15.555054056Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.555094469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.555097741Z 
2024-12-30T09:35:15.555278883Z     def _lazy_init():
2024-12-30T09:35:15.555281989Z         global _initialized, _queued_calls
2024-12-30T09:35:15.555284014Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.555286289Z             return
2024-12-30T09:35:15.555288207Z         with _initialization_lock:
2024-12-30T09:35:15.555290200Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.555292322Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.555309287Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.555314194Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.555317505Z             # find there is nothing left to do.
2024-12-30T09:35:15.555320689Z             if is_initialized():
2024-12-30T09:35:15.555324821Z                 return
2024-12-30T09:35:15.555327491Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.555344846Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.555348306Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.555350377Z             if _is_in_bad_fork():
2024-12-30T09:35:15.555379190Z                 raise RuntimeError(
2024-12-30T09:35:15.555383076Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.555386279Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.555411757Z                 )
2024-12-30T09:35:15.555415078Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.555417323Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.555419509Z             if _cudart is None:
2024-12-30T09:35:15.555421477Z                 raise AssertionError(
2024-12-30T09:35:15.555435668Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.555440863Z                 )
2024-12-30T09:35:15.555444025Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.555457223Z             # are found or any other error occurs
2024-12-30T09:35:15.555460316Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.555486832Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.555495272Z >           torch._C._cuda_init()
2024-12-30T09:35:15.555497467Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.555500832Z 
2024-12-30T09:35:15.555529544Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.555674531Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.555677948Z 
2024-12-30T09:35:15.555711126Z b = 4, n = 1024, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.555714846Z 
2024-12-30T09:35:15.555878561Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.555881603Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.555883817Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.555885812Z         torch.manual_seed(2024)
2024-12-30T09:35:15.555903249Z         atol = 5e-2
2024-12-30T09:35:15.555911903Z         rtol = 1e-2
2024-12-30T09:35:15.555913880Z         device = torch.device("cuda")
2024-12-30T09:35:15.555916052Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.555925141Z 
2024-12-30T09:35:15.555943815Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.555946617Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.555961092Z 
2024-12-30T09:35:15.556153021Z     def _lazy_init():
2024-12-30T09:35:15.556156466Z         global _initialized, _queued_calls
2024-12-30T09:35:15.556158471Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.556160648Z             return
2024-12-30T09:35:15.556162543Z         with _initialization_lock:
2024-12-30T09:35:15.556184069Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.556191512Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.556195149Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.556198429Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.556202327Z             # find there is nothing left to do.
2024-12-30T09:35:15.556205469Z             if is_initialized():
2024-12-30T09:35:15.556208457Z                 return
2024-12-30T09:35:15.556211384Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.556219310Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.556223595Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.556271499Z             if _is_in_bad_fork():
2024-12-30T09:35:15.556283545Z                 raise RuntimeError(
2024-12-30T09:35:15.556286086Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.556288587Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.556290906Z                 )
2024-12-30T09:35:15.556294199Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.556296463Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.556298740Z             if _cudart is None:
2024-12-30T09:35:15.556300792Z                 raise AssertionError(
2024-12-30T09:35:15.556318003Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.556323832Z                 )
2024-12-30T09:35:15.556334084Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.556356202Z             # are found or any other error occurs
2024-12-30T09:35:15.556373138Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.556376443Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.556379708Z >           torch._C._cuda_init()
2024-12-30T09:35:15.556382705Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.556411714Z 
2024-12-30T09:35:15.556416600Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.556582358Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.556587549Z 
2024-12-30T09:35:15.556620115Z b = 4, n = 1025, d = 768, dtype = torch.float16
2024-12-30T09:35:15.556626421Z 
2024-12-30T09:35:15.556809641Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.556813093Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.556815450Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.556817778Z         torch.manual_seed(2024)
2024-12-30T09:35:15.556819782Z         atol = 5e-2
2024-12-30T09:35:15.556821715Z         rtol = 1e-2
2024-12-30T09:35:15.556846490Z         device = torch.device("cuda")
2024-12-30T09:35:15.556853069Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.556856607Z 
2024-12-30T09:35:15.556872825Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.556876994Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.556880952Z 
2024-12-30T09:35:15.557066175Z     def _lazy_init():
2024-12-30T09:35:15.557069755Z         global _initialized, _queued_calls
2024-12-30T09:35:15.557071744Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.557073996Z             return
2024-12-30T09:35:15.557076044Z         with _initialization_lock:
2024-12-30T09:35:15.557118805Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.557132499Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.557143403Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.557145693Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.557147719Z             # find there is nothing left to do.
2024-12-30T09:35:15.557149959Z             if is_initialized():
2024-12-30T09:35:15.557153479Z                 return
2024-12-30T09:35:15.557155569Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.557157655Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.557159853Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.557161905Z             if _is_in_bad_fork():
2024-12-30T09:35:15.557163910Z                 raise RuntimeError(
2024-12-30T09:35:15.557166542Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.557168910Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.557181731Z                 )
2024-12-30T09:35:15.557185628Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.557200202Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.557203828Z             if _cudart is None:
2024-12-30T09:35:15.557220998Z                 raise AssertionError(
2024-12-30T09:35:15.557223917Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.557226509Z                 )
2024-12-30T09:35:15.557228412Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.557259355Z             # are found or any other error occurs
2024-12-30T09:35:15.557269095Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.557272733Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.557276275Z >           torch._C._cuda_init()
2024-12-30T09:35:15.557293867Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.557298948Z 
2024-12-30T09:35:15.557323361Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.557497750Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-4-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.557517926Z 
2024-12-30T09:35:15.557521853Z b = 4, n = 1025, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.557532136Z 
2024-12-30T09:35:15.557722421Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.557728134Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.557731411Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.557734169Z         torch.manual_seed(2024)
2024-12-30T09:35:15.557736857Z         atol = 5e-2
2024-12-30T09:35:15.557761100Z         rtol = 1e-2
2024-12-30T09:35:15.557764012Z         device = torch.device("cuda")
2024-12-30T09:35:15.557766067Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.557768142Z 
2024-12-30T09:35:15.557770600Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.557814897Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.557820567Z 
2024-12-30T09:35:15.557986042Z     def _lazy_init():
2024-12-30T09:35:15.557989492Z         global _initialized, _queued_calls
2024-12-30T09:35:15.557991486Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.557993980Z             return
2024-12-30T09:35:15.557996012Z         with _initialization_lock:
2024-12-30T09:35:15.557998066Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.558000995Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.558003570Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.558040568Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.558043581Z             # find there is nothing left to do.
2024-12-30T09:35:15.558045600Z             if is_initialized():
2024-12-30T09:35:15.558047609Z                 return
2024-12-30T09:35:15.558077397Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.558087125Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.558090472Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.558092733Z             if _is_in_bad_fork():
2024-12-30T09:35:15.558094740Z                 raise RuntimeError(
2024-12-30T09:35:15.558126131Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.558129863Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.558132042Z                 )
2024-12-30T09:35:15.558141415Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.558143696Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.558145850Z             if _cudart is None:
2024-12-30T09:35:15.558147924Z                 raise AssertionError(
2024-12-30T09:35:15.558150510Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.558152725Z                 )
2024-12-30T09:35:15.558202036Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.558204961Z             # are found or any other error occurs
2024-12-30T09:35:15.558207017Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.558209073Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.558211290Z >           torch._C._cuda_init()
2024-12-30T09:35:15.558213413Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.558311934Z 
2024-12-30T09:35:15.558322294Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.558406591Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-8-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.558413099Z 
2024-12-30T09:35:15.558439236Z b = 8, n = 127, d = 768, dtype = torch.float16
2024-12-30T09:35:15.558444336Z 
2024-12-30T09:35:15.558629093Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.558650853Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.558654994Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.558658199Z         torch.manual_seed(2024)
2024-12-30T09:35:15.558661332Z         atol = 5e-2
2024-12-30T09:35:15.558664440Z         rtol = 1e-2
2024-12-30T09:35:15.558668364Z         device = torch.device("cuda")
2024-12-30T09:35:15.558671800Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.558675455Z 
2024-12-30T09:35:15.558679082Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.558717067Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.558721513Z 
2024-12-30T09:35:15.558911183Z     def _lazy_init():
2024-12-30T09:35:15.558923876Z         global _initialized, _queued_calls
2024-12-30T09:35:15.558927220Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.558930666Z             return
2024-12-30T09:35:15.558933561Z         with _initialization_lock:
2024-12-30T09:35:15.558936470Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.558939872Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.558942930Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.558947209Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.558950464Z             # find there is nothing left to do.
2024-12-30T09:35:15.558954153Z             if is_initialized():
2024-12-30T09:35:15.558957343Z                 return
2024-12-30T09:35:15.558963678Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.558967324Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.558974560Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.558981208Z             if _is_in_bad_fork():
2024-12-30T09:35:15.559000741Z                 raise RuntimeError(
2024-12-30T09:35:15.559003167Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.559009535Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.559040217Z                 )
2024-12-30T09:35:15.559042890Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.559044908Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.559072217Z             if _cudart is None:
2024-12-30T09:35:15.559074786Z                 raise AssertionError(
2024-12-30T09:35:15.559076694Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.559078858Z                 )
2024-12-30T09:35:15.559087523Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.559089741Z             # are found or any other error occurs
2024-12-30T09:35:15.559125977Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.559132936Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.559136386Z >           torch._C._cuda_init()
2024-12-30T09:35:15.559139686Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.559155304Z 
2024-12-30T09:35:15.559158503Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.559310702Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.559317651Z 
2024-12-30T09:35:15.559346817Z b = 8, n = 127, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.559353712Z 
2024-12-30T09:35:15.559512946Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.559516301Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.559518573Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.559520623Z         torch.manual_seed(2024)
2024-12-30T09:35:15.559522656Z         atol = 5e-2
2024-12-30T09:35:15.559524539Z         rtol = 1e-2
2024-12-30T09:35:15.559565462Z         device = torch.device("cuda")
2024-12-30T09:35:15.559568915Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.559571088Z 
2024-12-30T09:35:15.559572876Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.559657309Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.559665697Z 
2024-12-30T09:35:15.559787317Z     def _lazy_init():
2024-12-30T09:35:15.559790433Z         global _initialized, _queued_calls
2024-12-30T09:35:15.559792453Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.559794690Z             return
2024-12-30T09:35:15.559796606Z         with _initialization_lock:
2024-12-30T09:35:15.559798580Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.559822176Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.559838548Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.559841623Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.559843870Z             # find there is nothing left to do.
2024-12-30T09:35:15.559846588Z             if is_initialized():
2024-12-30T09:35:15.559848628Z                 return
2024-12-30T09:35:15.559850687Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.559861434Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.559873134Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.559882614Z             if _is_in_bad_fork():
2024-12-30T09:35:15.559884584Z                 raise RuntimeError(
2024-12-30T09:35:15.559886471Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.559889975Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.559928878Z                 )
2024-12-30T09:35:15.559933126Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.559935450Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.559937597Z             if _cudart is None:
2024-12-30T09:35:15.559939652Z                 raise AssertionError(
2024-12-30T09:35:15.559942721Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.559944940Z                 )
2024-12-30T09:35:15.559963618Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.559966833Z             # are found or any other error occurs
2024-12-30T09:35:15.559968914Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.559970932Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.559973499Z >           torch._C._cuda_init()
2024-12-30T09:35:15.560030267Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.560034099Z 
2024-12-30T09:35:15.560036034Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.560190793Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-8-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.560194363Z 
2024-12-30T09:35:15.560196198Z b = 8, n = 128, d = 768, dtype = torch.float16
2024-12-30T09:35:15.560198187Z 
2024-12-30T09:35:15.560416472Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.560421134Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.560423407Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.560425483Z         torch.manual_seed(2024)
2024-12-30T09:35:15.560427396Z         atol = 5e-2
2024-12-30T09:35:15.560429255Z         rtol = 1e-2
2024-12-30T09:35:15.560431112Z         device = torch.device("cuda")
2024-12-30T09:35:15.560433227Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.560442917Z 
2024-12-30T09:35:15.560445469Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.560469845Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.560474497Z 
2024-12-30T09:35:15.560703364Z     def _lazy_init():
2024-12-30T09:35:15.560707417Z         global _initialized, _queued_calls
2024-12-30T09:35:15.560709519Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.560711683Z             return
2024-12-30T09:35:15.560713590Z         with _initialization_lock:
2024-12-30T09:35:15.560715539Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.560717607Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.560719597Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.560721567Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.560723531Z             # find there is nothing left to do.
2024-12-30T09:35:15.560725929Z             if is_initialized():
2024-12-30T09:35:15.560727831Z                 return
2024-12-30T09:35:15.560731912Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.560734120Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.560736217Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.560763398Z             if _is_in_bad_fork():
2024-12-30T09:35:15.560772409Z                 raise RuntimeError(
2024-12-30T09:35:15.560775198Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.560779681Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.560782056Z                 )
2024-12-30T09:35:15.560784803Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.560787201Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.560813727Z             if _cudart is None:
2024-12-30T09:35:15.560816906Z                 raise AssertionError(
2024-12-30T09:35:15.560818852Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.560821006Z                 )
2024-12-30T09:35:15.560844347Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.560847574Z             # are found or any other error occurs
2024-12-30T09:35:15.560859394Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.560861634Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.560863730Z >           torch._C._cuda_init()
2024-12-30T09:35:15.560882758Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.560886864Z 
2024-12-30T09:35:15.560934724Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.561068018Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.561072473Z 
2024-12-30T09:35:15.561117463Z b = 8, n = 128, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.561121330Z 
2024-12-30T09:35:15.561259458Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.561262703Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.561264946Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.561266925Z         torch.manual_seed(2024)
2024-12-30T09:35:15.561268931Z         atol = 5e-2
2024-12-30T09:35:15.561274323Z         rtol = 1e-2
2024-12-30T09:35:15.561277775Z         device = torch.device("cuda")
2024-12-30T09:35:15.561279848Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.561293765Z 
2024-12-30T09:35:15.561296590Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.561337241Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.561340694Z 
2024-12-30T09:35:15.561527724Z     def _lazy_init():
2024-12-30T09:35:15.561532445Z         global _initialized, _queued_calls
2024-12-30T09:35:15.561534573Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.561536889Z             return
2024-12-30T09:35:15.561538776Z         with _initialization_lock:
2024-12-30T09:35:15.561563438Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.561567861Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.561569921Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.561571927Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.561578178Z             # find there is nothing left to do.
2024-12-30T09:35:15.561585008Z             if is_initialized():
2024-12-30T09:35:15.561587086Z                 return
2024-12-30T09:35:15.561601727Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.561604211Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.561614104Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.561617738Z             if _is_in_bad_fork():
2024-12-30T09:35:15.561635759Z                 raise RuntimeError(
2024-12-30T09:35:15.561639427Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.561641652Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.561659856Z                 )
2024-12-30T09:35:15.561662153Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.561669372Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.561671625Z             if _cudart is None:
2024-12-30T09:35:15.561673615Z                 raise AssertionError(
2024-12-30T09:35:15.561690692Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.561693175Z                 )
2024-12-30T09:35:15.561702542Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.561705568Z             # are found or any other error occurs
2024-12-30T09:35:15.561749171Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.561752466Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.561754648Z >           torch._C._cuda_init()
2024-12-30T09:35:15.561756677Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.561759029Z 
2024-12-30T09:35:15.561784512Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.561932339Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-8-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.561935384Z 
2024-12-30T09:35:15.561958606Z b = 8, n = 256, d = 768, dtype = torch.float16
2024-12-30T09:35:15.561961907Z 
2024-12-30T09:35:15.562158911Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.562162337Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.562168033Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.562170102Z         torch.manual_seed(2024)
2024-12-30T09:35:15.562172048Z         atol = 5e-2
2024-12-30T09:35:15.562173964Z         rtol = 1e-2
2024-12-30T09:35:15.562175971Z         device = torch.device("cuda")
2024-12-30T09:35:15.562178034Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.562180211Z 
2024-12-30T09:35:15.562182472Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.562195487Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.562200903Z 
2024-12-30T09:35:15.562451217Z     def _lazy_init():
2024-12-30T09:35:15.562454616Z         global _initialized, _queued_calls
2024-12-30T09:35:15.562456641Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.562458950Z             return
2024-12-30T09:35:15.562461071Z         with _initialization_lock:
2024-12-30T09:35:15.562463053Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.562465054Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.562467055Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.562469199Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.562471230Z             # find there is nothing left to do.
2024-12-30T09:35:15.562473226Z             if is_initialized():
2024-12-30T09:35:15.562475216Z                 return
2024-12-30T09:35:15.562478117Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.562480174Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.562482202Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.562484186Z             if _is_in_bad_fork():
2024-12-30T09:35:15.562502499Z                 raise RuntimeError(
2024-12-30T09:35:15.562511310Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.562513752Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.562516094Z                 )
2024-12-30T09:35:15.562518731Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.562541300Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.562564975Z             if _cudart is None:
2024-12-30T09:35:15.562569397Z                 raise AssertionError(
2024-12-30T09:35:15.562572512Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.562576663Z                 )
2024-12-30T09:35:15.562585484Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.562589148Z             # are found or any other error occurs
2024-12-30T09:35:15.562593172Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.562596293Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.562616184Z >           torch._C._cuda_init()
2024-12-30T09:35:15.562621854Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.562630411Z 
2024-12-30T09:35:15.562646050Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.562834094Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.562840055Z 
2024-12-30T09:35:15.562843370Z b = 8, n = 256, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.562847151Z 
2024-12-30T09:35:15.563026041Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.563030368Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.563033785Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.563037076Z         torch.manual_seed(2024)
2024-12-30T09:35:15.563040312Z         atol = 5e-2
2024-12-30T09:35:15.563043371Z         rtol = 1e-2
2024-12-30T09:35:15.563046286Z         device = torch.device("cuda")
2024-12-30T09:35:15.563049292Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.563052251Z 
2024-12-30T09:35:15.563077486Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.563085320Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.563089293Z 
2024-12-30T09:35:15.563255726Z     def _lazy_init():
2024-12-30T09:35:15.563260179Z         global _initialized, _queued_calls
2024-12-30T09:35:15.563262264Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.563264594Z             return
2024-12-30T09:35:15.563284843Z         with _initialization_lock:
2024-12-30T09:35:15.563290761Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.563294029Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.563297417Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.563305799Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.563309562Z             # find there is nothing left to do.
2024-12-30T09:35:15.563311545Z             if is_initialized():
2024-12-30T09:35:15.563333291Z                 return
2024-12-30T09:35:15.563335891Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.563338097Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.563340114Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.563356716Z             if _is_in_bad_fork():
2024-12-30T09:35:15.563366433Z                 raise RuntimeError(
2024-12-30T09:35:15.563369213Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.563372295Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.563392074Z                 )
2024-12-30T09:35:15.563395700Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.563397883Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.563405405Z             if _cudart is None:
2024-12-30T09:35:15.563410220Z                 raise AssertionError(
2024-12-30T09:35:15.563421512Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.563424641Z                 )
2024-12-30T09:35:15.563451152Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.563455956Z             # are found or any other error occurs
2024-12-30T09:35:15.563459552Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.563462847Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.563484151Z >           torch._C._cuda_init()
2024-12-30T09:35:15.563487373Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.563489852Z 
2024-12-30T09:35:15.563517766Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.563658441Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype1-8-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.563666349Z 
2024-12-30T09:35:15.563701867Z b = 8, n = 257, d = 768, dtype = torch.float16
2024-12-30T09:35:15.563706016Z 
2024-12-30T09:35:15.563856436Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.563859308Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.563861514Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.563863430Z         torch.manual_seed(2024)
2024-12-30T09:35:15.563877689Z         atol = 5e-2
2024-12-30T09:35:15.563882756Z         rtol = 1e-2
2024-12-30T09:35:15.563885800Z         device = torch.device("cuda")
2024-12-30T09:35:15.563899881Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.563903073Z 
2024-12-30T09:35:15.563931542Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.563934155Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.563939843Z 
2024-12-30T09:35:15.564142772Z     def _lazy_init():
2024-12-30T09:35:15.564151035Z         global _initialized, _queued_calls
2024-12-30T09:35:15.564153060Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.564155430Z             return
2024-12-30T09:35:15.564157575Z         with _initialization_lock:
2024-12-30T09:35:15.564159541Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.564173809Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.564177897Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.564180036Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.564182146Z             # find there is nothing left to do.
2024-12-30T09:35:15.564214205Z             if is_initialized():
2024-12-30T09:35:15.564219197Z                 return
2024-12-30T09:35:15.564222509Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.564225837Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.564229106Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.564248203Z             if _is_in_bad_fork():
2024-12-30T09:35:15.564251426Z                 raise RuntimeError(
2024-12-30T09:35:15.564257210Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.564295277Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.564300667Z                 )
2024-12-30T09:35:15.564303430Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.564306651Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.564309990Z             if _cudart is None:
2024-12-30T09:35:15.564313820Z                 raise AssertionError(
2024-12-30T09:35:15.564316924Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.564320142Z                 )
2024-12-30T09:35:15.564323620Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.564326728Z             # are found or any other error occurs
2024-12-30T09:35:15.564330289Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.564342150Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.564347487Z >           torch._C._cuda_init()
2024-12-30T09:35:15.564428692Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.564441024Z 
2024-12-30T09:35:15.564444412Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.564570485Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.564577007Z 
2024-12-30T09:35:15.564580168Z b = 8, n = 257, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.564583357Z 
2024-12-30T09:35:15.564771938Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.564776759Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.564779058Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.564781071Z         torch.manual_seed(2024)
2024-12-30T09:35:15.564783025Z         atol = 5e-2
2024-12-30T09:35:15.564784981Z         rtol = 1e-2
2024-12-30T09:35:15.564786974Z         device = torch.device("cuda")
2024-12-30T09:35:15.564800772Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.564805550Z 
2024-12-30T09:35:15.564816413Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.564860252Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.564863291Z 
2024-12-30T09:35:15.565043085Z     def _lazy_init():
2024-12-30T09:35:15.565048370Z         global _initialized, _queued_calls
2024-12-30T09:35:15.565051741Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.565055087Z             return
2024-12-30T09:35:15.565058102Z         with _initialization_lock:
2024-12-30T09:35:15.565061139Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.565065348Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.565068669Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.565071864Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.565074965Z             # find there is nothing left to do.
2024-12-30T09:35:15.565083714Z             if is_initialized():
2024-12-30T09:35:15.565088132Z                 return
2024-12-30T09:35:15.565112698Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.565116666Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.565118706Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.565120788Z             if _is_in_bad_fork():
2024-12-30T09:35:15.565134786Z                 raise RuntimeError(
2024-12-30T09:35:15.565137063Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.565139249Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.565171650Z                 )
2024-12-30T09:35:15.565174696Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.565177059Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.565179246Z             if _cudart is None:
2024-12-30T09:35:15.565181280Z                 raise AssertionError(
2024-12-30T09:35:15.565220687Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.565225683Z                 )
2024-12-30T09:35:15.565228894Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.565232084Z             # are found or any other error occurs
2024-12-30T09:35:15.565235457Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.565238686Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.565248122Z >           torch._C._cuda_init()
2024-12-30T09:35:15.565267854Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.565275550Z 
2024-12-30T09:35:15.565278773Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.565441840Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.565446986Z 
2024-12-30T09:35:15.565459891Z b = 8, n = 1024, d = 768, dtype = torch.float16
2024-12-30T09:35:15.565464565Z 
2024-12-30T09:35:15.565644411Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.565656821Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.565660379Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.565663364Z         torch.manual_seed(2024)
2024-12-30T09:35:15.565666368Z         atol = 5e-2
2024-12-30T09:35:15.565670042Z         rtol = 1e-2
2024-12-30T09:35:15.565673248Z         device = torch.device("cuda")
2024-12-30T09:35:15.565676397Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.565686348Z 
2024-12-30T09:35:15.565690454Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.565736434Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.565740130Z 
2024-12-30T09:35:15.565914687Z     def _lazy_init():
2024-12-30T09:35:15.565917447Z         global _initialized, _queued_calls
2024-12-30T09:35:15.565919516Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.565921662Z             return
2024-12-30T09:35:15.565923615Z         with _initialization_lock:
2024-12-30T09:35:15.565925548Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.565928279Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.565942243Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.565947650Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.565950991Z             # find there is nothing left to do.
2024-12-30T09:35:15.565969794Z             if is_initialized():
2024-12-30T09:35:15.565975170Z                 return
2024-12-30T09:35:15.565987514Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.565991269Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.566013003Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.566017253Z             if _is_in_bad_fork():
2024-12-30T09:35:15.566020217Z                 raise RuntimeError(
2024-12-30T09:35:15.566023152Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.566026640Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.566057684Z                 )
2024-12-30T09:35:15.566060889Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.566063152Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.566065260Z             if _cudart is None:
2024-12-30T09:35:15.566067158Z                 raise AssertionError(
2024-12-30T09:35:15.566069753Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.566071979Z                 )
2024-12-30T09:35:15.566085701Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.566090344Z             # are found or any other error occurs
2024-12-30T09:35:15.566105536Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.566108859Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.566112009Z >           torch._C._cuda_init()
2024-12-30T09:35:15.566180260Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.566185042Z 
2024-12-30T09:35:15.566188141Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.566308212Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.566311760Z 
2024-12-30T09:35:15.566338114Z b = 8, n = 1024, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.566341056Z 
2024-12-30T09:35:15.566530942Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.566534777Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.566536968Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.566538912Z         torch.manual_seed(2024)
2024-12-30T09:35:15.566575304Z         atol = 5e-2
2024-12-30T09:35:15.566581507Z         rtol = 1e-2
2024-12-30T09:35:15.566584632Z         device = torch.device("cuda")
2024-12-30T09:35:15.566588632Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.566603076Z 
2024-12-30T09:35:15.566666523Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.566669710Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.566672097Z 
2024-12-30T09:35:15.566869752Z     def _lazy_init():
2024-12-30T09:35:15.566874408Z         global _initialized, _queued_calls
2024-12-30T09:35:15.566876560Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.566878814Z             return
2024-12-30T09:35:15.566893956Z         with _initialization_lock:
2024-12-30T09:35:15.566897065Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.566899120Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.566901183Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.566925516Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.566928276Z             # find there is nothing left to do.
2024-12-30T09:35:15.566930231Z             if is_initialized():
2024-12-30T09:35:15.566932806Z                 return
2024-12-30T09:35:15.566934749Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.566964118Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.566967814Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.566969877Z             if _is_in_bad_fork():
2024-12-30T09:35:15.566971858Z                 raise RuntimeError(
2024-12-30T09:35:15.566991723Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.566994243Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.566996443Z                 )
2024-12-30T09:35:15.567006993Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.567009288Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.567014586Z             if _cudart is None:
2024-12-30T09:35:15.567016737Z                 raise AssertionError(
2024-12-30T09:35:15.567034367Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.567041443Z                 )
2024-12-30T09:35:15.567059637Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.567063102Z             # are found or any other error occurs
2024-12-30T09:35:15.567065178Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.567078868Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.567082251Z >           torch._C._cuda_init()
2024-12-30T09:35:15.567115795Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.567119635Z 
2024-12-30T09:35:15.567121466Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.567299401Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.567305346Z 
2024-12-30T09:35:15.567309181Z b = 8, n = 1025, d = 768, dtype = torch.float16
2024-12-30T09:35:15.567312436Z 
2024-12-30T09:35:15.567533589Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.567538864Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.567541806Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.567544679Z         torch.manual_seed(2024)
2024-12-30T09:35:15.567547480Z         atol = 5e-2
2024-12-30T09:35:15.567560894Z         rtol = 1e-2
2024-12-30T09:35:15.567563703Z         device = torch.device("cuda")
2024-12-30T09:35:15.567568030Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.567571397Z 
2024-12-30T09:35:15.567602843Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.567610383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.567612810Z 
2024-12-30T09:35:15.567826606Z     def _lazy_init():
2024-12-30T09:35:15.567830092Z         global _initialized, _queued_calls
2024-12-30T09:35:15.567832050Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.567834300Z             return
2024-12-30T09:35:15.567836232Z         with _initialization_lock:
2024-12-30T09:35:15.567838134Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.567845939Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.567850874Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.567853001Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.567856767Z             # find there is nothing left to do.
2024-12-30T09:35:15.567858864Z             if is_initialized():
2024-12-30T09:35:15.567879323Z                 return
2024-12-30T09:35:15.567882059Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.567884114Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.567905887Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.567908637Z             if _is_in_bad_fork():
2024-12-30T09:35:15.567910572Z                 raise RuntimeError(
2024-12-30T09:35:15.567913363Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.567943341Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.567948649Z                 )
2024-12-30T09:35:15.567950795Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.567967223Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.567970576Z             if _cudart is None:
2024-12-30T09:35:15.567972590Z                 raise AssertionError(
2024-12-30T09:35:15.567974580Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.567976741Z                 )
2024-12-30T09:35:15.567979313Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.568009421Z             # are found or any other error occurs
2024-12-30T09:35:15.568011813Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.568013844Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.568015920Z >           torch._C._cuda_init()
2024-12-30T09:35:15.568044804Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.568048270Z 
2024-12-30T09:35:15.568050174Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.568239058Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype1-8-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.568245398Z 
2024-12-30T09:35:15.568280670Z b = 8, n = 1025, d = 1024, dtype = torch.float16
2024-12-30T09:35:15.568284376Z 
2024-12-30T09:35:15.568443452Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.568447929Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.568450048Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.568452102Z         torch.manual_seed(2024)
2024-12-30T09:35:15.568454133Z         atol = 5e-2
2024-12-30T09:35:15.568456030Z         rtol = 1e-2
2024-12-30T09:35:15.568465270Z         device = torch.device("cuda")
2024-12-30T09:35:15.568469682Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.568471824Z 
2024-12-30T09:35:15.568492273Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.568497447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.568501209Z 
2024-12-30T09:35:15.568734681Z     def _lazy_init():
2024-12-30T09:35:15.568738747Z         global _initialized, _queued_calls
2024-12-30T09:35:15.568740738Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.568742789Z             return
2024-12-30T09:35:15.568744760Z         with _initialization_lock:
2024-12-30T09:35:15.568772990Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.568777076Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.568779261Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.568781295Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.568783364Z             # find there is nothing left to do.
2024-12-30T09:35:15.568785297Z             if is_initialized():
2024-12-30T09:35:15.568787202Z                 return
2024-12-30T09:35:15.568789133Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.568791227Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.568793889Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.568795930Z             if _is_in_bad_fork():
2024-12-30T09:35:15.568811173Z                 raise RuntimeError(
2024-12-30T09:35:15.568828758Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.568832028Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.568838523Z                 )
2024-12-30T09:35:15.568840412Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.568843812Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.568874945Z             if _cudart is None:
2024-12-30T09:35:15.568877412Z                 raise AssertionError(
2024-12-30T09:35:15.568879342Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.568881415Z                 )
2024-12-30T09:35:15.568896948Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.568901085Z             # are found or any other error occurs
2024-12-30T09:35:15.568903112Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.568913930Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.568917461Z >           torch._C._cuda_init()
2024-12-30T09:35:15.568948784Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.568954004Z 
2024-12-30T09:35:15.568956934Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.569143294Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-1-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.569148101Z 
2024-12-30T09:35:15.569150028Z b = 1, n = 127, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.569152006Z 
2024-12-30T09:35:15.569374362Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.569378961Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.569381285Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.569383298Z         torch.manual_seed(2024)
2024-12-30T09:35:15.569385198Z         atol = 5e-2
2024-12-30T09:35:15.569387077Z         rtol = 1e-2
2024-12-30T09:35:15.569389058Z         device = torch.device("cuda")
2024-12-30T09:35:15.569391085Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.569393180Z 
2024-12-30T09:35:15.569424284Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.569427791Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.569430180Z 
2024-12-30T09:35:15.569634396Z     def _lazy_init():
2024-12-30T09:35:15.569638753Z         global _initialized, _queued_calls
2024-12-30T09:35:15.569640776Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.569642912Z             return
2024-12-30T09:35:15.569644779Z         with _initialization_lock:
2024-12-30T09:35:15.569646738Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.569648797Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.569689142Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.569700252Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.569702519Z             # find there is nothing left to do.
2024-12-30T09:35:15.569704560Z             if is_initialized():
2024-12-30T09:35:15.569706542Z                 return
2024-12-30T09:35:15.569708746Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.569714723Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.569716949Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.569718953Z             if _is_in_bad_fork():
2024-12-30T09:35:15.569720970Z                 raise RuntimeError(
2024-12-30T09:35:15.569729132Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.569731545Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.569736443Z                 )
2024-12-30T09:35:15.569743266Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.569777895Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.569781103Z             if _cudart is None:
2024-12-30T09:35:15.569783104Z                 raise AssertionError(
2024-12-30T09:35:15.569785138Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.569792716Z                 )
2024-12-30T09:35:15.569796012Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.569813758Z             # are found or any other error occurs
2024-12-30T09:35:15.569820870Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.569833809Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.569837292Z >           torch._C._cuda_init()
2024-12-30T09:35:15.569849641Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.569859159Z 
2024-12-30T09:35:15.569891176Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.570031052Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.570034606Z 
2024-12-30T09:35:15.570057926Z b = 1, n = 127, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.570061450Z 
2024-12-30T09:35:15.570233741Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.570238387Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.570241478Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.570244366Z         torch.manual_seed(2024)
2024-12-30T09:35:15.570247346Z         atol = 5e-2
2024-12-30T09:35:15.570251189Z         rtol = 1e-2
2024-12-30T09:35:15.570253852Z         device = torch.device("cuda")
2024-12-30T09:35:15.570256737Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.570259923Z 
2024-12-30T09:35:15.570314604Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.570318076Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.570320438Z 
2024-12-30T09:35:15.570497889Z     def _lazy_init():
2024-12-30T09:35:15.570503912Z         global _initialized, _queued_calls
2024-12-30T09:35:15.570507022Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.570510069Z             return
2024-12-30T09:35:15.570512758Z         with _initialization_lock:
2024-12-30T09:35:15.570515841Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.570530588Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.570533882Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.570538429Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.570540648Z             # find there is nothing left to do.
2024-12-30T09:35:15.570542976Z             if is_initialized():
2024-12-30T09:35:15.570567594Z                 return
2024-12-30T09:35:15.570588477Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.570591671Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.570597437Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.570618036Z             if _is_in_bad_fork():
2024-12-30T09:35:15.570621474Z                 raise RuntimeError(
2024-12-30T09:35:15.570623484Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.570632204Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.570636259Z                 )
2024-12-30T09:35:15.570638260Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.570646544Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.570648749Z             if _cudart is None:
2024-12-30T09:35:15.570666142Z                 raise AssertionError(
2024-12-30T09:35:15.570668471Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.570691442Z                 )
2024-12-30T09:35:15.570693784Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.570696412Z             # are found or any other error occurs
2024-12-30T09:35:15.570698494Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.570721154Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.570725012Z >           torch._C._cuda_init()
2024-12-30T09:35:15.570739002Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.570742469Z 
2024-12-30T09:35:15.570759168Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.570927616Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-1-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.570931058Z 
2024-12-30T09:35:15.570933378Z b = 1, n = 128, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.570935517Z 
2024-12-30T09:35:15.571127440Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.571131685Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.571134052Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.571136087Z         torch.manual_seed(2024)
2024-12-30T09:35:15.571138079Z         atol = 5e-2
2024-12-30T09:35:15.571140119Z         rtol = 1e-2
2024-12-30T09:35:15.571145210Z         device = torch.device("cuda")
2024-12-30T09:35:15.571152724Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.571155072Z 
2024-12-30T09:35:15.571173675Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.571180682Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.571183190Z 
2024-12-30T09:35:15.571405051Z     def _lazy_init():
2024-12-30T09:35:15.571408706Z         global _initialized, _queued_calls
2024-12-30T09:35:15.571410689Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.571413090Z             return
2024-12-30T09:35:15.571414988Z         with _initialization_lock:
2024-12-30T09:35:15.571416952Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.571418938Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.571421002Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.571423619Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.571464346Z             # find there is nothing left to do.
2024-12-30T09:35:15.571481568Z             if is_initialized():
2024-12-30T09:35:15.571485301Z                 return
2024-12-30T09:35:15.571488202Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.571491162Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.571494343Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.571497269Z             if _is_in_bad_fork():
2024-12-30T09:35:15.571509322Z                 raise RuntimeError(
2024-12-30T09:35:15.571512611Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.571515610Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.571518904Z                 )
2024-12-30T09:35:15.571521825Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.571525040Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.571539683Z             if _cudart is None:
2024-12-30T09:35:15.571542747Z                 raise AssertionError(
2024-12-30T09:35:15.571545448Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.571563466Z                 )
2024-12-30T09:35:15.571591440Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.571601475Z             # are found or any other error occurs
2024-12-30T09:35:15.571603578Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.571605717Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.571627657Z >           torch._C._cuda_init()
2024-12-30T09:35:15.571630060Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.571632523Z 
2024-12-30T09:35:15.571658569Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.571853520Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.571857703Z 
2024-12-30T09:35:15.571859668Z b = 1, n = 128, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.571861719Z 
2024-12-30T09:35:15.572007459Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.572010233Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.572012322Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.572014271Z         torch.manual_seed(2024)
2024-12-30T09:35:15.572016275Z         atol = 5e-2
2024-12-30T09:35:15.572018151Z         rtol = 1e-2
2024-12-30T09:35:15.572020045Z         device = torch.device("cuda")
2024-12-30T09:35:15.572022759Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.572024963Z 
2024-12-30T09:35:15.572078959Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.572086243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.572088703Z 
2024-12-30T09:35:15.572253429Z     def _lazy_init():
2024-12-30T09:35:15.572256633Z         global _initialized, _queued_calls
2024-12-30T09:35:15.572258686Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.572260823Z             return
2024-12-30T09:35:15.572262653Z         with _initialization_lock:
2024-12-30T09:35:15.572279327Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.572284074Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.572286122Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.572303724Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.572306104Z             # find there is nothing left to do.
2024-12-30T09:35:15.572308470Z             if is_initialized():
2024-12-30T09:35:15.572310615Z                 return
2024-12-30T09:35:15.572334740Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.572337647Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.572339667Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.572341732Z             if _is_in_bad_fork():
2024-12-30T09:35:15.572372923Z                 raise RuntimeError(
2024-12-30T09:35:15.572376563Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.572378914Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.572396163Z                 )
2024-12-30T09:35:15.572399534Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.572401637Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.572418271Z             if _cudart is None:
2024-12-30T09:35:15.572421281Z                 raise AssertionError(
2024-12-30T09:35:15.572423218Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.572459478Z                 )
2024-12-30T09:35:15.572462494Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.572464531Z             # are found or any other error occurs
2024-12-30T09:35:15.572466482Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.572472301Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.572474644Z >           torch._C._cuda_init()
2024-12-30T09:35:15.572517137Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.572520034Z 
2024-12-30T09:35:15.572521999Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.572700796Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-1-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.572713593Z 
2024-12-30T09:35:15.572721938Z b = 1, n = 256, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.572724110Z 
2024-12-30T09:35:15.572892543Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.572903040Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.572906298Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.572908300Z         torch.manual_seed(2024)
2024-12-30T09:35:15.572910329Z         atol = 5e-2
2024-12-30T09:35:15.572912759Z         rtol = 1e-2
2024-12-30T09:35:15.572914808Z         device = torch.device("cuda")
2024-12-30T09:35:15.572916766Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.572926560Z 
2024-12-30T09:35:15.572967781Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.572971326Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.572973715Z 
2024-12-30T09:35:15.573175515Z     def _lazy_init():
2024-12-30T09:35:15.573183322Z         global _initialized, _queued_calls
2024-12-30T09:35:15.573185414Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.573187758Z             return
2024-12-30T09:35:15.573189804Z         with _initialization_lock:
2024-12-30T09:35:15.573192086Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.573194218Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.573209935Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.573215018Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.573218128Z             # find there is nothing left to do.
2024-12-30T09:35:15.573221865Z             if is_initialized():
2024-12-30T09:35:15.573225820Z                 return
2024-12-30T09:35:15.573228408Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.573249375Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.573267708Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.573269849Z             if _is_in_bad_fork():
2024-12-30T09:35:15.573272506Z                 raise RuntimeError(
2024-12-30T09:35:15.573283106Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.573286970Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.573289156Z                 )
2024-12-30T09:35:15.573297542Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.573309950Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.573313724Z             if _cudart is None:
2024-12-30T09:35:15.573315810Z                 raise AssertionError(
2024-12-30T09:35:15.573334721Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.573339021Z                 )
2024-12-30T09:35:15.573340967Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.573348692Z             # are found or any other error occurs
2024-12-30T09:35:15.573354839Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.573370499Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.573384538Z >           torch._C._cuda_init()
2024-12-30T09:35:15.573401024Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.573404397Z 
2024-12-30T09:35:15.573450371Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.573592270Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.573596579Z 
2024-12-30T09:35:15.573616784Z b = 1, n = 256, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.573619671Z 
2024-12-30T09:35:15.573807949Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.573811804Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.573813967Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.573815928Z         torch.manual_seed(2024)
2024-12-30T09:35:15.573817838Z         atol = 5e-2
2024-12-30T09:35:15.573819779Z         rtol = 1e-2
2024-12-30T09:35:15.573821662Z         device = torch.device("cuda")
2024-12-30T09:35:15.573823639Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.573843790Z 
2024-12-30T09:35:15.573847443Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.573865670Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.573869117Z 
2024-12-30T09:35:15.574055614Z     def _lazy_init():
2024-12-30T09:35:15.574059161Z         global _initialized, _queued_calls
2024-12-30T09:35:15.574061215Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.574067550Z             return
2024-12-30T09:35:15.574074523Z         with _initialization_lock:
2024-12-30T09:35:15.574078364Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.574080536Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.574097471Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.574100849Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.574103218Z             # find there is nothing left to do.
2024-12-30T09:35:15.574105126Z             if is_initialized():
2024-12-30T09:35:15.574138536Z                 return
2024-12-30T09:35:15.574141700Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.574143924Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.574159588Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.574172791Z             if _is_in_bad_fork():
2024-12-30T09:35:15.574175008Z                 raise RuntimeError(
2024-12-30T09:35:15.574177883Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.574180031Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.574182211Z                 )
2024-12-30T09:35:15.574194957Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.574198189Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.574200270Z             if _cudart is None:
2024-12-30T09:35:15.574225941Z                 raise AssertionError(
2024-12-30T09:35:15.574229195Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.574231402Z                 )
2024-12-30T09:35:15.574233333Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.574256007Z             # are found or any other error occurs
2024-12-30T09:35:15.574259131Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.574276475Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.574279763Z >           torch._C._cuda_init()
2024-12-30T09:35:15.574281847Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.574301107Z 
2024-12-30T09:35:15.574304207Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.574482770Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-1-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.574487281Z 
2024-12-30T09:35:15.574489220Z b = 1, n = 257, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.574491223Z 
2024-12-30T09:35:15.574685328Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.574689916Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.574692108Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.574694131Z         torch.manual_seed(2024)
2024-12-30T09:35:15.574696060Z         atol = 5e-2
2024-12-30T09:35:15.574697959Z         rtol = 1e-2
2024-12-30T09:35:15.574713238Z         device = torch.device("cuda")
2024-12-30T09:35:15.574717028Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.574740796Z 
2024-12-30T09:35:15.574743645Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.574770000Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.574773423Z 
2024-12-30T09:35:15.575020021Z     def _lazy_init():
2024-12-30T09:35:15.575024223Z         global _initialized, _queued_calls
2024-12-30T09:35:15.575027289Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.575030633Z             return
2024-12-30T09:35:15.575033528Z         with _initialization_lock:
2024-12-30T09:35:15.575036722Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.575039518Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.575042442Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.575045486Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.575048109Z             # find there is nothing left to do.
2024-12-30T09:35:15.575050923Z             if is_initialized():
2024-12-30T09:35:15.575053986Z                 return
2024-12-30T09:35:15.575057827Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.575060932Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.575063021Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.575064965Z             if _is_in_bad_fork():
2024-12-30T09:35:15.575074116Z                 raise RuntimeError(
2024-12-30T09:35:15.575076877Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.575079076Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.575081075Z                 )
2024-12-30T09:35:15.575119910Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.575125326Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.575127444Z             if _cudart is None:
2024-12-30T09:35:15.575129382Z                 raise AssertionError(
2024-12-30T09:35:15.575131455Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.575144042Z                 )
2024-12-30T09:35:15.575147222Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.575149321Z             # are found or any other error occurs
2024-12-30T09:35:15.575158870Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.575161071Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.575178133Z >           torch._C._cuda_init()
2024-12-30T09:35:15.575213636Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.575217314Z 
2024-12-30T09:35:15.575219211Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.575381658Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.575385758Z 
2024-12-30T09:35:15.575436280Z b = 1, n = 257, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.575439786Z 
2024-12-30T09:35:15.575654189Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.575658613Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.575660714Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.575662720Z         torch.manual_seed(2024)
2024-12-30T09:35:15.575664655Z         atol = 5e-2
2024-12-30T09:35:15.575666620Z         rtol = 1e-2
2024-12-30T09:35:15.575668505Z         device = torch.device("cuda")
2024-12-30T09:35:15.575670461Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.575672601Z 
2024-12-30T09:35:15.575689213Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.575693388Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.575695759Z 
2024-12-30T09:35:15.575921696Z     def _lazy_init():
2024-12-30T09:35:15.575925233Z         global _initialized, _queued_calls
2024-12-30T09:35:15.575927168Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.575929316Z             return
2024-12-30T09:35:15.575931210Z         with _initialization_lock:
2024-12-30T09:35:15.575933250Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.575935245Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.575973185Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.575978250Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.575981459Z             # find there is nothing left to do.
2024-12-30T09:35:15.575984587Z             if is_initialized():
2024-12-30T09:35:15.575987600Z                 return
2024-12-30T09:35:15.575997844Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.576000232Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.576002225Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.576004248Z             if _is_in_bad_fork():
2024-12-30T09:35:15.576018630Z                 raise RuntimeError(
2024-12-30T09:35:15.576020954Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.576023051Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.576045431Z                 )
2024-12-30T09:35:15.576077555Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.576087946Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.576090245Z             if _cudart is None:
2024-12-30T09:35:15.576092228Z                 raise AssertionError(
2024-12-30T09:35:15.576095190Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.576097536Z                 )
2024-12-30T09:35:15.576124958Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.576129064Z             # are found or any other error occurs
2024-12-30T09:35:15.576131205Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.576136519Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.576139305Z >           torch._C._cuda_init()
2024-12-30T09:35:15.576166142Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.576172312Z 
2024-12-30T09:35:15.576177131Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.576344051Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.576347715Z 
2024-12-30T09:35:15.576356737Z b = 1, n = 1024, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.576369186Z 
2024-12-30T09:35:15.576573103Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.576577128Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.576579369Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.576581306Z         torch.manual_seed(2024)
2024-12-30T09:35:15.576586770Z         atol = 5e-2
2024-12-30T09:35:15.576593188Z         rtol = 1e-2
2024-12-30T09:35:15.576600190Z         device = torch.device("cuda")
2024-12-30T09:35:15.576612942Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.576616143Z 
2024-12-30T09:35:15.576665177Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.576674610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.576677398Z 
2024-12-30T09:35:15.576885081Z     def _lazy_init():
2024-12-30T09:35:15.576888841Z         global _initialized, _queued_calls
2024-12-30T09:35:15.576890850Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.576893096Z             return
2024-12-30T09:35:15.576895020Z         with _initialization_lock:
2024-12-30T09:35:15.576896937Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.576898942Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.576901003Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.576916953Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.576921298Z             # find there is nothing left to do.
2024-12-30T09:35:15.576930051Z             if is_initialized():
2024-12-30T09:35:15.576932223Z                 return
2024-12-30T09:35:15.576943708Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.576947011Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.576964632Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.576968005Z             if _is_in_bad_fork():
2024-12-30T09:35:15.576969945Z                 raise RuntimeError(
2024-12-30T09:35:15.576974953Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.576984402Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.577001905Z                 )
2024-12-30T09:35:15.577006159Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.577008386Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.577011278Z             if _cudart is None:
2024-12-30T09:35:15.577013434Z                 raise AssertionError(
2024-12-30T09:35:15.577050095Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.577054324Z                 )
2024-12-30T09:35:15.577056279Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.577058382Z             # are found or any other error occurs
2024-12-30T09:35:15.577071687Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.577075711Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.577077945Z >           torch._C._cuda_init()
2024-12-30T09:35:15.577143412Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.577147176Z 
2024-12-30T09:35:15.577149104Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.577273128Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.577276805Z 
2024-12-30T09:35:15.577304862Z b = 1, n = 1024, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.577308122Z 
2024-12-30T09:35:15.577511395Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.577515296Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.577517473Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.577523761Z         torch.manual_seed(2024)
2024-12-30T09:35:15.577525767Z         atol = 5e-2
2024-12-30T09:35:15.577527802Z         rtol = 1e-2
2024-12-30T09:35:15.577534353Z         device = torch.device("cuda")
2024-12-30T09:35:15.577539510Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.577541753Z 
2024-12-30T09:35:15.577615357Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.577619820Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.577622247Z 
2024-12-30T09:35:15.577785682Z     def _lazy_init():
2024-12-30T09:35:15.577788920Z         global _initialized, _queued_calls
2024-12-30T09:35:15.577790976Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.577793256Z             return
2024-12-30T09:35:15.577795158Z         with _initialization_lock:
2024-12-30T09:35:15.577798462Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.577817705Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.577821484Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.577840701Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.577843783Z             # find there is nothing left to do.
2024-12-30T09:35:15.577845756Z             if is_initialized():
2024-12-30T09:35:15.577881603Z                 return
2024-12-30T09:35:15.577884781Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.577886870Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.577888982Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.577890977Z             if _is_in_bad_fork():
2024-12-30T09:35:15.577913534Z                 raise RuntimeError(
2024-12-30T09:35:15.577916554Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.577919374Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.577921432Z                 )
2024-12-30T09:35:15.577928551Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.577936655Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.577946078Z             if _cudart is None:
2024-12-30T09:35:15.577948335Z                 raise AssertionError(
2024-12-30T09:35:15.578048762Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.578075079Z                 )
2024-12-30T09:35:15.578079026Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.578082365Z             # are found or any other error occurs
2024-12-30T09:35:15.578085942Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.578089596Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.578092748Z >           torch._C._cuda_init()
2024-12-30T09:35:15.578095025Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.578097736Z 
2024-12-30T09:35:15.578099757Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.578212338Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.578219622Z 
2024-12-30T09:35:15.578252097Z b = 1, n = 1025, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.578257317Z 
2024-12-30T09:35:15.578419196Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.578424453Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.578426751Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.578428793Z         torch.manual_seed(2024)
2024-12-30T09:35:15.578430723Z         atol = 5e-2
2024-12-30T09:35:15.578432726Z         rtol = 1e-2
2024-12-30T09:35:15.578438767Z         device = torch.device("cuda")
2024-12-30T09:35:15.578440872Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.578444699Z 
2024-12-30T09:35:15.578477641Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.578483305Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.578486116Z 
2024-12-30T09:35:15.578730080Z     def _lazy_init():
2024-12-30T09:35:15.578735337Z         global _initialized, _queued_calls
2024-12-30T09:35:15.578737466Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.578740136Z             return
2024-12-30T09:35:15.578742375Z         with _initialization_lock:
2024-12-30T09:35:15.578757807Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.578760259Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.578762222Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.578764311Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.578767806Z             # find there is nothing left to do.
2024-12-30T09:35:15.578769764Z             if is_initialized():
2024-12-30T09:35:15.578771681Z                 return
2024-12-30T09:35:15.578773810Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.578779540Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.578781743Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.578799701Z             if _is_in_bad_fork():
2024-12-30T09:35:15.578806328Z                 raise RuntimeError(
2024-12-30T09:35:15.578834664Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.578840009Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.578842249Z                 )
2024-12-30T09:35:15.578844450Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.578846670Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.578849292Z             if _cudart is None:
2024-12-30T09:35:15.578873225Z                 raise AssertionError(
2024-12-30T09:35:15.578881360Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.578883846Z                 )
2024-12-30T09:35:15.578885801Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.578897041Z             # are found or any other error occurs
2024-12-30T09:35:15.578899290Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.578901486Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.578947403Z >           torch._C._cuda_init()
2024-12-30T09:35:15.578950798Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.578954383Z 
2024-12-30T09:35:15.578956371Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.579115845Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-1-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.579127676Z 
2024-12-30T09:35:15.579141227Z b = 1, n = 1025, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.579147206Z 
2024-12-30T09:35:15.579327776Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.579346945Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.579350616Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.579352652Z         torch.manual_seed(2024)
2024-12-30T09:35:15.579354726Z         atol = 5e-2
2024-12-30T09:35:15.579356608Z         rtol = 1e-2
2024-12-30T09:35:15.579368713Z         device = torch.device("cuda")
2024-12-30T09:35:15.579372988Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.579375447Z 
2024-12-30T09:35:15.579377316Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.579399091Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.579404576Z 
2024-12-30T09:35:15.579599862Z     def _lazy_init():
2024-12-30T09:35:15.579606393Z         global _initialized, _queued_calls
2024-12-30T09:35:15.579608515Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.579610912Z             return
2024-12-30T09:35:15.579612973Z         with _initialization_lock:
2024-12-30T09:35:15.579614880Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.579617614Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.579620937Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.579623383Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.579688168Z             # find there is nothing left to do.
2024-12-30T09:35:15.579694189Z             if is_initialized():
2024-12-30T09:35:15.579696225Z                 return
2024-12-30T09:35:15.579698297Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.579700620Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.579702720Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.579704740Z             if _is_in_bad_fork():
2024-12-30T09:35:15.579706740Z                 raise RuntimeError(
2024-12-30T09:35:15.579708674Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.579722520Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.579724869Z                 )
2024-12-30T09:35:15.579726807Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.579728782Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.579730835Z             if _cudart is None:
2024-12-30T09:35:15.579768232Z                 raise AssertionError(
2024-12-30T09:35:15.579771377Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.579773876Z                 )
2024-12-30T09:35:15.579775771Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.579777764Z             # are found or any other error occurs
2024-12-30T09:35:15.579789193Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.579795217Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.579797467Z >           torch._C._cuda_init()
2024-12-30T09:35:15.579838243Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.579844606Z 
2024-12-30T09:35:15.579846535Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.579988201Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-2-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.579994492Z 
2024-12-30T09:35:15.580021008Z b = 2, n = 127, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.580026914Z 
2024-12-30T09:35:15.580187170Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.580192339Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.580195135Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.580201443Z         torch.manual_seed(2024)
2024-12-30T09:35:15.580204607Z         atol = 5e-2
2024-12-30T09:35:15.580206945Z         rtol = 1e-2
2024-12-30T09:35:15.580233688Z         device = torch.device("cuda")
2024-12-30T09:35:15.580240268Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.580242951Z 
2024-12-30T09:35:15.580268197Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.580273055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.580279594Z 
2024-12-30T09:35:15.580481776Z     def _lazy_init():
2024-12-30T09:35:15.580488214Z         global _initialized, _queued_calls
2024-12-30T09:35:15.580490331Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.580492759Z             return
2024-12-30T09:35:15.580494729Z         with _initialization_lock:
2024-12-30T09:35:15.580496693Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.580498762Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.580589234Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.580596146Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.580598589Z             # find there is nothing left to do.
2024-12-30T09:35:15.580600968Z             if is_initialized():
2024-12-30T09:35:15.580602995Z                 return
2024-12-30T09:35:15.580605416Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.580608052Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.580610058Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.580612036Z             if _is_in_bad_fork():
2024-12-30T09:35:15.580613947Z                 raise RuntimeError(
2024-12-30T09:35:15.580617169Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.580619413Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.580621498Z                 )
2024-12-30T09:35:15.580625297Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.580629751Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.580632446Z             if _cudart is None:
2024-12-30T09:35:15.580659788Z                 raise AssertionError(
2024-12-30T09:35:15.580665303Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.580667748Z                 )
2024-12-30T09:35:15.580694601Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.580698437Z             # are found or any other error occurs
2024-12-30T09:35:15.580700647Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.580702754Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.580704908Z >           torch._C._cuda_init()
2024-12-30T09:35:15.580730496Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.580737510Z 
2024-12-30T09:35:15.580756043Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.580899101Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.580904986Z 
2024-12-30T09:35:15.580928380Z b = 2, n = 127, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.580934544Z 
2024-12-30T09:35:15.581093729Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.581101226Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.581104517Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.581107376Z         torch.manual_seed(2024)
2024-12-30T09:35:15.581110268Z         atol = 5e-2
2024-12-30T09:35:15.581177443Z         rtol = 1e-2
2024-12-30T09:35:15.581182975Z         device = torch.device("cuda")
2024-12-30T09:35:15.581185016Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.581187442Z 
2024-12-30T09:35:15.581189236Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.581191316Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.581193732Z 
2024-12-30T09:35:15.581377219Z     def _lazy_init():
2024-12-30T09:35:15.581385620Z         global _initialized, _queued_calls
2024-12-30T09:35:15.581388180Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.581391009Z             return
2024-12-30T09:35:15.581393097Z         with _initialization_lock:
2024-12-30T09:35:15.581424593Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.581433625Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.581436896Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.581440410Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.581443541Z             # find there is nothing left to do.
2024-12-30T09:35:15.581450618Z             if is_initialized():
2024-12-30T09:35:15.581453552Z                 return
2024-12-30T09:35:15.581455507Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.581464466Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.581466697Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.581491310Z             if _is_in_bad_fork():
2024-12-30T09:35:15.581505342Z                 raise RuntimeError(
2024-12-30T09:35:15.581507953Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.581512902Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.581515588Z                 )
2024-12-30T09:35:15.581518022Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.581572428Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.581579149Z             if _cudart is None:
2024-12-30T09:35:15.581581292Z                 raise AssertionError(
2024-12-30T09:35:15.581583302Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.581585423Z                 )
2024-12-30T09:35:15.581587273Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.581589277Z             # are found or any other error occurs
2024-12-30T09:35:15.581592990Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.581596573Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.581598970Z >           torch._C._cuda_init()
2024-12-30T09:35:15.581647142Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.581652626Z 
2024-12-30T09:35:15.581655197Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.581812841Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-2-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.581819307Z 
2024-12-30T09:35:15.581829655Z b = 2, n = 128, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.581835477Z 
2024-12-30T09:35:15.582020961Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.582026653Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.582029144Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.582031039Z         torch.manual_seed(2024)
2024-12-30T09:35:15.582032897Z         atol = 5e-2
2024-12-30T09:35:15.582038422Z         rtol = 1e-2
2024-12-30T09:35:15.582040328Z         device = torch.device("cuda")
2024-12-30T09:35:15.582066122Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.582070321Z 
2024-12-30T09:35:15.582072401Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.582111350Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.582118381Z 
2024-12-30T09:35:15.582281922Z     def _lazy_init():
2024-12-30T09:35:15.582287424Z         global _initialized, _queued_calls
2024-12-30T09:35:15.582289417Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.582291890Z             return
2024-12-30T09:35:15.582293733Z         with _initialization_lock:
2024-12-30T09:35:15.582295865Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.582305661Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.582308177Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.582311840Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.582327730Z             # find there is nothing left to do.
2024-12-30T09:35:15.582332967Z             if is_initialized():
2024-12-30T09:35:15.582342472Z                 return
2024-12-30T09:35:15.582396098Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.582399565Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.582401554Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.582403628Z             if _is_in_bad_fork():
2024-12-30T09:35:15.582405531Z                 raise RuntimeError(
2024-12-30T09:35:15.582408932Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.582411038Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.582414213Z                 )
2024-12-30T09:35:15.582416237Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.582455179Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.582466344Z             if _cudart is None:
2024-12-30T09:35:15.582469426Z                 raise AssertionError(
2024-12-30T09:35:15.582472333Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.582484912Z                 )
2024-12-30T09:35:15.582509319Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.582512805Z             # are found or any other error occurs
2024-12-30T09:35:15.582517217Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.582520492Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.582523434Z >           torch._C._cuda_init()
2024-12-30T09:35:15.582526270Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.582538616Z 
2024-12-30T09:35:15.582542657Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.582730499Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.582737016Z 
2024-12-30T09:35:15.582749443Z b = 2, n = 128, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.582752679Z 
2024-12-30T09:35:15.582920163Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.582924682Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.582926941Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.582928970Z         torch.manual_seed(2024)
2024-12-30T09:35:15.582930957Z         atol = 5e-2
2024-12-30T09:35:15.582932949Z         rtol = 1e-2
2024-12-30T09:35:15.582956900Z         device = torch.device("cuda")
2024-12-30T09:35:15.582960150Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.582962303Z 
2024-12-30T09:35:15.582986659Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.582989393Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.582995195Z 
2024-12-30T09:35:15.583179198Z     def _lazy_init():
2024-12-30T09:35:15.583184884Z         global _initialized, _queued_calls
2024-12-30T09:35:15.583187154Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.583190106Z             return
2024-12-30T09:35:15.583206177Z         with _initialization_lock:
2024-12-30T09:35:15.583211098Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.583213206Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.583237586Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.583243723Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.583246129Z             # find there is nothing left to do.
2024-12-30T09:35:15.583248189Z             if is_initialized():
2024-12-30T09:35:15.583260039Z                 return
2024-12-30T09:35:15.583275678Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.583278037Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.583280081Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.583287495Z             if _is_in_bad_fork():
2024-12-30T09:35:15.583289567Z                 raise RuntimeError(
2024-12-30T09:35:15.583291583Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.583296375Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.583302275Z                 )
2024-12-30T09:35:15.583324084Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.583328447Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.583332356Z             if _cudart is None:
2024-12-30T09:35:15.583350250Z                 raise AssertionError(
2024-12-30T09:35:15.583395462Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.583403780Z                 )
2024-12-30T09:35:15.583406174Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.583408317Z             # are found or any other error occurs
2024-12-30T09:35:15.583410362Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.583412548Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.583414730Z >           torch._C._cuda_init()
2024-12-30T09:35:15.583445820Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.583454237Z 
2024-12-30T09:35:15.583457405Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.583603739Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-2-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.583610824Z 
2024-12-30T09:35:15.583638521Z b = 2, n = 256, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.583648344Z 
2024-12-30T09:35:15.583817248Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.583823261Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.583825573Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.583827690Z         torch.manual_seed(2024)
2024-12-30T09:35:15.583829711Z         atol = 5e-2
2024-12-30T09:35:15.583851684Z         rtol = 1e-2
2024-12-30T09:35:15.583857088Z         device = torch.device("cuda")
2024-12-30T09:35:15.583859275Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.583861296Z 
2024-12-30T09:35:15.583880997Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.583905216Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.583913138Z 
2024-12-30T09:35:15.584074316Z     def _lazy_init():
2024-12-30T09:35:15.584081442Z         global _initialized, _queued_calls
2024-12-30T09:35:15.584083576Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.584088922Z             return
2024-12-30T09:35:15.584091372Z         with _initialization_lock:
2024-12-30T09:35:15.584093391Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.584095990Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.584099718Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.584102130Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.584129065Z             # find there is nothing left to do.
2024-12-30T09:35:15.584134708Z             if is_initialized():
2024-12-30T09:35:15.584136843Z                 return
2024-12-30T09:35:15.584140648Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.584143163Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.584215778Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.584221002Z             if _is_in_bad_fork():
2024-12-30T09:35:15.584223228Z                 raise RuntimeError(
2024-12-30T09:35:15.584225152Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.584227390Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.584229796Z                 )
2024-12-30T09:35:15.584244989Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.584247157Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.584249276Z             if _cudart is None:
2024-12-30T09:35:15.584253190Z                 raise AssertionError(
2024-12-30T09:35:15.584255164Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.584257323Z                 )
2024-12-30T09:35:15.584259305Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.584281509Z             # are found or any other error occurs
2024-12-30T09:35:15.584287777Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.584290157Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.584292317Z >           torch._C._cuda_init()
2024-12-30T09:35:15.584324161Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.584329930Z 
2024-12-30T09:35:15.584332230Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.584494489Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.584500753Z 
2024-12-30T09:35:15.584516033Z b = 2, n = 256, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.584521057Z 
2024-12-30T09:35:15.584704456Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.584710742Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.584712988Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.584715926Z         torch.manual_seed(2024)
2024-12-30T09:35:15.584717895Z         atol = 5e-2
2024-12-30T09:35:15.584719948Z         rtol = 1e-2
2024-12-30T09:35:15.584732006Z         device = torch.device("cuda")
2024-12-30T09:35:15.584739041Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.584742196Z 
2024-12-30T09:35:15.584760942Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.584810262Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.584815879Z 
2024-12-30T09:35:15.584969945Z     def _lazy_init():
2024-12-30T09:35:15.584979719Z         global _initialized, _queued_calls
2024-12-30T09:35:15.584981992Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.584984887Z             return
2024-12-30T09:35:15.584986921Z         with _initialization_lock:
2024-12-30T09:35:15.584990174Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.584992178Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.585028916Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.585034796Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.585037229Z             # find there is nothing left to do.
2024-12-30T09:35:15.585039354Z             if is_initialized():
2024-12-30T09:35:15.585041381Z                 return
2024-12-30T09:35:15.585043490Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.585045595Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.585076478Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.585082486Z             if _is_in_bad_fork():
2024-12-30T09:35:15.585084503Z                 raise RuntimeError(
2024-12-30T09:35:15.585086695Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.585088813Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.585091302Z                 )
2024-12-30T09:35:15.585093940Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.585121336Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.585126245Z             if _cudart is None:
2024-12-30T09:35:15.585128168Z                 raise AssertionError(
2024-12-30T09:35:15.585130450Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.585132796Z                 )
2024-12-30T09:35:15.585181799Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.585189760Z             # are found or any other error occurs
2024-12-30T09:35:15.585191892Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.585193984Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.585196228Z >           torch._C._cuda_init()
2024-12-30T09:35:15.585198465Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.585206568Z 
2024-12-30T09:35:15.585233774Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.585369321Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-2-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.585378322Z 
2024-12-30T09:35:15.585395335Z b = 2, n = 257, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.585402649Z 
2024-12-30T09:35:15.585577883Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.585590186Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.585592802Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.585594842Z         torch.manual_seed(2024)
2024-12-30T09:35:15.585596823Z         atol = 5e-2
2024-12-30T09:35:15.585601622Z         rtol = 1e-2
2024-12-30T09:35:15.585603946Z         device = torch.device("cuda")
2024-12-30T09:35:15.585606051Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.585630872Z 
2024-12-30T09:35:15.585636685Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.585683023Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.585689632Z 
2024-12-30T09:35:15.585855464Z     def _lazy_init():
2024-12-30T09:35:15.585860645Z         global _initialized, _queued_calls
2024-12-30T09:35:15.585863009Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.585865434Z             return
2024-12-30T09:35:15.585873720Z         with _initialization_lock:
2024-12-30T09:35:15.585879402Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.585884066Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.585886231Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.585913147Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.585915829Z             # find there is nothing left to do.
2024-12-30T09:35:15.585917888Z             if is_initialized():
2024-12-30T09:35:15.585929941Z                 return
2024-12-30T09:35:15.585933763Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.585936678Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.585940496Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.585951233Z             if _is_in_bad_fork():
2024-12-30T09:35:15.585954665Z                 raise RuntimeError(
2024-12-30T09:35:15.585975228Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.585980875Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.585990881Z                 )
2024-12-30T09:35:15.585996295Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.586017968Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.586023310Z             if _cudart is None:
2024-12-30T09:35:15.586025337Z                 raise AssertionError(
2024-12-30T09:35:15.586064320Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.586070135Z                 )
2024-12-30T09:35:15.586072093Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.586074163Z             # are found or any other error occurs
2024-12-30T09:35:15.586076488Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.586078513Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.586081502Z >           torch._C._cuda_init()
2024-12-30T09:35:15.586097738Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.586100493Z 
2024-12-30T09:35:15.586145390Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.586362868Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.586376480Z 
2024-12-30T09:35:15.586378894Z b = 2, n = 257, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.586381006Z 
2024-12-30T09:35:15.586497403Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.586502499Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.586504984Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.586507492Z         torch.manual_seed(2024)
2024-12-30T09:35:15.586509483Z         atol = 5e-2
2024-12-30T09:35:15.586511501Z         rtol = 1e-2
2024-12-30T09:35:15.586513556Z         device = torch.device("cuda")
2024-12-30T09:35:15.586542466Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.586563801Z 
2024-12-30T09:35:15.586566623Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.586584820Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.586591285Z 
2024-12-30T09:35:15.586793725Z     def _lazy_init():
2024-12-30T09:35:15.586799441Z         global _initialized, _queued_calls
2024-12-30T09:35:15.586801554Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.586804132Z             return
2024-12-30T09:35:15.586806021Z         with _initialization_lock:
2024-12-30T09:35:15.586808027Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.586814309Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.586816569Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.586818538Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.586843178Z             # find there is nothing left to do.
2024-12-30T09:35:15.586848351Z             if is_initialized():
2024-12-30T09:35:15.586850611Z                 return
2024-12-30T09:35:15.586859213Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.586862013Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.586876019Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.586878421Z             if _is_in_bad_fork():
2024-12-30T09:35:15.586880316Z                 raise RuntimeError(
2024-12-30T09:35:15.586886394Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.586888962Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.586961368Z                 )
2024-12-30T09:35:15.586974417Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.586977252Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.586979646Z             if _cudart is None:
2024-12-30T09:35:15.586981596Z                 raise AssertionError(
2024-12-30T09:35:15.586983632Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.586986304Z                 )
2024-12-30T09:35:15.586991557Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.586993754Z             # are found or any other error occurs
2024-12-30T09:35:15.587007332Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.587009638Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.587012722Z >           torch._C._cuda_init()
2024-12-30T09:35:15.587040147Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.587047380Z 
2024-12-30T09:35:15.587049358Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.587205035Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.587211146Z 
2024-12-30T09:35:15.587234470Z b = 2, n = 1024, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.587240798Z 
2024-12-30T09:35:15.587459889Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.587465200Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.587467575Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.587469579Z         torch.manual_seed(2024)
2024-12-30T09:35:15.587471492Z         atol = 5e-2
2024-12-30T09:35:15.587473366Z         rtol = 1e-2
2024-12-30T09:35:15.587475344Z         device = torch.device("cuda")
2024-12-30T09:35:15.587477463Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.587479564Z 
2024-12-30T09:35:15.587482903Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.587544307Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.587557194Z 
2024-12-30T09:35:15.587701517Z     def _lazy_init():
2024-12-30T09:35:15.587706663Z         global _initialized, _queued_calls
2024-12-30T09:35:15.587709207Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.587711680Z             return
2024-12-30T09:35:15.587737057Z         with _initialization_lock:
2024-12-30T09:35:15.587742939Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.587745439Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.587747628Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.587780749Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.587794287Z             # find there is nothing left to do.
2024-12-30T09:35:15.587796487Z             if is_initialized():
2024-12-30T09:35:15.587798718Z                 return
2024-12-30T09:35:15.587800767Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.587802811Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.587806808Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.587808944Z             if _is_in_bad_fork():
2024-12-30T09:35:15.587810887Z                 raise RuntimeError(
2024-12-30T09:35:15.587883838Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.587890091Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.587892509Z                 )
2024-12-30T09:35:15.587894465Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.587896591Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.587898751Z             if _cudart is None:
2024-12-30T09:35:15.587900738Z                 raise AssertionError(
2024-12-30T09:35:15.587902620Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.587904709Z                 )
2024-12-30T09:35:15.587906565Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.587923385Z             # are found or any other error occurs
2024-12-30T09:35:15.587929969Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.587932442Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.587934642Z >           torch._C._cuda_init()
2024-12-30T09:35:15.587953251Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.587958812Z 
2024-12-30T09:35:15.587979174Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.588132904Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.588138651Z 
2024-12-30T09:35:15.588160805Z b = 2, n = 1024, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.588163421Z 
2024-12-30T09:35:15.588318508Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.588321225Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.588328173Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.588330235Z         torch.manual_seed(2024)
2024-12-30T09:35:15.588346184Z         atol = 5e-2
2024-12-30T09:35:15.588352842Z         rtol = 1e-2
2024-12-30T09:35:15.588356203Z         device = torch.device("cuda")
2024-12-30T09:35:15.588374903Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.588378612Z 
2024-12-30T09:35:15.588416882Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.588421418Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.588424057Z 
2024-12-30T09:35:15.588620175Z     def _lazy_init():
2024-12-30T09:35:15.588624707Z         global _initialized, _queued_calls
2024-12-30T09:35:15.588626879Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.588629299Z             return
2024-12-30T09:35:15.588631258Z         with _initialization_lock:
2024-12-30T09:35:15.588633263Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.588651245Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.588656265Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.588658456Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.588660533Z             # find there is nothing left to do.
2024-12-30T09:35:15.588663528Z             if is_initialized():
2024-12-30T09:35:15.588666289Z                 return
2024-12-30T09:35:15.588674356Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.588718245Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.588724178Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.588726694Z             if _is_in_bad_fork():
2024-12-30T09:35:15.588729157Z                 raise RuntimeError(
2024-12-30T09:35:15.588732095Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.588734494Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.588737380Z                 )
2024-12-30T09:35:15.588749981Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.588786064Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.588798836Z             if _cudart is None:
2024-12-30T09:35:15.588801054Z                 raise AssertionError(
2024-12-30T09:35:15.588804519Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.588806886Z                 )
2024-12-30T09:35:15.588808787Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.588811263Z             # are found or any other error occurs
2024-12-30T09:35:15.588813913Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.588816794Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.588821155Z >           torch._C._cuda_init()
2024-12-30T09:35:15.588863390Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.588869002Z 
2024-12-30T09:35:15.588871097Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.589033166Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.589039859Z 
2024-12-30T09:35:15.589053120Z b = 2, n = 1025, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.589058923Z 
2024-12-30T09:35:15.589217009Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.589224531Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.589228150Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.589231169Z         torch.manual_seed(2024)
2024-12-30T09:35:15.589235456Z         atol = 5e-2
2024-12-30T09:35:15.589238985Z         rtol = 1e-2
2024-12-30T09:35:15.589243057Z         device = torch.device("cuda")
2024-12-30T09:35:15.589246376Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.589249317Z 
2024-12-30T09:35:15.589301822Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.589308009Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.589310851Z 
2024-12-30T09:35:15.589504006Z     def _lazy_init():
2024-12-30T09:35:15.589509367Z         global _initialized, _queued_calls
2024-12-30T09:35:15.589511538Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.589521233Z             return
2024-12-30T09:35:15.589525255Z         with _initialization_lock:
2024-12-30T09:35:15.589527371Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.589529586Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.589531567Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.589565742Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.589572932Z             # find there is nothing left to do.
2024-12-30T09:35:15.589575278Z             if is_initialized():
2024-12-30T09:35:15.589577345Z                 return
2024-12-30T09:35:15.589606967Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.589610289Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.589613636Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.589616912Z             if _is_in_bad_fork():
2024-12-30T09:35:15.589619918Z                 raise RuntimeError(
2024-12-30T09:35:15.589643663Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.589649352Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.589651651Z                 )
2024-12-30T09:35:15.589670893Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.589678610Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.589682104Z             if _cudart is None:
2024-12-30T09:35:15.589693841Z                 raise AssertionError(
2024-12-30T09:35:15.589697562Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.589699920Z                 )
2024-12-30T09:35:15.589701979Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.589724471Z             # are found or any other error occurs
2024-12-30T09:35:15.589732317Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.589735879Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.589738885Z >           torch._C._cuda_init()
2024-12-30T09:35:15.589776173Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.589782228Z 
2024-12-30T09:35:15.589784205Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.589938318Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-2-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.589945185Z 
2024-12-30T09:35:15.589962513Z b = 2, n = 1025, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.589967421Z 
2024-12-30T09:35:15.590140598Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.590145814Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.590148048Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.590150089Z         torch.manual_seed(2024)
2024-12-30T09:35:15.590152125Z         atol = 5e-2
2024-12-30T09:35:15.590154233Z         rtol = 1e-2
2024-12-30T09:35:15.590157323Z         device = torch.device("cuda")
2024-12-30T09:35:15.590159510Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.590175595Z 
2024-12-30T09:35:15.590191299Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.590235425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.590239308Z 
2024-12-30T09:35:15.590410915Z     def _lazy_init():
2024-12-30T09:35:15.590414184Z         global _initialized, _queued_calls
2024-12-30T09:35:15.590416374Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.590418691Z             return
2024-12-30T09:35:15.590434325Z         with _initialization_lock:
2024-12-30T09:35:15.590438587Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.590440817Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.590442933Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.590454463Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.590460158Z             # find there is nothing left to do.
2024-12-30T09:35:15.590520610Z             if is_initialized():
2024-12-30T09:35:15.590524024Z                 return
2024-12-30T09:35:15.590526485Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.590529637Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.590531633Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.590534026Z             if _is_in_bad_fork():
2024-12-30T09:35:15.590536319Z                 raise RuntimeError(
2024-12-30T09:35:15.590542117Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.590545516Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.590547580Z                 )
2024-12-30T09:35:15.590589001Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.590596633Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.590598914Z             if _cudart is None:
2024-12-30T09:35:15.590600929Z                 raise AssertionError(
2024-12-30T09:35:15.590602952Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.590605126Z                 )
2024-12-30T09:35:15.590608543Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.590611051Z             # are found or any other error occurs
2024-12-30T09:35:15.590626532Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.590632217Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.590653357Z >           torch._C._cuda_init()
2024-12-30T09:35:15.590658209Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.590660786Z 
2024-12-30T09:35:15.590697856Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.590857506Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-4-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.590862072Z 
2024-12-30T09:35:15.590864124Z b = 4, n = 127, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.590867013Z 
2024-12-30T09:35:15.591046992Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.591055245Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.591058069Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.591060053Z         torch.manual_seed(2024)
2024-12-30T09:35:15.591062162Z         atol = 5e-2
2024-12-30T09:35:15.591064070Z         rtol = 1e-2
2024-12-30T09:35:15.591068048Z         device = torch.device("cuda")
2024-12-30T09:35:15.591070334Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.591072771Z 
2024-12-30T09:35:15.591123332Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.591139091Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.591141765Z 
2024-12-30T09:35:15.591319979Z     def _lazy_init():
2024-12-30T09:35:15.591326236Z         global _initialized, _queued_calls
2024-12-30T09:35:15.591329076Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.591332117Z             return
2024-12-30T09:35:15.591334074Z         with _initialization_lock:
2024-12-30T09:35:15.591336073Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.591338187Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.591374148Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.591380123Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.591382537Z             # find there is nothing left to do.
2024-12-30T09:35:15.591384674Z             if is_initialized():
2024-12-30T09:35:15.591386685Z                 return
2024-12-30T09:35:15.591412965Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.591418694Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.591421010Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.591423151Z             if _is_in_bad_fork():
2024-12-30T09:35:15.591425206Z                 raise RuntimeError(
2024-12-30T09:35:15.591441143Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.591449172Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.591452233Z                 )
2024-12-30T09:35:15.591466886Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.591471360Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.591473629Z             if _cudart is None:
2024-12-30T09:35:15.591476080Z                 raise AssertionError(
2024-12-30T09:35:15.591568984Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.591575724Z                 )
2024-12-30T09:35:15.591578570Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.591581608Z             # are found or any other error occurs
2024-12-30T09:35:15.591584402Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.591587414Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.591595611Z >           torch._C._cuda_init()
2024-12-30T09:35:15.591598722Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.591602371Z 
2024-12-30T09:35:15.591605467Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.591758138Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.591767191Z 
2024-12-30T09:35:15.591769348Z b = 4, n = 127, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.591771346Z 
2024-12-30T09:35:15.591937194Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.591942316Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.591944517Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.591946488Z         torch.manual_seed(2024)
2024-12-30T09:35:15.591948378Z         atol = 5e-2
2024-12-30T09:35:15.591950394Z         rtol = 1e-2
2024-12-30T09:35:15.591969021Z         device = torch.device("cuda")
2024-12-30T09:35:15.591972919Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.591975033Z 
2024-12-30T09:35:15.592022501Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.592028542Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.592032415Z 
2024-12-30T09:35:15.592200663Z     def _lazy_init():
2024-12-30T09:35:15.592205106Z         global _initialized, _queued_calls
2024-12-30T09:35:15.592207196Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.592209406Z             return
2024-12-30T09:35:15.592211370Z         with _initialization_lock:
2024-12-30T09:35:15.592213311Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.592248601Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.592262350Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.592266656Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.592270714Z             # find there is nothing left to do.
2024-12-30T09:35:15.592274507Z             if is_initialized():
2024-12-30T09:35:15.592290043Z                 return
2024-12-30T09:35:15.592293330Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.592296371Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.592305988Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.592309496Z             if _is_in_bad_fork():
2024-12-30T09:35:15.592312737Z                 raise RuntimeError(
2024-12-30T09:35:15.592316055Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.592320675Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.592324355Z                 )
2024-12-30T09:35:15.592328189Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.592332992Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.592336692Z             if _cudart is None:
2024-12-30T09:35:15.592351407Z                 raise AssertionError(
2024-12-30T09:35:15.592355711Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.592395431Z                 )
2024-12-30T09:35:15.592400944Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.592404599Z             # are found or any other error occurs
2024-12-30T09:35:15.592408025Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.592411574Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.592416826Z >           torch._C._cuda_init()
2024-12-30T09:35:15.592420606Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.592464713Z 
2024-12-30T09:35:15.592469377Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.592632470Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-4-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.592640282Z 
2024-12-30T09:35:15.592643705Z b = 4, n = 128, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.592647387Z 
2024-12-30T09:35:15.592817798Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.592821215Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.592823532Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.592825513Z         torch.manual_seed(2024)
2024-12-30T09:35:15.592834454Z         atol = 5e-2
2024-12-30T09:35:15.592837214Z         rtol = 1e-2
2024-12-30T09:35:15.592839221Z         device = torch.device("cuda")
2024-12-30T09:35:15.592852515Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.592859016Z 
2024-12-30T09:35:15.592907308Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.592912827Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.592916794Z 
2024-12-30T09:35:15.593114706Z     def _lazy_init():
2024-12-30T09:35:15.593120361Z         global _initialized, _queued_calls
2024-12-30T09:35:15.593124019Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.593127923Z             return
2024-12-30T09:35:15.593131179Z         with _initialization_lock:
2024-12-30T09:35:15.593134700Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.593138076Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.593141580Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.593144768Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.593147884Z             # find there is nothing left to do.
2024-12-30T09:35:15.593151899Z             if is_initialized():
2024-12-30T09:35:15.593155210Z                 return
2024-12-30T09:35:15.593158330Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.593162494Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.593165845Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.593169124Z             if _is_in_bad_fork():
2024-12-30T09:35:15.593201308Z                 raise RuntimeError(
2024-12-30T09:35:15.593206391Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.593208751Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.593230874Z                 )
2024-12-30T09:35:15.593235125Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.593237336Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.593239450Z             if _cudart is None:
2024-12-30T09:35:15.593241505Z                 raise AssertionError(
2024-12-30T09:35:15.593262045Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.593280153Z                 )
2024-12-30T09:35:15.593284177Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.593288052Z             # are found or any other error occurs
2024-12-30T09:35:15.593291053Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.593294167Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.593297288Z >           torch._C._cuda_init()
2024-12-30T09:35:15.593372477Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.593376573Z 
2024-12-30T09:35:15.593378586Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.593511525Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.593516251Z 
2024-12-30T09:35:15.593518492Z b = 4, n = 128, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.593520517Z 
2024-12-30T09:35:15.593708925Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.593713299Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.593715570Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.593717600Z         torch.manual_seed(2024)
2024-12-30T09:35:15.593720266Z         atol = 5e-2
2024-12-30T09:35:15.593722274Z         rtol = 1e-2
2024-12-30T09:35:15.593749312Z         device = torch.device("cuda")
2024-12-30T09:35:15.593752343Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.593754465Z 
2024-12-30T09:35:15.593780413Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.593802435Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.593806585Z 
2024-12-30T09:35:15.593996290Z     def _lazy_init():
2024-12-30T09:35:15.593999951Z         global _initialized, _queued_calls
2024-12-30T09:35:15.594001914Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.594004134Z             return
2024-12-30T09:35:15.594006005Z         with _initialization_lock:
2024-12-30T09:35:15.594007925Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.594015750Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.594029313Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.594032861Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.594034888Z             # find there is nothing left to do.
2024-12-30T09:35:15.594036843Z             if is_initialized():
2024-12-30T09:35:15.594065758Z                 return
2024-12-30T09:35:15.594070404Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.594073712Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.594076923Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.594091263Z             if _is_in_bad_fork():
2024-12-30T09:35:15.594094149Z                 raise RuntimeError(
2024-12-30T09:35:15.594096105Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.594098211Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.594123354Z                 )
2024-12-30T09:35:15.594126134Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.594128263Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.594130913Z             if _cudart is None:
2024-12-30T09:35:15.594188050Z                 raise AssertionError(
2024-12-30T09:35:15.594191618Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.594193921Z                 )
2024-12-30T09:35:15.594195867Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.594197886Z             # are found or any other error occurs
2024-12-30T09:35:15.594199921Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.594201952Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.594214047Z >           torch._C._cuda_init()
2024-12-30T09:35:15.594217190Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.594238842Z 
2024-12-30T09:35:15.594241731Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.594428070Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-4-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.594438993Z 
2024-12-30T09:35:15.594442015Z b = 4, n = 256, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.594444914Z 
2024-12-30T09:35:15.594621628Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.594626047Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.594628347Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.594630276Z         torch.manual_seed(2024)
2024-12-30T09:35:15.594632261Z         atol = 5e-2
2024-12-30T09:35:15.594652344Z         rtol = 1e-2
2024-12-30T09:35:15.594655717Z         device = torch.device("cuda")
2024-12-30T09:35:15.594657726Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.594659827Z 
2024-12-30T09:35:15.594721857Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.594733587Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.594736471Z 
2024-12-30T09:35:15.594895569Z     def _lazy_init():
2024-12-30T09:35:15.594898724Z         global _initialized, _queued_calls
2024-12-30T09:35:15.594900638Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.594902732Z             return
2024-12-30T09:35:15.594904657Z         with _initialization_lock:
2024-12-30T09:35:15.594914515Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.594916696Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.594923253Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.594925467Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.594944589Z             # find there is nothing left to do.
2024-12-30T09:35:15.594946968Z             if is_initialized():
2024-12-30T09:35:15.594966664Z                 return
2024-12-30T09:35:15.594969097Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.594971270Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.594994232Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.594996801Z             if _is_in_bad_fork():
2024-12-30T09:35:15.594998811Z                 raise RuntimeError(
2024-12-30T09:35:15.595011376Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.595015241Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.595032035Z                 )
2024-12-30T09:35:15.595035096Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.595037520Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.595063553Z             if _cudart is None:
2024-12-30T09:35:15.595066294Z                 raise AssertionError(
2024-12-30T09:35:15.595068417Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.595078188Z                 )
2024-12-30T09:35:15.595081673Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.595104968Z             # are found or any other error occurs
2024-12-30T09:35:15.595108072Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.595110227Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.595112433Z >           torch._C._cuda_init()
2024-12-30T09:35:15.595155542Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.595160231Z 
2024-12-30T09:35:15.595162123Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.595315030Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.595318584Z 
2024-12-30T09:35:15.595335452Z b = 4, n = 256, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.595338649Z 
2024-12-30T09:35:15.595530606Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.595534604Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.595536816Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.595538811Z         torch.manual_seed(2024)
2024-12-30T09:35:15.595540787Z         atol = 5e-2
2024-12-30T09:35:15.595542647Z         rtol = 1e-2
2024-12-30T09:35:15.595545238Z         device = torch.device("cuda")
2024-12-30T09:35:15.595547209Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.595576529Z 
2024-12-30T09:35:15.595580031Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.595627811Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.595635052Z 
2024-12-30T09:35:15.595820028Z     def _lazy_init():
2024-12-30T09:35:15.595823302Z         global _initialized, _queued_calls
2024-12-30T09:35:15.595825267Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.595827439Z             return
2024-12-30T09:35:15.595836379Z         with _initialization_lock:
2024-12-30T09:35:15.595839478Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.595841525Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.595862707Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.595865360Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.595867333Z             # find there is nothing left to do.
2024-12-30T09:35:15.595869246Z             if is_initialized():
2024-12-30T09:35:15.595871906Z                 return
2024-12-30T09:35:15.595875218Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.595907487Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.595912002Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.595914095Z             if _is_in_bad_fork():
2024-12-30T09:35:15.595916060Z                 raise RuntimeError(
2024-12-30T09:35:15.595921819Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.595924156Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.595968444Z                 )
2024-12-30T09:35:15.595971598Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.595973698Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.595975829Z             if _cudart is None:
2024-12-30T09:35:15.595978636Z                 raise AssertionError(
2024-12-30T09:35:15.595980616Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.595983582Z                 )
2024-12-30T09:35:15.595987184Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.596027091Z             # are found or any other error occurs
2024-12-30T09:35:15.596032154Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.596035170Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.596038004Z >           torch._C._cuda_init()
2024-12-30T09:35:15.596062123Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.596068364Z 
2024-12-30T09:35:15.596070182Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.596246596Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-4-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.596251646Z 
2024-12-30T09:35:15.596275605Z b = 4, n = 257, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.596278904Z 
2024-12-30T09:35:15.596472350Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.596475709Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.596477827Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.596479755Z         torch.manual_seed(2024)
2024-12-30T09:35:15.596481638Z         atol = 5e-2
2024-12-30T09:35:15.596483556Z         rtol = 1e-2
2024-12-30T09:35:15.596485498Z         device = torch.device("cuda")
2024-12-30T09:35:15.596494008Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.596496322Z 
2024-12-30T09:35:15.596543045Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.596545984Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.596548351Z 
2024-12-30T09:35:15.596745477Z     def _lazy_init():
2024-12-30T09:35:15.596749318Z         global _initialized, _queued_calls
2024-12-30T09:35:15.596751291Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.596753403Z             return
2024-12-30T09:35:15.596791161Z         with _initialization_lock:
2024-12-30T09:35:15.596794349Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.596796499Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.596798627Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.596800611Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.596820527Z             # find there is nothing left to do.
2024-12-30T09:35:15.596823628Z             if is_initialized():
2024-12-30T09:35:15.596841265Z                 return
2024-12-30T09:35:15.596844381Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.596872386Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.596881050Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.596883438Z             if _is_in_bad_fork():
2024-12-30T09:35:15.596920110Z                 raise RuntimeError(
2024-12-30T09:35:15.596923654Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.596925869Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.596927948Z                 )
2024-12-30T09:35:15.596929802Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.596951640Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.596955011Z             if _cudart is None:
2024-12-30T09:35:15.596957028Z                 raise AssertionError(
2024-12-30T09:35:15.596958950Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.596980456Z                 )
2024-12-30T09:35:15.596983425Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.596985617Z             # are found or any other error occurs
2024-12-30T09:35:15.596987609Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.597010559Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.597014356Z >           torch._C._cuda_init()
2024-12-30T09:35:15.597028008Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.597031389Z 
2024-12-30T09:35:15.597077400Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.597221360Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.597225292Z 
2024-12-30T09:35:15.597251634Z b = 4, n = 257, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.597254854Z 
2024-12-30T09:35:15.597482631Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.597488014Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.597490879Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.597493508Z         torch.manual_seed(2024)
2024-12-30T09:35:15.597502426Z         atol = 5e-2
2024-12-30T09:35:15.597505843Z         rtol = 1e-2
2024-12-30T09:35:15.597507763Z         device = torch.device("cuda")
2024-12-30T09:35:15.597513596Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.597515792Z 
2024-12-30T09:35:15.597544846Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.597573533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.597577340Z 
2024-12-30T09:35:15.597760848Z     def _lazy_init():
2024-12-30T09:35:15.597767795Z         global _initialized, _queued_calls
2024-12-30T09:35:15.597769878Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.597772237Z             return
2024-12-30T09:35:15.597781576Z         with _initialization_lock:
2024-12-30T09:35:15.597786703Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.597789129Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.597804948Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.597808830Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.597811790Z             # find there is nothing left to do.
2024-12-30T09:35:15.597834841Z             if is_initialized():
2024-12-30T09:35:15.597840767Z                 return
2024-12-30T09:35:15.597842837Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.597845129Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.597848506Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.597850642Z             if _is_in_bad_fork():
2024-12-30T09:35:15.597853435Z                 raise RuntimeError(
2024-12-30T09:35:15.597877630Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.597881552Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.597884590Z                 )
2024-12-30T09:35:15.597919722Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.597922625Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.597924756Z             if _cudart is None:
2024-12-30T09:35:15.597927564Z                 raise AssertionError(
2024-12-30T09:35:15.597929557Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.597932439Z                 )
2024-12-30T09:35:15.597935201Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.597943666Z             # are found or any other error occurs
2024-12-30T09:35:15.597959082Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.597962627Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.597969552Z >           torch._C._cuda_init()
2024-12-30T09:35:15.598004391Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.598010236Z 
2024-12-30T09:35:15.598013117Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.598177557Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.598181602Z 
2024-12-30T09:35:15.598199404Z b = 4, n = 1024, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.598201811Z 
2024-12-30T09:35:15.598389324Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.598393450Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.598395691Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.598397614Z         torch.manual_seed(2024)
2024-12-30T09:35:15.598399549Z         atol = 5e-2
2024-12-30T09:35:15.598401581Z         rtol = 1e-2
2024-12-30T09:35:15.598403435Z         device = torch.device("cuda")
2024-12-30T09:35:15.598422140Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.598427054Z 
2024-12-30T09:35:15.598460676Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.598463981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.598466479Z 
2024-12-30T09:35:15.598703783Z     def _lazy_init():
2024-12-30T09:35:15.598709527Z         global _initialized, _queued_calls
2024-12-30T09:35:15.598711569Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.598713673Z             return
2024-12-30T09:35:15.598715705Z         with _initialization_lock:
2024-12-30T09:35:15.598717841Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.598719984Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.598721966Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.598728542Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.598731612Z             # find there is nothing left to do.
2024-12-30T09:35:15.598733673Z             if is_initialized():
2024-12-30T09:35:15.598735792Z                 return
2024-12-30T09:35:15.598738500Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.598740650Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.598756352Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.598759910Z             if _is_in_bad_fork():
2024-12-30T09:35:15.598792199Z                 raise RuntimeError(
2024-12-30T09:35:15.598795511Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.598797575Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.598799623Z                 )
2024-12-30T09:35:15.598801534Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.598819270Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.598822643Z             if _cudart is None:
2024-12-30T09:35:15.598824579Z                 raise AssertionError(
2024-12-30T09:35:15.598835495Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.598854532Z                 )
2024-12-30T09:35:15.598857623Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.598860003Z             # are found or any other error occurs
2024-12-30T09:35:15.598866493Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.598916286Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.598927841Z >           torch._C._cuda_init()
2024-12-30T09:35:15.598930350Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.598933501Z 
2024-12-30T09:35:15.598935435Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.599102312Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.599108433Z 
2024-12-30T09:35:15.599110477Z b = 4, n = 1024, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.599112533Z 
2024-12-30T09:35:15.599281997Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.599288094Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.599290643Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.599292788Z         torch.manual_seed(2024)
2024-12-30T09:35:15.599294844Z         atol = 5e-2
2024-12-30T09:35:15.599296835Z         rtol = 1e-2
2024-12-30T09:35:15.599327250Z         device = torch.device("cuda")
2024-12-30T09:35:15.599333348Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.599335852Z 
2024-12-30T09:35:15.599337821Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.599382004Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.599388104Z 
2024-12-30T09:35:15.599586135Z     def _lazy_init():
2024-12-30T09:35:15.599594357Z         global _initialized, _queued_calls
2024-12-30T09:35:15.599596362Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.599598770Z             return
2024-12-30T09:35:15.599600682Z         with _initialization_lock:
2024-12-30T09:35:15.599602670Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.599604926Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.599607151Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.599610116Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.599612121Z             # find there is nothing left to do.
2024-12-30T09:35:15.599644551Z             if is_initialized():
2024-12-30T09:35:15.599648067Z                 return
2024-12-30T09:35:15.599649940Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.599652292Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.599654463Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.599656418Z             if _is_in_bad_fork():
2024-12-30T09:35:15.599659235Z                 raise RuntimeError(
2024-12-30T09:35:15.599682678Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.599686387Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.599690895Z                 )
2024-12-30T09:35:15.599692993Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.599699395Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.599701812Z             if _cudart is None:
2024-12-30T09:35:15.599705705Z                 raise AssertionError(
2024-12-30T09:35:15.599722190Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.599729630Z                 )
2024-12-30T09:35:15.599733641Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.599735938Z             # are found or any other error occurs
2024-12-30T09:35:15.599742526Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.599746475Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.599756950Z >           torch._C._cuda_init()
2024-12-30T09:35:15.599779039Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.599782862Z 
2024-12-30T09:35:15.599817398Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.599959417Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.599963328Z 
2024-12-30T09:35:15.599990148Z b = 4, n = 1025, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.599993135Z 
2024-12-30T09:35:15.600187052Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.600190697Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.600193401Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.600195375Z         torch.manual_seed(2024)
2024-12-30T09:35:15.600197260Z         atol = 5e-2
2024-12-30T09:35:15.600199152Z         rtol = 1e-2
2024-12-30T09:35:15.600201076Z         device = torch.device("cuda")
2024-12-30T09:35:15.600203199Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.600205790Z 
2024-12-30T09:35:15.600207671Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.600271602Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.600276543Z 
2024-12-30T09:35:15.600463621Z     def _lazy_init():
2024-12-30T09:35:15.600468751Z         global _initialized, _queued_calls
2024-12-30T09:35:15.600476524Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.600479445Z             return
2024-12-30T09:35:15.600482150Z         with _initialization_lock:
2024-12-30T09:35:15.600485020Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.600487906Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.600492704Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.600495639Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.600498618Z             # find there is nothing left to do.
2024-12-30T09:35:15.600501536Z             if is_initialized():
2024-12-30T09:35:15.600505167Z                 return
2024-12-30T09:35:15.600507889Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.600528640Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.600533127Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.600536501Z             if _is_in_bad_fork():
2024-12-30T09:35:15.600566801Z                 raise RuntimeError(
2024-12-30T09:35:15.600572181Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.600575101Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.600604370Z                 )
2024-12-30T09:35:15.600607976Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.600610924Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.600613971Z             if _cudart is None:
2024-12-30T09:35:15.600616739Z                 raise AssertionError(
2024-12-30T09:35:15.600638520Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.600646549Z                 )
2024-12-30T09:35:15.600648963Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.600652120Z             # are found or any other error occurs
2024-12-30T09:35:15.600654213Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.600664693Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.600668341Z >           torch._C._cuda_init()
2024-12-30T09:35:15.600713260Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.600717142Z 
2024-12-30T09:35:15.600719094Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.600866552Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-4-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.600870272Z 
2024-12-30T09:35:15.600904217Z b = 4, n = 1025, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.600907903Z 
2024-12-30T09:35:15.601068308Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.601071445Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.601073681Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.601075756Z         torch.manual_seed(2024)
2024-12-30T09:35:15.601098776Z         atol = 5e-2
2024-12-30T09:35:15.601101778Z         rtol = 1e-2
2024-12-30T09:35:15.601103728Z         device = torch.device("cuda")
2024-12-30T09:35:15.601105861Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.601108484Z 
2024-12-30T09:35:15.601133704Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.601159528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.601162943Z 
2024-12-30T09:35:15.601348307Z     def _lazy_init():
2024-12-30T09:35:15.601351634Z         global _initialized, _queued_calls
2024-12-30T09:35:15.601353578Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.601355785Z             return
2024-12-30T09:35:15.601365806Z         with _initialization_lock:
2024-12-30T09:35:15.601369270Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.601385488Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.601388809Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.601423338Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.601427271Z             # find there is nothing left to do.
2024-12-30T09:35:15.601430222Z             if is_initialized():
2024-12-30T09:35:15.601432950Z                 return
2024-12-30T09:35:15.601435576Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.601442326Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.601447468Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.601455565Z             if _is_in_bad_fork():
2024-12-30T09:35:15.601461761Z                 raise RuntimeError(
2024-12-30T09:35:15.601463952Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.601466123Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.601478485Z                 )
2024-12-30T09:35:15.601506144Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.601511033Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.601514082Z             if _cudart is None:
2024-12-30T09:35:15.601534535Z                 raise AssertionError(
2024-12-30T09:35:15.601545697Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.601557841Z                 )
2024-12-30T09:35:15.601560025Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.601563231Z             # are found or any other error occurs
2024-12-30T09:35:15.601565419Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.601586924Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.601592031Z >           torch._C._cuda_init()
2024-12-30T09:35:15.601628516Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.601633981Z 
2024-12-30T09:35:15.601636817Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.601786049Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-8-127-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.601791603Z 
2024-12-30T09:35:15.601810970Z b = 8, n = 127, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.601814337Z 
2024-12-30T09:35:15.601992332Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.601997210Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.602000712Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.602003906Z         torch.manual_seed(2024)
2024-12-30T09:35:15.602012315Z         atol = 5e-2
2024-12-30T09:35:15.602015397Z         rtol = 1e-2
2024-12-30T09:35:15.602039528Z         device = torch.device("cuda")
2024-12-30T09:35:15.602042677Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.602044972Z 
2024-12-30T09:35:15.602065321Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.602068162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.602070671Z 
2024-12-30T09:35:15.602284751Z     def _lazy_init():
2024-12-30T09:35:15.602288803Z         global _initialized, _queued_calls
2024-12-30T09:35:15.602290889Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.602293225Z             return
2024-12-30T09:35:15.602295120Z         with _initialization_lock:
2024-12-30T09:35:15.602297517Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.602300955Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.602304164Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.602333204Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.602336467Z             # find there is nothing left to do.
2024-12-30T09:35:15.602338531Z             if is_initialized():
2024-12-30T09:35:15.602340519Z                 return
2024-12-30T09:35:15.602342398Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.602344536Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.602390085Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.602396280Z             if _is_in_bad_fork():
2024-12-30T09:35:15.602399199Z                 raise RuntimeError(
2024-12-30T09:35:15.602402072Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.602405230Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.602408293Z                 )
2024-12-30T09:35:15.602421312Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.602424607Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.602427473Z             if _cudart is None:
2024-12-30T09:35:15.602441062Z                 raise AssertionError(
2024-12-30T09:35:15.602464118Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.602467537Z                 )
2024-12-30T09:35:15.602470168Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.602472993Z             # are found or any other error occurs
2024-12-30T09:35:15.602475825Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.602505182Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.602515903Z >           torch._C._cuda_init()
2024-12-30T09:35:15.602518411Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.602520942Z 
2024-12-30T09:35:15.602541537Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.602692523Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-127-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.602700316Z 
2024-12-30T09:35:15.602714245Z b = 8, n = 127, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.602718352Z 
2024-12-30T09:35:15.602885061Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.602888181Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.602890493Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.602892500Z         torch.manual_seed(2024)
2024-12-30T09:35:15.602906392Z         atol = 5e-2
2024-12-30T09:35:15.602909101Z         rtol = 1e-2
2024-12-30T09:35:15.602911750Z         device = torch.device("cuda")
2024-12-30T09:35:15.602920655Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.602924238Z 
2024-12-30T09:35:15.602997356Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.603000431Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.603002772Z 
2024-12-30T09:35:15.603157924Z     def _lazy_init():
2024-12-30T09:35:15.603160691Z         global _initialized, _queued_calls
2024-12-30T09:35:15.603162760Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.603165062Z             return
2024-12-30T09:35:15.603167196Z         with _initialization_lock:
2024-12-30T09:35:15.603169179Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.603191723Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.603194980Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.603202099Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.603205088Z             # find there is nothing left to do.
2024-12-30T09:35:15.603221817Z             if is_initialized():
2024-12-30T09:35:15.603224186Z                 return
2024-12-30T09:35:15.603226094Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.603252480Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.603256344Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.603258366Z             if _is_in_bad_fork():
2024-12-30T09:35:15.603260423Z                 raise RuntimeError(
2024-12-30T09:35:15.603300241Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.603303814Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.603305906Z                 )
2024-12-30T09:35:15.603307833Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.603314931Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.603317146Z             if _cudart is None:
2024-12-30T09:35:15.603319090Z                 raise AssertionError(
2024-12-30T09:35:15.603327760Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.603330035Z                 )
2024-12-30T09:35:15.603353524Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.603355978Z             # are found or any other error occurs
2024-12-30T09:35:15.603371830Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.603374140Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.603414783Z >           torch._C._cuda_init()
2024-12-30T09:35:15.603418481Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.603420923Z 
2024-12-30T09:35:15.603422758Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.603586298Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-8-128-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.603592102Z 
2024-12-30T09:35:15.603616253Z b = 8, n = 128, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.603619799Z 
2024-12-30T09:35:15.603781298Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.603785847Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.603792821Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.603794892Z         torch.manual_seed(2024)
2024-12-30T09:35:15.603797825Z         atol = 5e-2
2024-12-30T09:35:15.603799813Z         rtol = 1e-2
2024-12-30T09:35:15.603801673Z         device = torch.device("cuda")
2024-12-30T09:35:15.603832134Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.603835940Z 
2024-12-30T09:35:15.603837928Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.603878928Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.603882374Z 
2024-12-30T09:35:15.604053388Z     def _lazy_init():
2024-12-30T09:35:15.604057235Z         global _initialized, _queued_calls
2024-12-30T09:35:15.604059224Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.604061629Z             return
2024-12-30T09:35:15.604066528Z         with _initialization_lock:
2024-12-30T09:35:15.604069896Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.604071910Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.604085608Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.604088638Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.604090709Z             # find there is nothing left to do.
2024-12-30T09:35:15.604118852Z             if is_initialized():
2024-12-30T09:35:15.604121157Z                 return
2024-12-30T09:35:15.604151642Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.604158264Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.604160470Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.604162632Z             if _is_in_bad_fork():
2024-12-30T09:35:15.604164806Z                 raise RuntimeError(
2024-12-30T09:35:15.604167752Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.604170589Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.604182250Z                 )
2024-12-30T09:35:15.604194532Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.604197868Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.604211811Z             if _cudart is None:
2024-12-30T09:35:15.604219387Z                 raise AssertionError(
2024-12-30T09:35:15.604221464Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.604241676Z                 )
2024-12-30T09:35:15.604245034Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.604247154Z             # are found or any other error occurs
2024-12-30T09:35:15.604268475Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.604271704Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.604273961Z >           torch._C._cuda_init()
2024-12-30T09:35:15.604336445Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.604340094Z 
2024-12-30T09:35:15.604342082Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.604479417Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-128-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.604483807Z 
2024-12-30T09:35:15.604520638Z b = 8, n = 128, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.604523957Z 
2024-12-30T09:35:15.604716532Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.604724104Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.604726606Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.604728637Z         torch.manual_seed(2024)
2024-12-30T09:35:15.604730562Z         atol = 5e-2
2024-12-30T09:35:15.604732451Z         rtol = 1e-2
2024-12-30T09:35:15.604734438Z         device = torch.device("cuda")
2024-12-30T09:35:15.604737336Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.604739570Z 
2024-12-30T09:35:15.604801699Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.604806012Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.604808417Z 
2024-12-30T09:35:15.604978715Z     def _lazy_init():
2024-12-30T09:35:15.604981821Z         global _initialized, _queued_calls
2024-12-30T09:35:15.604984046Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.604986239Z             return
2024-12-30T09:35:15.604988118Z         with _initialization_lock:
2024-12-30T09:35:15.604997969Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.605014104Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.605017755Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.605019892Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.605022724Z             # find there is nothing left to do.
2024-12-30T09:35:15.605056810Z             if is_initialized():
2024-12-30T09:35:15.605061191Z                 return
2024-12-30T09:35:15.605064465Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.605067552Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.605083027Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.605087826Z             if _is_in_bad_fork():
2024-12-30T09:35:15.605089880Z                 raise RuntimeError(
2024-12-30T09:35:15.605091828Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.605103802Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.605106314Z                 )
2024-12-30T09:35:15.605118626Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.605140998Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.605144028Z             if _cudart is None:
2024-12-30T09:35:15.605146073Z                 raise AssertionError(
2024-12-30T09:35:15.605148058Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.605168758Z                 )
2024-12-30T09:35:15.605171451Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.605173643Z             # are found or any other error occurs
2024-12-30T09:35:15.605256748Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.605262920Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.605265105Z >           torch._C._cuda_init()
2024-12-30T09:35:15.605267176Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.605269516Z 
2024-12-30T09:35:15.605271383Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.605402868Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-8-256-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.605410481Z 
2024-12-30T09:35:15.605440330Z b = 8, n = 256, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.605444202Z 
2024-12-30T09:35:15.605639420Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.605644128Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.605646380Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.605648332Z         torch.manual_seed(2024)
2024-12-30T09:35:15.605650533Z         atol = 5e-2
2024-12-30T09:35:15.605652408Z         rtol = 1e-2
2024-12-30T09:35:15.605654267Z         device = torch.device("cuda")
2024-12-30T09:35:15.605666432Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.605670073Z 
2024-12-30T09:35:15.605681819Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.605708548Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.605711841Z 
2024-12-30T09:35:15.605901159Z     def _lazy_init():
2024-12-30T09:35:15.605904482Z         global _initialized, _queued_calls
2024-12-30T09:35:15.605906688Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.605908864Z             return
2024-12-30T09:35:15.605910745Z         with _initialization_lock:
2024-12-30T09:35:15.605937308Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.605942157Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.605944286Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.605952770Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.605959937Z             # find there is nothing left to do.
2024-12-30T09:35:15.605962204Z             if is_initialized():
2024-12-30T09:35:15.605970140Z                 return
2024-12-30T09:35:15.605988585Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.605990990Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.606004021Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.606007607Z             if _is_in_bad_fork():
2024-12-30T09:35:15.606024937Z                 raise RuntimeError(
2024-12-30T09:35:15.606027178Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.606033830Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.606057675Z                 )
2024-12-30T09:35:15.606062707Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.606072041Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.606075746Z             if _cudart is None:
2024-12-30T09:35:15.606090996Z                 raise AssertionError(
2024-12-30T09:35:15.606094155Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.606096384Z                 )
2024-12-30T09:35:15.606109889Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.606112318Z             # are found or any other error occurs
2024-12-30T09:35:15.606122263Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.606124719Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.606132983Z >           torch._C._cuda_init()
2024-12-30T09:35:15.606170419Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.606174395Z 
2024-12-30T09:35:15.606176331Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.606335982Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-256-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.606339585Z 
2024-12-30T09:35:15.606376527Z b = 8, n = 256, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.606387777Z 
2024-12-30T09:35:15.606565711Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.606570239Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.606572417Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.606574452Z         torch.manual_seed(2024)
2024-12-30T09:35:15.606597009Z         atol = 5e-2
2024-12-30T09:35:15.606600050Z         rtol = 1e-2
2024-12-30T09:35:15.606602001Z         device = torch.device("cuda")
2024-12-30T09:35:15.606604081Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.606631915Z 
2024-12-30T09:35:15.606634892Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.606668751Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.606677922Z 
2024-12-30T09:35:15.606851900Z     def _lazy_init():
2024-12-30T09:35:15.606855666Z         global _initialized, _queued_calls
2024-12-30T09:35:15.606857760Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.606860004Z             return
2024-12-30T09:35:15.606872645Z         with _initialization_lock:
2024-12-30T09:35:15.606875801Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.606883609Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.606887072Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.606900564Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.606903604Z             # find there is nothing left to do.
2024-12-30T09:35:15.606923593Z             if is_initialized():
2024-12-30T09:35:15.606926521Z                 return
2024-12-30T09:35:15.606928604Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.606937936Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.606964174Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.606967329Z             if _is_in_bad_fork():
2024-12-30T09:35:15.606969475Z                 raise RuntimeError(
2024-12-30T09:35:15.606971402Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.606982415Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.606985604Z                 )
2024-12-30T09:35:15.607017499Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.607020261Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.607022263Z             if _cudart is None:
2024-12-30T09:35:15.607024137Z                 raise AssertionError(
2024-12-30T09:35:15.607034837Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.607037356Z                 )
2024-12-30T09:35:15.607039864Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.607069375Z             # are found or any other error occurs
2024-12-30T09:35:15.607073652Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.607075878Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.607078551Z >           torch._C._cuda_init()
2024-12-30T09:35:15.607108479Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.607112295Z 
2024-12-30T09:35:15.607114261Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.607290824Z [31m[1m______________________________________________________________________________ test_srmsnorm[dtype2-8-257-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.607296449Z 
2024-12-30T09:35:15.607327127Z b = 8, n = 257, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.607330486Z 
2024-12-30T09:35:15.607523682Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.607527096Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.607529309Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.607531280Z         torch.manual_seed(2024)
2024-12-30T09:35:15.607533206Z         atol = 5e-2
2024-12-30T09:35:15.607535140Z         rtol = 1e-2
2024-12-30T09:35:15.607538634Z         device = torch.device("cuda")
2024-12-30T09:35:15.607540835Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.607543551Z 
2024-12-30T09:35:15.607623567Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.607632261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.607634773Z 
2024-12-30T09:35:15.607809948Z     def _lazy_init():
2024-12-30T09:35:15.607813387Z         global _initialized, _queued_calls
2024-12-30T09:35:15.607815400Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.607817732Z             return
2024-12-30T09:35:15.607819654Z         with _initialization_lock:
2024-12-30T09:35:15.607821590Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.607849351Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.607852249Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.607854383Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.607856443Z             # find there is nothing left to do.
2024-12-30T09:35:15.607866672Z             if is_initialized():
2024-12-30T09:35:15.607870767Z                 return
2024-12-30T09:35:15.607872720Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.607890519Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.607894059Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.607896094Z             if _is_in_bad_fork():
2024-12-30T09:35:15.607917545Z                 raise RuntimeError(
2024-12-30T09:35:15.607920615Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.607922755Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.607924830Z                 )
2024-12-30T09:35:15.607939860Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.607942880Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.607945018Z             if _cudart is None:
2024-12-30T09:35:15.607965338Z                 raise AssertionError(
2024-12-30T09:35:15.607969240Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.607972416Z                 )
2024-12-30T09:35:15.607995693Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.608000114Z             # are found or any other error occurs
2024-12-30T09:35:15.608002155Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.608007785Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.608010036Z >           torch._C._cuda_init()
2024-12-30T09:35:15.608071802Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.608075043Z 
2024-12-30T09:35:15.608076988Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.608239181Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-257-1024] ______________________________________________________________________________[0m
2024-12-30T09:35:15.608245082Z 
2024-12-30T09:35:15.608248002Z b = 8, n = 257, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.608251127Z 
2024-12-30T09:35:15.608429669Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.608433759Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.608436142Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.608438218Z         torch.manual_seed(2024)
2024-12-30T09:35:15.608440149Z         atol = 5e-2
2024-12-30T09:35:15.608446838Z         rtol = 1e-2
2024-12-30T09:35:15.608450092Z         device = torch.device("cuda")
2024-12-30T09:35:15.608462782Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.608466518Z 
2024-12-30T09:35:15.608469440Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.608500916Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.608505035Z 
2024-12-30T09:35:15.608696787Z     def _lazy_init():
2024-12-30T09:35:15.608700927Z         global _initialized, _queued_calls
2024-12-30T09:35:15.608703100Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.608705287Z             return
2024-12-30T09:35:15.608707153Z         with _initialization_lock:
2024-12-30T09:35:15.608709138Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.608713887Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.608716056Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.608725095Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.608727325Z             # find there is nothing left to do.
2024-12-30T09:35:15.608759907Z             if is_initialized():
2024-12-30T09:35:15.608762632Z                 return
2024-12-30T09:35:15.608771524Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.608775129Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.608777214Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.608793199Z             if _is_in_bad_fork():
2024-12-30T09:35:15.608796014Z                 raise RuntimeError(
2024-12-30T09:35:15.608797976Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.608812329Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.608814818Z                 )
2024-12-30T09:35:15.608921871Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.608925135Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.608927152Z             if _cudart is None:
2024-12-30T09:35:15.608929175Z                 raise AssertionError(
2024-12-30T09:35:15.608931099Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.608938734Z                 )
2024-12-30T09:35:15.608940684Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.608942790Z             # are found or any other error occurs
2024-12-30T09:35:15.608944769Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.608946943Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.608948978Z >           torch._C._cuda_init()
2024-12-30T09:35:15.608950977Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.608953843Z 
2024-12-30T09:35:15.608955841Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.609120508Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-1024-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.609124415Z 
2024-12-30T09:35:15.609126274Z b = 8, n = 1024, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.609128383Z 
2024-12-30T09:35:15.609338018Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.609341928Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.609344160Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.609346140Z         torch.manual_seed(2024)
2024-12-30T09:35:15.609348062Z         atol = 5e-2
2024-12-30T09:35:15.609350031Z         rtol = 1e-2
2024-12-30T09:35:15.609351899Z         device = torch.device("cuda")
2024-12-30T09:35:15.609353886Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.609380291Z 
2024-12-30T09:35:15.609382916Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.609414955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.609417822Z 
2024-12-30T09:35:15.609617669Z     def _lazy_init():
2024-12-30T09:35:15.609622436Z         global _initialized, _queued_calls
2024-12-30T09:35:15.609624456Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.609626657Z             return
2024-12-30T09:35:15.609628549Z         with _initialization_lock:
2024-12-30T09:35:15.609630477Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.609638921Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.609645037Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.609653474Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.609655681Z             # find there is nothing left to do.
2024-12-30T09:35:15.609662053Z             if is_initialized():
2024-12-30T09:35:15.609664071Z                 return
2024-12-30T09:35:15.609702237Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.609705649Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.609707771Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.609709734Z             if _is_in_bad_fork():
2024-12-30T09:35:15.609736585Z                 raise RuntimeError(
2024-12-30T09:35:15.609739824Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.609742013Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.609744043Z                 )
2024-12-30T09:35:15.609754473Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.609757998Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.609767315Z             if _cudart is None:
2024-12-30T09:35:15.609770307Z                 raise AssertionError(
2024-12-30T09:35:15.609776676Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.609784482Z                 )
2024-12-30T09:35:15.609786499Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.609810725Z             # are found or any other error occurs
2024-12-30T09:35:15.609813098Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.609815133Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.609831189Z >           torch._C._cuda_init()
2024-12-30T09:35:15.609864930Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.609868362Z 
2024-12-30T09:35:15.609870211Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.610030549Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-1024-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.610034853Z 
2024-12-30T09:35:15.610052681Z b = 8, n = 1024, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.610058598Z 
2024-12-30T09:35:15.610254767Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.610258231Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.610260466Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.610262435Z         torch.manual_seed(2024)
2024-12-30T09:35:15.610278910Z         atol = 5e-2
2024-12-30T09:35:15.610282557Z         rtol = 1e-2
2024-12-30T09:35:15.610306953Z         device = torch.device("cuda")
2024-12-30T09:35:15.610310262Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.610312379Z 
2024-12-30T09:35:15.610334723Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.610338007Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.610375866Z 
2024-12-30T09:35:15.610581818Z     def _lazy_init():
2024-12-30T09:35:15.610585886Z         global _initialized, _queued_calls
2024-12-30T09:35:15.610587911Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.610590253Z             return
2024-12-30T09:35:15.610592197Z         with _initialization_lock:
2024-12-30T09:35:15.610594218Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.610596311Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.610599943Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.610602080Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.610604144Z             # find there is nothing left to do.
2024-12-30T09:35:15.610617915Z             if is_initialized():
2024-12-30T09:35:15.610620633Z                 return
2024-12-30T09:35:15.610639107Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.610643444Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.610645650Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.610655883Z             if _is_in_bad_fork():
2024-12-30T09:35:15.610673691Z                 raise RuntimeError(
2024-12-30T09:35:15.610676654Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.610678770Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.610690968Z                 )
2024-12-30T09:35:15.610699925Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.610710558Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.610714506Z             if _cudart is None:
2024-12-30T09:35:15.610717566Z                 raise AssertionError(
2024-12-30T09:35:15.610753935Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.610757430Z                 )
2024-12-30T09:35:15.610759372Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.610761423Z             # are found or any other error occurs
2024-12-30T09:35:15.610763549Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.610803090Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.610808707Z >           torch._C._cuda_init()
2024-12-30T09:35:15.610812109Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.610815893Z 
2024-12-30T09:35:15.610833730Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.610985705Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-1025-768] ______________________________________________________________________________[0m
2024-12-30T09:35:15.610988688Z 
2024-12-30T09:35:15.611007408Z b = 8, n = 1025, d = 768, dtype = torch.bfloat16
2024-12-30T09:35:15.611009846Z 
2024-12-30T09:35:15.611199866Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.611204868Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.611208292Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.611211156Z         torch.manual_seed(2024)
2024-12-30T09:35:15.611214091Z         atol = 5e-2
2024-12-30T09:35:15.611216954Z         rtol = 1e-2
2024-12-30T09:35:15.611219943Z         device = torch.device("cuda")
2024-12-30T09:35:15.611223921Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.611227311Z 
2024-12-30T09:35:15.611230353Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.611286677Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.611290274Z 
2024-12-30T09:35:15.611494910Z     def _lazy_init():
2024-12-30T09:35:15.611504215Z         global _initialized, _queued_calls
2024-12-30T09:35:15.611507433Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.611517786Z             return
2024-12-30T09:35:15.611520813Z         with _initialization_lock:
2024-12-30T09:35:15.611523699Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.611526767Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.611530024Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.611533017Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.611536104Z             # find there is nothing left to do.
2024-12-30T09:35:15.611540442Z             if is_initialized():
2024-12-30T09:35:15.611543666Z                 return
2024-12-30T09:35:15.611546874Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.611559288Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.611562516Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.611565629Z             if _is_in_bad_fork():
2024-12-30T09:35:15.611585029Z                 raise RuntimeError(
2024-12-30T09:35:15.611589078Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.611591311Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.611593398Z                 )
2024-12-30T09:35:15.611602216Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.611615700Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.611618931Z             if _cudart is None:
2024-12-30T09:35:15.611620905Z                 raise AssertionError(
2024-12-30T09:35:15.611686921Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.611689790Z                 )
2024-12-30T09:35:15.611691805Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.611693837Z             # are found or any other error occurs
2024-12-30T09:35:15.611695780Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.611697814Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.611699987Z >           torch._C._cuda_init()
2024-12-30T09:35:15.611702503Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.611710295Z 
2024-12-30T09:35:15.611743079Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.611877216Z [31m[1m_____________________________________________________________________________ test_srmsnorm[dtype2-8-1025-1024] _____________________________________________________________________________[0m
2024-12-30T09:35:15.611880975Z 
2024-12-30T09:35:15.611910848Z b = 8, n = 1025, d = 1024, dtype = torch.bfloat16
2024-12-30T09:35:15.611914321Z 
2024-12-30T09:35:15.612093069Z     @pytest.mark.parametrize("b, n, d", get_params())
2024-12-30T09:35:15.612096546Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16, torch.bfloat16])
2024-12-30T09:35:15.612098740Z     def test_srmsnorm(b, n, d, dtype):
2024-12-30T09:35:15.612100665Z         torch.manual_seed(2024)
2024-12-30T09:35:15.612102641Z         atol = 5e-2
2024-12-30T09:35:15.612104518Z         rtol = 1e-2
2024-12-30T09:35:15.612106454Z         device = torch.device("cuda")
2024-12-30T09:35:15.612120023Z >       x = torch.randn((b, n, d), dtype=dtype, device=device).requires_grad_()
2024-12-30T09:35:15.612123438Z 
2024-12-30T09:35:15.612147340Z [1m[31mtests/ops/test_srmsnorm.py[0m:25: 
2024-12-30T09:35:15.612150430Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2024-12-30T09:35:15.612152698Z 
2024-12-30T09:35:15.612368610Z     def _lazy_init():
2024-12-30T09:35:15.612372877Z         global _initialized, _queued_calls
2024-12-30T09:35:15.612375992Z         if is_initialized() or hasattr(_tls, "is_initializing"):
2024-12-30T09:35:15.612378712Z             return
2024-12-30T09:35:15.612381089Z         with _initialization_lock:
2024-12-30T09:35:15.612383876Z             # We be double-checked locking, boys!  This is OK because
2024-12-30T09:35:15.612386829Z             # the above test was GIL protected anyway.  The inner test
2024-12-30T09:35:15.612390825Z             # is for when a thread blocked on some other thread which was
2024-12-30T09:35:15.612394438Z             # doing the initialization; when they get the lock, they will
2024-12-30T09:35:15.612426345Z             # find there is nothing left to do.
2024-12-30T09:35:15.612432409Z             if is_initialized():
2024-12-30T09:35:15.612435501Z                 return
2024-12-30T09:35:15.612438606Z             # It is important to prevent other threads from entering _lazy_init
2024-12-30T09:35:15.612446086Z             # immediately, while we are still guaranteed to have the GIL, because some
2024-12-30T09:35:15.612453874Z             # of the C calls we make below will release the GIL
2024-12-30T09:35:15.612464231Z             if _is_in_bad_fork():
2024-12-30T09:35:15.612467430Z                 raise RuntimeError(
2024-12-30T09:35:15.612470428Z                     "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
2024-12-30T09:35:15.612484013Z                     "multiprocessing, you must use the 'spawn' start method"
2024-12-30T09:35:15.612487559Z                 )
2024-12-30T09:35:15.612512344Z             if not hasattr(torch._C, "_cuda_getDeviceCount"):
2024-12-30T09:35:15.612516195Z                 raise AssertionError("Torch not compiled with CUDA enabled")
2024-12-30T09:35:15.612518301Z             if _cudart is None:
2024-12-30T09:35:15.612520229Z                 raise AssertionError(
2024-12-30T09:35:15.612522156Z                     "libcudart functions unavailable. It looks like you have a broken build?"
2024-12-30T09:35:15.612572422Z                 )
2024-12-30T09:35:15.612577859Z             # This function throws if there's a driver initialization error, no GPUs
2024-12-30T09:35:15.612581177Z             # are found or any other error occurs
2024-12-30T09:35:15.612584311Z             if "CUDA_MODULE_LOADING" not in os.environ:
2024-12-30T09:35:15.612587488Z                 os.environ["CUDA_MODULE_LOADING"] = "LAZY"
2024-12-30T09:35:15.612591859Z >           torch._C._cuda_init()
2024-12-30T09:35:15.612595323Z [1m[31mE           RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx[0m
2024-12-30T09:35:15.612616123Z 
2024-12-30T09:35:15.612619343Z [1m[31m/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py[0m:319: RuntimeError
2024-12-30T09:35:15.612813809Z [33m===================================================================================== warnings summary ======================================================================================[0m
2024-12-30T09:35:15.612931343Z ../usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295
2024-12-30T09:35:15.612934515Z   /usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
2024-12-30T09:35:15.612936937Z     cpu = _conversion_method_template(device=torch.device("cpu"))
2024-12-30T09:35:15.612939227Z 
2024-12-30T09:35:15.612941090Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2024-12-30T09:35:15.626322469Z [36m[1m================================================================================== short test summary info ==================================================================================[0m
2024-12-30T09:35:15.626351388Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-256-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626373908Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-512-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626377727Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-1024-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626381495Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626385153Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-4096-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626388823Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-8192-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626392513Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-2048-32-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626398983Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-2048-64-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626403166Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-12-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626407267Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-16-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626410898Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-20-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626414658Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-1-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626421806Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-2-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626425702Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-3-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626431998Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-913-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626435602Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-513-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626439100Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-1213-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626442572Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype0-6-8-2048-16-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626454031Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-256-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626562704Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-512-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626569797Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-1024-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626573693Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626577465Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-4096-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626584353Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-8192-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626596488Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-2048-32-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626599328Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-2048-64-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626619783Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-12-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626625258Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-16-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626629502Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-20-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626711683Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-1-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626715507Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-2-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626717977Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-3-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626720527Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-913-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626722881Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-513-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626725193Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-1213-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626730287Z [31mFAILED[0m tests/ops/test_lightning2.py::[1mtest_lightning2[dtype1-6-8-2048-16-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626734469Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-256-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626736930Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-512-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626761287Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-1024-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626764322Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626766864Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-4096-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626769444Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-8192-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626772607Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-2048-32-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626815451Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-2048-64-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626824452Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-12-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626827272Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-16-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626831377Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-20-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626840040Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-1-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626842527Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-2-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626844998Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-3-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626847375Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-913-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626880717Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-513-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626884157Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-1213-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626886560Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype0-6-8-2048-16-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626888963Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-256-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626912695Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-512-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626915893Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-1024-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626918313Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626925765Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-4096-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626947666Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-8192-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626950569Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-2048-32-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626953178Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-2048-64-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626956489Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-12-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626984521Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-16-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626987387Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-20-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.626989930Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-1-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627007827Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-2-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627012021Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-3-8-2048-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627014472Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-913-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627017823Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-513-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627045415Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-1213-128-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627049413Z [31mFAILED[0m tests/ops/test_lightning2_no_decay.py::[1mtest_lightning2[dtype1-6-8-2048-16-64][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627061722Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627065595Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627076888Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627080167Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627086472Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627109062Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627112163Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627141389Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627145274Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627150419Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627152883Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627158580Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-1-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627161100Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627186980Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627189760Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627195801Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627198369Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627239915Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627243879Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627246347Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627248743Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627255555Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627258146Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627288516Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-2-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627291465Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627293790Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627333612Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627337383Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627339782Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627342274Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627384534Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627387876Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627390294Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627395555Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627397972Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627428539Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-4-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627435723Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627439195Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627443044Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627464740Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627468931Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627479813Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627483162Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627485550Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627487975Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627561056Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627564995Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627567336Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype0-8-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627569754Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627572215Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627574554Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627576903Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627599533Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627610478Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627613130Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627615678Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627618087Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627628399Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627631076Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627634566Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-1-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627636998Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627658857Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627688097Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627698603Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627701274Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627703816Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627707786Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627710309Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627744675Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627751129Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627753503Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627755892Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-2-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627782842Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627785995Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627788372Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627790911Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627813339Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627817589Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627820010Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627823933Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627838831Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627847664Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627851990Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627868025Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-4-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627871907Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627887382Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627891251Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627924379Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627927368Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627930678Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627933192Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627951432Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627957303Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627984732Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627989020Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627991492Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype1-8-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.627999004Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628001530Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628029480Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628032777Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628035295Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628038172Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628068767Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628072619Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628074936Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628097098Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628099742Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628102153Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-1-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628119811Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628122644Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628131650Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628158743Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628162734Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628165136Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628169192Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628180943Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628184514Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628207082Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628211045Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628217464Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-2-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628219953Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628255349Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628259146Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628261552Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628263927Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628288735Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628292285Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628294673Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628297072Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628317824Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628321709Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628324087Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-4-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628342133Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-127-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628345741Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-127-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628348136Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-128-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628371653Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-128-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628397695Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-256-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628401110Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-256-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628403461Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-257-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628405879Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-257-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628437490Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-1024-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628443666Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-1024-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628446271Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-1025-768][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628448614Z [31mFAILED[0m tests/ops/test_srmsnorm.py::[1mtest_srmsnorm[dtype2-8-1025-1024][0m - RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
2024-12-30T09:35:15.628645908Z [31m============================================================================== [31m[1m216 failed[0m, [33m1 warning[0m[31m in 7.93s[0m[31m ===============================================================================[0m
2024-12-30T09:35:49.238778348Z [?2004hroot@f3ad0414bf1d:/lightning-attention# pytest --collect-only -q
2024-12-30T09:35:51.006512798Z [?2004l
tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-256-128-64]
2024-12-30T09:35:51.006559418Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-512-128-64]
2024-12-30T09:35:51.006563584Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-1024-128-64]
2024-12-30T09:35:51.006566504Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-128-64]
2024-12-30T09:35:51.006568964Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-4096-128-64]
2024-12-30T09:35:51.006571344Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-8192-128-64]
2024-12-30T09:35:51.006573784Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-32-64]
2024-12-30T09:35:51.006576104Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-64-64]
2024-12-30T09:35:51.006578378Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-12-2048-128-64]
2024-12-30T09:35:51.006580696Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-16-2048-128-64]
2024-12-30T09:35:51.006582951Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-20-2048-128-64]
2024-12-30T09:35:51.006585539Z tests/ops/test_lightning2.py::test_lightning2[dtype0-1-8-2048-128-64]
2024-12-30T09:35:51.006597147Z tests/ops/test_lightning2.py::test_lightning2[dtype0-2-8-2048-128-64]
2024-12-30T09:35:51.006599501Z tests/ops/test_lightning2.py::test_lightning2[dtype0-3-8-2048-128-64]
2024-12-30T09:35:51.006601761Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-913-128-64]
2024-12-30T09:35:51.006603968Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-513-128-64]
2024-12-30T09:35:51.006606201Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-1213-128-64]
2024-12-30T09:35:51.006619763Z tests/ops/test_lightning2.py::test_lightning2[dtype0-6-8-2048-16-64]
2024-12-30T09:35:51.006621965Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-256-128-64]
2024-12-30T09:35:51.006624052Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-512-128-64]
2024-12-30T09:35:51.006626219Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-1024-128-64]
2024-12-30T09:35:51.006628231Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-128-64]
2024-12-30T09:35:51.006630344Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-4096-128-64]
2024-12-30T09:35:51.006632344Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-8192-128-64]
2024-12-30T09:35:51.006634353Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-32-64]
2024-12-30T09:35:51.006636374Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-64-64]
2024-12-30T09:35:51.006638399Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-12-2048-128-64]
2024-12-30T09:35:51.006640451Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-16-2048-128-64]
2024-12-30T09:35:51.006643208Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-20-2048-128-64]
2024-12-30T09:35:51.006645256Z tests/ops/test_lightning2.py::test_lightning2[dtype1-1-8-2048-128-64]
2024-12-30T09:35:51.006647285Z tests/ops/test_lightning2.py::test_lightning2[dtype1-2-8-2048-128-64]
2024-12-30T09:35:51.006649295Z tests/ops/test_lightning2.py::test_lightning2[dtype1-3-8-2048-128-64]
2024-12-30T09:35:51.006651320Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-913-128-64]
2024-12-30T09:35:51.006653411Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-513-128-64]
2024-12-30T09:35:51.006655443Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-1213-128-64]
2024-12-30T09:35:51.006657491Z tests/ops/test_lightning2.py::test_lightning2[dtype1-6-8-2048-16-64]
2024-12-30T09:35:51.006659502Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-256-128-64]
2024-12-30T09:35:51.006661563Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-512-128-64]
2024-12-30T09:35:51.006663693Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-1024-128-64]
2024-12-30T09:35:51.006665746Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-128-64]
2024-12-30T09:35:51.006667774Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-4096-128-64]
2024-12-30T09:35:51.006669794Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-8192-128-64]
2024-12-30T09:35:51.006671816Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-32-64]
2024-12-30T09:35:51.006673920Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-64-64]
2024-12-30T09:35:51.006675985Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-12-2048-128-64]
2024-12-30T09:35:51.006680658Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-16-2048-128-64]
2024-12-30T09:35:51.006684025Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-20-2048-128-64]
2024-12-30T09:35:51.006686108Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-1-8-2048-128-64]
2024-12-30T09:35:51.006688117Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-2-8-2048-128-64]
2024-12-30T09:35:51.006690123Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-3-8-2048-128-64]
2024-12-30T09:35:51.006702929Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-913-128-64]
2024-12-30T09:35:51.006705209Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-513-128-64]
2024-12-30T09:35:51.006708222Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-1213-128-64]
2024-12-30T09:35:51.006710318Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype0-6-8-2048-16-64]
2024-12-30T09:35:51.006712374Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-256-128-64]
2024-12-30T09:35:51.006714429Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-512-128-64]
2024-12-30T09:35:51.006717118Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-1024-128-64]
2024-12-30T09:35:51.006719177Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-128-64]
2024-12-30T09:35:51.006746443Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-4096-128-64]
2024-12-30T09:35:51.006750125Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-8192-128-64]
2024-12-30T09:35:51.006752132Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-32-64]
2024-12-30T09:35:51.006787482Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-64-64]
2024-12-30T09:35:51.006791661Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-12-2048-128-64]
2024-12-30T09:35:51.006793743Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-16-2048-128-64]
2024-12-30T09:35:51.006795921Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-20-2048-128-64]
2024-12-30T09:35:51.006798944Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-1-8-2048-128-64]
2024-12-30T09:35:51.006801050Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-2-8-2048-128-64]
2024-12-30T09:35:51.006827703Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-3-8-2048-128-64]
2024-12-30T09:35:51.006845644Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-913-128-64]
2024-12-30T09:35:51.006848159Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-513-128-64]
2024-12-30T09:35:51.006851224Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-1213-128-64]
2024-12-30T09:35:51.006859299Z tests/ops/test_lightning2_no_decay.py::test_lightning2[dtype1-6-8-2048-16-64]
2024-12-30T09:35:51.006865982Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-127-768]
2024-12-30T09:35:51.006868846Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-127-1024]
2024-12-30T09:35:51.006871014Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-128-768]
2024-12-30T09:35:51.006872968Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-128-1024]
2024-12-30T09:35:51.006897293Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-256-768]
2024-12-30T09:35:51.006901042Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-256-1024]
2024-12-30T09:35:51.006903139Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-257-768]
2024-12-30T09:35:51.006905133Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-257-1024]
2024-12-30T09:35:51.006929452Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1024-768]
2024-12-30T09:35:51.006937943Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1024-1024]
2024-12-30T09:35:51.006952526Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1025-768]
2024-12-30T09:35:51.006956363Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-1-1025-1024]
2024-12-30T09:35:51.006958734Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-127-768]
2024-12-30T09:35:51.006960819Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-127-1024]
2024-12-30T09:35:51.006981325Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-128-768]
2024-12-30T09:35:51.006985352Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-128-1024]
2024-12-30T09:35:51.006987331Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-256-768]
2024-12-30T09:35:51.006989246Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-256-1024]
2024-12-30T09:35:51.007006464Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-257-768]
2024-12-30T09:35:51.007010077Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-257-1024]
2024-12-30T09:35:51.007034755Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1024-768]
2024-12-30T09:35:51.007038495Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1024-1024]
2024-12-30T09:35:51.007040524Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1025-768]
2024-12-30T09:35:51.007042570Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-2-1025-1024]
2024-12-30T09:35:51.007071475Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-127-768]
2024-12-30T09:35:51.007074953Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-127-1024]
2024-12-30T09:35:51.007076892Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-128-768]
2024-12-30T09:35:51.007078853Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-128-1024]
2024-12-30T09:35:51.007080802Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-256-768]
2024-12-30T09:35:51.007092717Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-256-1024]
2024-12-30T09:35:51.007095071Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-257-768]
2024-12-30T09:35:51.007105221Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-257-1024]
2024-12-30T09:35:51.007145262Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1024-768]
2024-12-30T09:35:51.007161002Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1024-1024]
2024-12-30T09:35:51.007164860Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1025-768]
2024-12-30T09:35:51.007168321Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-4-1025-1024]
2024-12-30T09:35:51.007171272Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-127-768]
2024-12-30T09:35:51.007174287Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-127-1024]
2024-12-30T09:35:51.007178741Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-128-768]
2024-12-30T09:35:51.007181916Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-128-1024]
2024-12-30T09:35:51.007184957Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-256-768]
2024-12-30T09:35:51.007188831Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-256-1024]
2024-12-30T09:35:51.007191896Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-257-768]
2024-12-30T09:35:51.007214261Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-257-1024]
2024-12-30T09:35:51.007217624Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1024-768]
2024-12-30T09:35:51.007219794Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1024-1024]
2024-12-30T09:35:51.007221837Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1025-768]
2024-12-30T09:35:51.007238398Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype0-8-1025-1024]
2024-12-30T09:35:51.007250088Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-127-768]
2024-12-30T09:35:51.007268220Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-127-1024]
2024-12-30T09:35:51.007270802Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-128-768]
2024-12-30T09:35:51.007272799Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-128-1024]
2024-12-30T09:35:51.007275228Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-256-768]
2024-12-30T09:35:51.007283275Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-256-1024]
2024-12-30T09:35:51.007295937Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-257-768]
2024-12-30T09:35:51.007301962Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-257-1024]
2024-12-30T09:35:51.007314955Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1024-768]
2024-12-30T09:35:51.007341556Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1024-1024]
2024-12-30T09:35:51.007356276Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1025-768]
2024-12-30T09:35:51.007373807Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-1-1025-1024]
2024-12-30T09:35:51.007377062Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-127-768]
2024-12-30T09:35:51.007381452Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-127-1024]
2024-12-30T09:35:51.007392848Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-128-768]
2024-12-30T09:35:51.007396276Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-128-1024]
2024-12-30T09:35:51.007399467Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-256-768]
2024-12-30T09:35:51.007403619Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-256-1024]
2024-12-30T09:35:51.007406727Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-257-768]
2024-12-30T09:35:51.007445242Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-257-1024]
2024-12-30T09:35:51.007452672Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1024-768]
2024-12-30T09:35:51.007454997Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1024-1024]
2024-12-30T09:35:51.007457046Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1025-768]
2024-12-30T09:35:51.007467295Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-2-1025-1024]
2024-12-30T09:35:51.007471005Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-127-768]
2024-12-30T09:35:51.007472961Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-127-1024]
2024-12-30T09:35:51.007474894Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-128-768]
2024-12-30T09:35:51.007476935Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-128-1024]
2024-12-30T09:35:51.007501409Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-256-768]
2024-12-30T09:35:51.007505522Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-256-1024]
2024-12-30T09:35:51.007526583Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-257-768]
2024-12-30T09:35:51.007531406Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-257-1024]
2024-12-30T09:35:51.007533518Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1024-768]
2024-12-30T09:35:51.007535467Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1024-1024]
2024-12-30T09:35:51.007569325Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1025-768]
2024-12-30T09:35:51.007573294Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-4-1025-1024]
2024-12-30T09:35:51.007575294Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-127-768]
2024-12-30T09:35:51.007577332Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-127-1024]
2024-12-30T09:35:51.007579261Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-128-768]
2024-12-30T09:35:51.007613661Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-128-1024]
2024-12-30T09:35:51.007617734Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-256-768]
2024-12-30T09:35:51.007619992Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-256-1024]
2024-12-30T09:35:51.007649073Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-257-768]
2024-12-30T09:35:51.007662930Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-257-1024]
2024-12-30T09:35:51.007666975Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1024-768]
2024-12-30T09:35:51.007680282Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1024-1024]
2024-12-30T09:35:51.007683705Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1025-768]
2024-12-30T09:35:51.007686633Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype1-8-1025-1024]
2024-12-30T09:35:51.007691263Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-127-768]
2024-12-30T09:35:51.007694550Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-127-1024]
2024-12-30T09:35:51.007697593Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-128-768]
2024-12-30T09:35:51.007700848Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-128-1024]
2024-12-30T09:35:51.007732967Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-256-768]
2024-12-30T09:35:51.007736148Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-256-1024]
2024-12-30T09:35:51.007738128Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-257-768]
2024-12-30T09:35:51.007740217Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-257-1024]
2024-12-30T09:35:51.007742209Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1024-768]
2024-12-30T09:35:51.007744124Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1024-1024]
2024-12-30T09:35:51.007746681Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1025-768]
2024-12-30T09:35:51.007748647Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-1-1025-1024]
2024-12-30T09:35:51.007770227Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-127-768]
2024-12-30T09:35:51.007773849Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-127-1024]
2024-12-30T09:35:51.007776007Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-128-768]
2024-12-30T09:35:51.007794354Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-128-1024]
2024-12-30T09:35:51.007807261Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-256-768]
2024-12-30T09:35:51.007810594Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-256-1024]
2024-12-30T09:35:51.007812772Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-257-768]
2024-12-30T09:35:51.007843518Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-257-1024]
2024-12-30T09:35:51.007850879Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1024-768]
2024-12-30T09:35:51.007853064Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1024-1024]
2024-12-30T09:35:51.007887501Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1025-768]
2024-12-30T09:35:51.007891366Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-2-1025-1024]
2024-12-30T09:35:51.007893319Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-127-768]
2024-12-30T09:35:51.007895240Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-127-1024]
2024-12-30T09:35:51.007897175Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-128-768]
2024-12-30T09:35:51.007899184Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-128-1024]
2024-12-30T09:35:51.007905296Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-256-768]
2024-12-30T09:35:51.007922202Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-256-1024]
2024-12-30T09:35:51.007925826Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-257-768]
2024-12-30T09:35:51.007927896Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-257-1024]
2024-12-30T09:35:51.007929916Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1024-768]
2024-12-30T09:35:51.007943657Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1024-1024]
2024-12-30T09:35:51.007948304Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1025-768]
2024-12-30T09:35:51.007950459Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-4-1025-1024]
2024-12-30T09:35:51.007967597Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-127-768]
2024-12-30T09:35:51.007970964Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-127-1024]
2024-12-30T09:35:51.007981614Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-128-768]
2024-12-30T09:35:51.007983950Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-128-1024]
2024-12-30T09:35:51.007999441Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-256-768]
2024-12-30T09:35:51.008002518Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-256-1024]
2024-12-30T09:35:51.008013454Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-257-768]
2024-12-30T09:35:51.008016839Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-257-1024]
2024-12-30T09:35:51.008030802Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1024-768]
2024-12-30T09:35:51.008035620Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1024-1024]
2024-12-30T09:35:51.008049933Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1025-768]
2024-12-30T09:35:51.008052290Z tests/ops/test_srmsnorm.py::test_srmsnorm[dtype2-8-1025-1024]
2024-12-30T09:35:51.008930477Z 
2024-12-30T09:35:51.009003939Z [33m===================================================================================== warnings summary ======================================================================================[0m
2024-12-30T09:35:51.009122248Z ../usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295
2024-12-30T09:35:51.009132764Z   /usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
2024-12-30T09:35:51.009137030Z     cpu = _conversion_method_template(device=torch.device("cpu"))
2024-12-30T09:35:51.009165195Z 
2024-12-30T09:35:51.009168474Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2024-12-30T09:35:51.009288352Z [32m[32m216 tests collected[0m[32m in 1.60s[0m[0m
2024-12-30T09:36:49.781197988Z [?2004hroot@f3ad0414bf1d:/lightning-attention# exit
2024-12-30T09:36:49.781243501Z [?2004l
exit
